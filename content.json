{"posts":[{"title":"0x01 TV Layout (Thread-Value Layout) Guide","text":"This article explains TV Layout in CuTe, the core abstraction for describing how data is distributed across threads. 示例代码: 0x01_tv_layout.cu 1. IntroductionTV Layout is the core abstraction in CuTe that describes how data is distributed across threads. Understanding TV Layout is key to mastering WGMMA and TMA. 2. TV Layout 的本质TV Layout 本质上是一个函数： 123f: (thread_id, value_id) → offset给定 (线程ID, 值索引) → 返回内存偏移 123456// Layout 定义Layout&lt;Shape&lt;Thr_shape, Val_shape&gt;, Stride&lt;Thr_stride, Val_stride&gt;&gt;// 等价于函数:offset = f(thread_id, value_id) = thread_id • Thr_stride + value_id • Val_stride 3. 简单示例3.1 连续分布1234567// 32 个元素分配给 4 个线程，每线程 8 个值using TVLayout = Layout&lt; Shape &lt;_4, _8&gt;, // (Thr=4, Val=8) Stride&lt;_8, _1&gt; // (Thr_stride=8, Val_stride=1)&gt;;// offset = thread_idx * 8 + value_idx * 1 TV Table: 12345678910 Val: 0 1 2 3 4 5 6 7 +---+---+---+---+---+---+---+---+Thr 0 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | +---+---+---+---+---+---+---+---+Thr 1 | 8 | 9 |10 |11 |12 |13 |14 |15 | +---+---+---+---+---+---+---+---+Thr 2 |16 |17 |18 |19 |20 |21 |22 |23 | +---+---+---+---+---+---+---+---+Thr 3 |24 |25 |26 |27 |28 |29 |30 |31 | +---+---+---+---+---+---+---+---+ 3.2 交错分布123456using TVLayout = Layout&lt; Shape &lt;_4, _8&gt;, // (Thr=4, Val=8) Stride&lt;_1, _4&gt; // (Thr_stride=1, Val_stride=4)&gt;;// offset = thread_idx * 1 + value_idx * 4 TV Table: 12345678910 Val: 0 1 2 3 4 5 6 7 +---+---+---+---+---+---+---+---+Thr 0 | 0 | 4 | 8 |12 |16 |20 |24 |28 | +---+---+---+---+---+---+---+---+Thr 1 | 1 | 5 | 9 |13 |17 |21 |25 |29 | +---+---+---+---+---+---+---+---+Thr 2 | 2 | 6 |10 |14 |18 |22 |26 |30 | +---+---+---+---+---+---+---+---+Thr 3 | 3 | 7 |11 |15 |19 |23 |27 |31 | +---+---+---+---+---+---+---+---+ 4. TV Layout 的两个 SliceTV Layout 可以理解为两部分的组合： 12345Layout&lt; Shape &lt;Thr_Shape, Val_Shape&gt;, Stride&lt;Thr_Stride, Val_Stride&gt;&gt;// +-- Thr Slice --+ +-- Val Slice --+ Slice 描述 含义 Thr Slice 第一个维度 Thread 的起始位置 (Register 0 的位置) Val Slice 第二个维度 每个 Register 相对起始的偏移 (所有 Thread 共用) 4.1 公式分解12offset(thread_id, value_id) = thr_offset(thread_id) + val_offset(value_id)// +------ base position ----+ +-- relative offset --+ 类比理解： Thr Slice = 每个员工的工位起始地址 Val Slice = 工位内部抽屉的相对位置 5. 复杂示例：ALayout_64x165.1 Layout 定义123456using ALayout_64x16 = Layout&lt; Shape &lt;Shape &lt; _4, _8, _4&gt;, Shape &lt; _2, _2, _2&gt;&gt;, Stride&lt;Stride&lt;_128, _1, _16&gt;, Stride&lt;_64, _8, _512&gt;&gt;&gt;;// +------ Thr Slice ------+ +---- Val Slice ----+// 128 线程 (4×8×4) 8 值/线程 (2×2×2) 5.2 维度拆解 维度 Shape Stride 元素数 Thr (线程) (4, 8, 4) (128, 1, 16) 4×8×4 = 128 Val (值) (2, 2, 2) (64, 8, 512) 2×2×2 = 8 总元素数: 128 线程 × 8 值/线程 = 1024 = 64 × 16 ✓ 5.3 Thread 0 的 8 个 Value (Column-Major)1234567891011121314// Thread 0: t0=0, t1=0, t2=0// thr_offset = 0// 8 个 Value 的 offset 和 (m, k) 坐标:Val (v0,v1,v2) offset m = offset%64 k = offset/64------------------------------------------------------------ 0 (0,0,0) 0 0 0 1 (1,0,0) 64 0 1 2 (0,1,0) 8 8 0 3 (1,1,0) 72 8 1 4 (0,0,1) 512 0 8 5 (1,0,1) 576 0 9 6 (0,1,1) 520 8 8 7 (1,1,1) 584 8 9 5.4 Diagram: Thread 0’s 8 positions1234567891011121314151664x16 matrix (Column-Major): k=0 k=1 k=8 k=9 +-------+-------+ +-------+-------+ m=0 | V0 | V1 | ... | V4 | V5 | | (0,0) | (0,1) | | (0,8) | (0,9) | +-------+-------+ +-------+-------+ +-------+-------+ +-------+-------+ m=8 | V2 | V3 | ... | V6 | V7 | | (8,0) | (8,1) | | (8,8) | (8,9) | +-------+-------+ +-------+-------+Thread 0's 8 positions: - V0,V1,V4,V5 at row m=0 - V2,V3,V6,V7 at row m=8 6. SS 模式的特殊 TV Layout6.1 Thr Stride = 012345678// SS 模式: 所有线程共享相同数据using TVLayout = Layout&lt; Shape &lt;_128, Shape &lt;_64, _16&gt;&gt;, // (Thr=128, Val=(64,16)) Stride&lt; _0, Stride&lt; _1, _64&gt;&gt; // Thr stride = 0 !&gt;;// offset = thread_idx * 0 + val_m * 1 + val_k * 64// = val_m + val_k * 64 (与 thread 无关!) 6.2 Diagram12345678910 Val: (0,0) (1,0) (2,0) ... (0,1) (1,1) ... +-----+-----+-----+---+-----+-----+---+ Thr 0 | 0 | 1 | 2 |...| 64 | 65 |...| +-----+-----+-----+---+-----+-----+---+ Thr 1 | 0 | 1 | 2 |...| 64 | 65 |...| &lt;- same! +-----+-----+-----+---+-----+-----+---+ Thr 2 | 0 | 1 | 2 |...| 64 | 65 |...| &lt;- same! +-----+-----+-----+---+-----+-----+---+All threads see the same offset -&gt; access via same SMEM descriptor 7. TV Layout 的伪代码实现123456789101112131415161718// 多维 TV Layout 函数int tv_layout(int thread_id, int value_id) { // Step 1: 分解 thread_id 到多维坐标 (t0, t1, t2) int t0 = (thread_id / 1) % 4; int t1 = (thread_id / 4) % 8; int t2 = (thread_id / 32) % 4; // Step 2: 分解 value_id 到多维坐标 (v0, v1, v2) int v0 = (value_id / 1) % 2; int v1 = (value_id / 2) % 2; int v2 = (value_id / 4) % 2; // Step 3: 计算 offset int thr_offset = t0 * 128 + t1 * 1 + t2 * 16; int val_offset = v0 * 64 + v1 * 8 + v2 * 512; return thr_offset + val_offset;} 8. CuTe 中的 Layout 调用1234567891011auto layout = ALayout_64x16{};// 方式 1: 直接调用int offset = layout(thread_id, value_id);// 方式 2: 分开调用int offset = layout(make_coord(t0, t1, t2), make_coord(v0, v1, v2));// 方式 3: 获取特定线程的子 layoutauto thr_layout = layout(thread_id, _); // 固定 thread，返回 Val 的 layoutint offset = thr_layout(value_id); 9. 总结1234567891011121314TV Layout 核心理解: Layout = 坐标到整数的映射函数 (i, j, k, ...) --&gt; offsetTV Layout: (thread_id, value_id) --&gt; offset Thr Slice: 决定&quot;从哪开始&quot; Val Slice: 决定&quot;怎么分布&quot; 最终 offset = 起始 + 相对偏移 特性 SS 模式 RS 模式 Thr Stride 0 ≠ 0 每线程数据 共享整块 SMEM 各自不同位置 数据位置 SMEM Register 复杂度 低 高","link":"/2024/12/24/0x01-tv-layout-guide/"},{"title":"0x03 ABLayout vs ALayout_64x16 - SS and RS Mode TV Layout","text":"This article compares two TV Layout types for WGMMA operand access: ABLayout (SS mode) vs ALayout_64x16 (RS mode). 示例代码: 0x03_ablayout_vs_alayout.cu 1. IntroductionIn CUTLASS WGMMA (Warpgroup Matrix Multiply-Accumulate), there are two operand access modes: SS mode: A and B both in Shared Memory, uses ABLayout RS mode: A in Register, B in Shared Memory, uses ALayout_64x16 This article compares these two TV Layout structures, mapping functions, and semantic differences. 2. Layout 定义2.1 ABLayout (SS 模式)1234// Shared memory source layouts for any value typetemplate &lt;int M, int K&gt;using ABLayout = Layout&lt;Shape &lt;_128, Shape &lt;Int&lt;M&gt;, Int&lt;K&gt;&gt;&gt;, Stride&lt; _0, Stride&lt; _1, Int&lt;M&gt;&gt;&gt;&gt;; 2.2 ALayout_64x16 (RS 模式)123// Register source layout for 16-bit (sparse 32-bit) value typesusing ALayout_64x16 = Layout&lt;Shape &lt;Shape &lt; _4, _8, _4&gt;, Shape &lt; _2, _2, _2&gt;&gt;, Stride&lt;Stride&lt;_128, _1, _16&gt;, Stride&lt;_64, _8, _512&gt;&gt;&gt;; 3. ABLayout 详解3.1 实例化 ABLayout&lt;64, 16&gt;1234ABLayout&lt;64, 16&gt; = Layout&lt; Shape &lt;_128, Shape &lt;_64, _16&gt;&gt;, // (Thr, (Val_M, Val_K)) Stride&lt; _0, Stride&lt; _1, _64&gt;&gt; // (Thr_stride, (Val_M_stride, Val_K_stride))&gt;; 3.2 维度分解 维度 Shape Stride 含义 Thr 128 0 128 个线程，stride=0 Val_M 64 1 M 方向 64 行 Val_K 16 64 K 方向 16 列 3.3 映射函数TV Layout 本质是一个函数： $$f: (\\text{thread_id}, \\text{value_id}) \\rightarrow \\text{offset}$$ 对于 ABLayout： $$\\text{offset}(\\text{thr}, v_m, v_k) = \\text{thr} \\times 0 + v_m \\times 1 + v_k \\times 64$$ 简化为： $$\\boxed{\\text{offset}(v_m, v_k) = v_m + v_k \\times 64}$$ 3.4 Thr Stride = 0 的含义这是 ABLayout 最关键的特性： 12345678910111213Thread Stride = 0 意味着: offset(thread_0, v_m, v_k) = v_m + v_k × 64 offset(thread_1, v_m, v_k) = v_m + v_k * 64 &lt;- same! offset(thread_2, v_m, v_k) = v_m + v_k * 64 &lt;- same! ... offset(thread_127, v_m, v_k) = v_m + v_k * 64 &lt;- same!+-------------------------------------------------------------+| All 128 threads compute the same offset! || || They share the same SMEM data block |+-------------------------------------------------------------+ 3.5 64x16 Matrix Offset Distribution1234567891011121314151617181964x16 Matrix (Column-Major, shared by all threads): k=0 k=1 k=2 ... k=15 +------+------+------+------+------+ m=0 | 0 | 64 | 128 | ... | 960 | +------+------+------+------+------+ m=1 | 1 | 65 | 129 | ... | 961 | +------+------+------+------+------+ m=2 | 2 | 66 | 130 | ... | 962 | +------+------+------+------+------+ ... | ... | ... | ... | ... | ... | +------+------+------+------+------+ m=63 | 63 | 127 | 191 | ... | 1023 | +------+------+------+------+------+ offset = m + k x 64 Total 64 x 16 = 1024 elements All 128 threads can access the entire matrix 3.6 SS Mode Data Flow1234567891011121314+-------------------------------------------------------------+| SS Mode (ABLayout) |+-------------------------------------------------------------+| || GMEM --TMA--&gt; SMEM --descriptor--&gt; WGMMA Hardware || ^ || 64-bit descriptor || points to SMEM start address || ^ || shared by all 128 threads || || Feature: data stays in SMEM, HW reads directly, no load || |+-------------------------------------------------------------+ 4. ALayout_64x16 详解4.1 结构分解1234ALayout_64x16 = Layout&lt; Shape &lt;Shape &lt; _4, _8, _4&gt;, Shape &lt; _2, _2, _2&gt;&gt;, Stride&lt;Stride&lt;_128, _1, _16&gt;, Stride&lt;_64, _8, _512&gt;&gt;&gt;; 部分 Shape Stride 元素数 Thr (4, 8, 4) (128, 1, 16) 4×8×4 = 128 Val (2, 2, 2) (64, 8, 512) 2×2×2 = 8 总元素数: 128 线程 × 8 值/线程 = 1024 = 64 × 16 ✓ 4.2 坐标展开公式Thread ID → (t₀, t₁, t₂): $$t_0 = \\text{thread_id} \\mod 4$$$$t_1 = \\lfloor \\text{thread_id} / 4 \\rfloor \\mod 8$$$$t_2 = \\lfloor \\text{thread_id} / 32 \\rfloor \\mod 4$$ Value ID → (v₀, v₁, v₂): $$v_0 = \\text{value_id} \\mod 2$$$$v_1 = \\lfloor \\text{value_id} / 2 \\rfloor \\mod 2$$$$v_2 = \\lfloor \\text{value_id} / 4 \\rfloor \\mod 2$$ 4.3 映射函数$$\\text{offset} = \\underbrace{t_0 \\times 128 + t_1 \\times 1 + t_2 \\times 16}{\\text{thr_offset}} + \\underbrace{v_0 \\times 64 + v_1 \\times 8 + v_2 \\times 512}{\\text{val_offset}}$$ 4.4 Thread 0 的 8 个 ValueThread 0: $t_0=0, t_1=0, t_2=0$ → thr_offset = 0 v_idx (v₀, v₁, v₂) val_offset offset m k 0 (0, 0, 0) 0 0 0 0 1 (1, 0, 0) 64 64 0 1 2 (0, 1, 0) 8 8 8 0 3 (1, 1, 0) 72 72 8 1 4 (0, 0, 1) 512 512 0 8 5 (1, 0, 1) 576 576 0 9 6 (0, 1, 1) 520 520 8 8 7 (1, 1, 1) 584 584 8 9 坐标转换: m = offset % 64, k = offset / 64 4.5 Thread 0 在 64×16 矩阵中的位置123456789101112131415161718192064x16 matrix (Thread 0's 8 positions marked with *): k=0 k=1 k=8 k=9 +---------+---------+ +---------+---------+ m=0 | V0 * | V1 * | ... | V4 * | V5 * | | off=0 | off=64 | | off=512 | off=576 | +---------+---------+ +---------+---------+ m=1 | | | | | | +---------+---------+ +---------+---------+ ... | | | | | | +---------+---------+ +---------+---------+ m=8 | V2 * | V3 * | ... | V6 * | V7 * | | off=8 | off=72 | | off=520 | off=584 | +---------+---------+ +---------+---------+ ... | | | | | | +---------+---------+ +---------+---------+Thread 0 owns: - 4 positions at row m=0: (0,0), (0,1), (0,8), (0,9) - 4 positions at row m=8: (8,0), (8,1), (8,8), (8,9) 4.6 Thread 1 的 8 个 ValueThread 1: $t_0=1, t_1=0, t_2=0$ → thr_offset = 1×128 = 128 v_idx val_offset offset m k 0 0 128 0 2 1 64 192 0 3 2 8 136 8 2 3 72 200 8 3 4 512 640 0 10 5 576 704 0 11 6 520 648 8 10 7 584 712 8 11 4.7 完整线程分布图1234567891011121314151617181964x16 matrix (showing which Thread owns each position): k: 0 1 | 2 3 | 4 5 | 6 7 || 8 9 |10 11 |12 13 |14 15 +---------+---------+---------+---------++---------+---------+---------+---------+ m=0 | T0 T0 | T1 T1 | T2 T2 | T3 T3 || T0 T0 | T1 T1 | T2 T2 | T3 T3 | +---------+---------+---------+---------++---------+---------+---------+---------+ m=1 | T4 T4 | T5 T5 | T6 T6 | T7 T7 || T4 T4 | T5 T5 | T6 T6 | T7 T7 | +---------+---------+---------+---------++---------+---------+---------+---------+ ... | ... | ... | ... | ... || ... | ... | ... | ... | +---------+---------+---------+---------++---------+---------+---------+---------+ m=8 | T0 T0 | T1 T1 | T2 T2 | T3 T3 || T0 T0 | T1 T1 | T2 T2 | T3 T3 | +---------+---------+---------+---------++---------+---------+---------+---------+ m=9 | T4 T4 | T5 T5 | T6 T6 | T7 T7 || T4 T4 | T5 T5 | T6 T6 | T7 T7 | +---------+---------+---------+---------++---------+---------+---------+---------+Pattern: - k direction: T0-T3 alternate, each thread covers 2 columns - m direction: same thread repeats every 8 rows - each thread's 8 values spread across 4 2x2 blocks 4.8 RS Mode Data Flow123456789101112131415+-------------------------------------------------------------+| RS Mode (ALayout_64x16) |+-------------------------------------------------------------+| || GMEM --TMA--&gt; SMEM --ldmatrix--&gt; Register --&gt; WGMMA HW || ^ ^ || each thread each thread 8 values || loads its 8 stored in register || ^ || ALayout_64x16 specifies || which positions each thread owns || || Feature: data needs to be loaded from SMEM to Register || |+-------------------------------------------------------------+ 5. 核心对比5.1 公式对比 Layout 映射函数 ABLayout&lt;64,16&gt; $\\text{offset} = v_m + v_k \\times 64$ ALayout_64x16 $\\text{offset} = (t_0 \\times 128 + t_1 + t_2 \\times 16) + (v_0 \\times 64 + v_1 \\times 8 + v_2 \\times 512)$ 5.2 Thr Stride 对比 Layout Thr Stride 含义 ABLayout 0 所有线程共享相同数据 ALayout_64x16 (128, 1, 16) ≠ 0 每线程负责不同位置 5.3 数据位置对比1234567891011121314151617181920212223242526272829+-------------------------------------------------------------+| ABLayout (SS) |+-------------------------------------------------------------+| || Thread 0 --&gt; SMEM descriptor --&gt; access entire 64x16 || Thread 1 --&gt; SMEM descriptor --&gt; access entire 64x16 || Thread 2 --&gt; SMEM descriptor --&gt; access entire 64x16 || ... || Thread 127 --&gt; SMEM descriptor --&gt; access entire 64x16 || || Data location: SMEM (no movement) || Engine: smem_desc&lt;T&gt; (64-bit descriptor) || |+-------------------------------------------------------------++-------------------------------------------------------------+| ALayout_64x16 (RS) |+-------------------------------------------------------------+| || Thread 0 --&gt; Register[0..7] --&gt; pos (0,0)(0,1)(8,0)... || Thread 1 --&gt; Register[0..7] --&gt; pos (0,2)(0,3)(8,2)... || Thread 2 --&gt; Register[0..7] --&gt; pos (0,4)(0,5)(8,4)... || ... || Thread 127 --&gt; Register[0..7] --&gt; different 8 positions || || Data location: Register (needs SMEM load) || Engine: ArrayEngine&lt;uint32_t, 4&gt; || |+-------------------------------------------------------------+ 5.4 Summary Diagram123456789101112131415161718192021222324 ABLayout (SS) ALayout_64x16 (RS) +-------------+ +-------------+ | | | |Thr Stride | 0 | | != 0 | | | | | +------+------+ +------+------+ | | v v +----------------+ +----------------+Thread View | all threads | | each thread | | see same off | | sees diff off | +-------+--------+ +-------+--------+ | | v v +----------------+ +----------------+Data Location | SMEM | | Register | | (descriptor) | | (per-thread) | +-------+--------+ +-------+--------+ | | v v +----------------+ +----------------+Engine | smem_desc&lt;T&gt; | | ArrayEngine | | (64-bit) | | &lt;T, 8&gt; | +----------------+ +----------------+ 6. 为什么需要两种模式？6.1 SS 模式的优势 简单: 数据不移动，硬件直接读 SMEM 省寄存器: 不需要额外寄存器存储 A 矩阵 适合大矩阵: SMEM 容量大 6.2 RS 模式的优势 复用寄存器数据: A 矩阵在 register 中可多次使用 减少 SMEM 访问: 一次 load，多次 MMA 适合 K 维度循环: 同一份 A 数据参与多个 K 步的计算 6.3 选择建议 场景 推荐模式 A 只用一次 SS A 需要复用 RS 寄存器紧张 SS SMEM 带宽瓶颈 RS 7. 总结 特性 ABLayout (SS) ALayout_64x16 (RS) Thr Stride 0 ≠ 0 线程数据 所有线程共享 每线程独立 8 个值 数据位置 SMEM Register Engine 类型 smem_desc ArrayEngine 数据移动 无 SMEM → Register 映射函数 offset = m + k×M 复杂多维映射 核心理解： ABLayout 的 Thr Stride = 0 意味着所有线程共享同一块 SMEM，是 WGMMA SS 模式的布局描述 ALayout_64x16 的 Thr Stride ≠ 0 意味着每个线程负责矩阵的不同位置，需要从 SMEM load 到 Register，是 WGMMA RS 模式的布局描述","link":"/2024/12/24/0x03-ablayout-vs-alayout/"},{"title":"0x04 CUTLASS Atom MMA (SS&#x2F;RS Mode)","text":"This article explains CUTLASS Atom MMA SS and RS modes, including WGMMA instruction parameters and MMA_Traits definitions. 示例代码: 0x04_atom_mma_ss_rs.cu 1. MMA SS 模式Atom MMA的一个例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// GMMA 64x256x16 F16+=F16*F16template &lt; GMMA::Major tnspA, GMMA::Major tnspB, GMMA::ScaleIn scaleA = GMMA::ScaleIn::One, GMMA::ScaleIn scaleB = GMMA::ScaleIn::One&gt;struct MMA_64x256x16_F16F16F16_SS{ using DRegisters = void; using ARegisters = uint64_t[1]; using BRegisters = uint64_t[1]; using CRegisters = uint32_t[64]; CUTE_HOST_DEVICE static void fma(uint64_t const&amp; desc_a, uint64_t const&amp; desc_b, uint32_t &amp; d00, uint32_t &amp; d01, uint32_t &amp; d02, uint32_t &amp; d03, uint32_t &amp; d04, uint32_t &amp; d05, uint32_t &amp; d06, uint32_t &amp; d07, uint32_t &amp; d08, uint32_t &amp; d09, uint32_t &amp; d10, uint32_t &amp; d11, uint32_t &amp; d12, uint32_t &amp; d13, uint32_t &amp; d14, uint32_t &amp; d15, uint32_t &amp; d16, uint32_t &amp; d17, uint32_t &amp; d18, uint32_t &amp; d19, uint32_t &amp; d20, uint32_t &amp; d21, uint32_t &amp; d22, uint32_t &amp; d23, uint32_t &amp; d24, uint32_t &amp; d25, uint32_t &amp; d26, uint32_t &amp; d27, uint32_t &amp; d28, uint32_t &amp; d29, uint32_t &amp; d30, uint32_t &amp; d31, uint32_t &amp; d32, uint32_t &amp; d33, uint32_t &amp; d34, uint32_t &amp; d35, uint32_t &amp; d36, uint32_t &amp; d37, uint32_t &amp; d38, uint32_t &amp; d39, uint32_t &amp; d40, uint32_t &amp; d41, uint32_t &amp; d42, uint32_t &amp; d43, uint32_t &amp; d44, uint32_t &amp; d45, uint32_t &amp; d46, uint32_t &amp; d47, uint32_t &amp; d48, uint32_t &amp; d49, uint32_t &amp; d50, uint32_t &amp; d51, uint32_t &amp; d52, uint32_t &amp; d53, uint32_t &amp; d54, uint32_t &amp; d55, uint32_t &amp; d56, uint32_t &amp; d57, uint32_t &amp; d58, uint32_t &amp; d59, uint32_t &amp; d60, uint32_t &amp; d61, uint32_t &amp; d62, uint32_t &amp; d63, GMMA::ScaleOut const scale_D = GMMA::ScaleOut::One) {#if defined(CUTE_ARCH_MMA_SM90A_ENABLED) cutlass::arch::synclog_emit_wgmma_smem_smem(__LINE__, desc_a, desc_b); asm volatile( &quot;{\\n&quot; &quot;.reg .pred p;\\n&quot; &quot;setp.ne.b32 p, %66, 0;\\n&quot; &quot;wgmma.mma_async.sync.aligned.m64n256k16.f16.f16.f16 &quot; &quot;{%0, %1, %2, %3, %4, %5, %6, %7, &quot; &quot; %8, %9, %10, %11, %12, %13, %14, %15, &quot; &quot; %16, %17, %18, %19, %20, %21, %22, %23, &quot; &quot; %24, %25, %26, %27, %28, %29, %30, %31, &quot; &quot; %32, %33, %34, %35, %36, %37, %38, %39, &quot; &quot; %40, %41, %42, %43, %44, %45, %46, %47, &quot; &quot; %48, %49, %50, %51, %52, %53, %54, %55, &quot; &quot; %56, %57, %58, %59, %60, %61, %62, %63},&quot; &quot; %64,&quot; &quot; %65,&quot; &quot; p, %67, %68, %69, %70;\\n&quot; &quot;}\\n&quot; : &quot;+r&quot;(d00), &quot;+r&quot;(d01), &quot;+r&quot;(d02), &quot;+r&quot;(d03), &quot;+r&quot;(d04), &quot;+r&quot;(d05), &quot;+r&quot;(d06), &quot;+r&quot;(d07), &quot;+r&quot;(d08), &quot;+r&quot;(d09), &quot;+r&quot;(d10), &quot;+r&quot;(d11), &quot;+r&quot;(d12), &quot;+r&quot;(d13), &quot;+r&quot;(d14), &quot;+r&quot;(d15), &quot;+r&quot;(d16), &quot;+r&quot;(d17), &quot;+r&quot;(d18), &quot;+r&quot;(d19), &quot;+r&quot;(d20), &quot;+r&quot;(d21), &quot;+r&quot;(d22), &quot;+r&quot;(d23), &quot;+r&quot;(d24), &quot;+r&quot;(d25), &quot;+r&quot;(d26), &quot;+r&quot;(d27), &quot;+r&quot;(d28), &quot;+r&quot;(d29), &quot;+r&quot;(d30), &quot;+r&quot;(d31), &quot;+r&quot;(d32), &quot;+r&quot;(d33), &quot;+r&quot;(d34), &quot;+r&quot;(d35), &quot;+r&quot;(d36), &quot;+r&quot;(d37), &quot;+r&quot;(d38), &quot;+r&quot;(d39), &quot;+r&quot;(d40), &quot;+r&quot;(d41), &quot;+r&quot;(d42), &quot;+r&quot;(d43), &quot;+r&quot;(d44), &quot;+r&quot;(d45), &quot;+r&quot;(d46), &quot;+r&quot;(d47), &quot;+r&quot;(d48), &quot;+r&quot;(d49), &quot;+r&quot;(d50), &quot;+r&quot;(d51), &quot;+r&quot;(d52), &quot;+r&quot;(d53), &quot;+r&quot;(d54), &quot;+r&quot;(d55), &quot;+r&quot;(d56), &quot;+r&quot;(d57), &quot;+r&quot;(d58), &quot;+r&quot;(d59), &quot;+r&quot;(d60), &quot;+r&quot;(d61), &quot;+r&quot;(d62), &quot;+r&quot;(d63) : &quot;l&quot;(desc_a), &quot;l&quot;(desc_b), &quot;r&quot;(int32_t(scale_D)), &quot;n&quot;(int32_t(scaleA)), &quot;n&quot;(int32_t(scaleB)), &quot;n&quot;(int32_t(tnspA)), &quot;n&quot;(int32_t(tnspB)));#else CUTE_INVALID_CONTROL_PATH(&quot;Attempting to use MMA_64x256x16_F16F16F16_SS without CUTE_ARCH_MMA_SM90A_ENABLED&quot;);#endif }}; 可以看到这里存在输入是scaleA和scaleB，类型是GMMA::ScaleIn，这里的取值可以是1或者-1。 1234enum class ScaleIn { Neg = -1, One = 1}; 另外可以看到fma函数中的输入参数为: desc_a: TMA descriptor desc_b: TMA descriptor d矩阵的0-63的register d矩阵的ScaleOut 其中ScaleOut取值为： 1234enum class ScaleOut { Zero = 0, One = 1}; 可以简单认为，第一次MMA选Zero，后续都要配置为One。 MMA RS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081// GMMA 64x256x16 F16+=F16*F16template &lt; GMMA::Major tnspA, GMMA::Major tnspB, GMMA::ScaleIn scaleA = GMMA::ScaleIn::One, GMMA::ScaleIn scaleB = GMMA::ScaleIn::One&gt;struct MMA_64x256x16_F16F16F16_RS{ using DRegisters = void; using ARegisters = uint32_t[4]; using BRegisters = uint64_t[1]; using CRegisters = uint32_t[64]; static_assert(tnspA == GMMA::Major::K, &quot;Register source operand A must have K major layout.&quot;); CUTE_HOST_DEVICE static void fma(uint32_t const&amp; a00, uint32_t const&amp; a01, uint32_t const&amp; a02, uint32_t const&amp; a03, uint64_t const&amp; desc_b, uint32_t &amp; d00, uint32_t &amp; d01, uint32_t &amp; d02, uint32_t &amp; d03, uint32_t &amp; d04, uint32_t &amp; d05, uint32_t &amp; d06, uint32_t &amp; d07, uint32_t &amp; d08, uint32_t &amp; d09, uint32_t &amp; d10, uint32_t &amp; d11, uint32_t &amp; d12, uint32_t &amp; d13, uint32_t &amp; d14, uint32_t &amp; d15, uint32_t &amp; d16, uint32_t &amp; d17, uint32_t &amp; d18, uint32_t &amp; d19, uint32_t &amp; d20, uint32_t &amp; d21, uint32_t &amp; d22, uint32_t &amp; d23, uint32_t &amp; d24, uint32_t &amp; d25, uint32_t &amp; d26, uint32_t &amp; d27, uint32_t &amp; d28, uint32_t &amp; d29, uint32_t &amp; d30, uint32_t &amp; d31, uint32_t &amp; d32, uint32_t &amp; d33, uint32_t &amp; d34, uint32_t &amp; d35, uint32_t &amp; d36, uint32_t &amp; d37, uint32_t &amp; d38, uint32_t &amp; d39, uint32_t &amp; d40, uint32_t &amp; d41, uint32_t &amp; d42, uint32_t &amp; d43, uint32_t &amp; d44, uint32_t &amp; d45, uint32_t &amp; d46, uint32_t &amp; d47, uint32_t &amp; d48, uint32_t &amp; d49, uint32_t &amp; d50, uint32_t &amp; d51, uint32_t &amp; d52, uint32_t &amp; d53, uint32_t &amp; d54, uint32_t &amp; d55, uint32_t &amp; d56, uint32_t &amp; d57, uint32_t &amp; d58, uint32_t &amp; d59, uint32_t &amp; d60, uint32_t &amp; d61, uint32_t &amp; d62, uint32_t &amp; d63, GMMA::ScaleOut const scale_D = GMMA::ScaleOut::One) {#if defined(CUTE_ARCH_MMA_SM90A_ENABLED) cutlass::arch::synclog_emit_wgmma_reg_smem(__LINE__, desc_b); asm volatile( &quot;{\\n&quot; &quot;.reg .pred p;\\n&quot; &quot;setp.ne.b32 p, %69, 0;\\n&quot; &quot;wgmma.mma_async.sync.aligned.m64n256k16.f16.f16.f16 &quot; &quot;{%0, %1, %2, %3, %4, %5, %6, %7, &quot; &quot; %8, %9, %10, %11, %12, %13, %14, %15, &quot; &quot; %16, %17, %18, %19, %20, %21, %22, %23, &quot; &quot; %24, %25, %26, %27, %28, %29, %30, %31, &quot; &quot; %32, %33, %34, %35, %36, %37, %38, %39, &quot; &quot; %40, %41, %42, %43, %44, %45, %46, %47, &quot; &quot; %48, %49, %50, %51, %52, %53, %54, %55, &quot; &quot; %56, %57, %58, %59, %60, %61, %62, %63},&quot; &quot;{%64, %65, %66, %67},&quot; &quot; %68,&quot; &quot; p, %70, %71, %72;\\n&quot; &quot;}\\n&quot; : &quot;+r&quot;(d00), &quot;+r&quot;(d01), &quot;+r&quot;(d02), &quot;+r&quot;(d03), &quot;+r&quot;(d04), &quot;+r&quot;(d05), &quot;+r&quot;(d06), &quot;+r&quot;(d07), &quot;+r&quot;(d08), &quot;+r&quot;(d09), &quot;+r&quot;(d10), &quot;+r&quot;(d11), &quot;+r&quot;(d12), &quot;+r&quot;(d13), &quot;+r&quot;(d14), &quot;+r&quot;(d15), &quot;+r&quot;(d16), &quot;+r&quot;(d17), &quot;+r&quot;(d18), &quot;+r&quot;(d19), &quot;+r&quot;(d20), &quot;+r&quot;(d21), &quot;+r&quot;(d22), &quot;+r&quot;(d23), &quot;+r&quot;(d24), &quot;+r&quot;(d25), &quot;+r&quot;(d26), &quot;+r&quot;(d27), &quot;+r&quot;(d28), &quot;+r&quot;(d29), &quot;+r&quot;(d30), &quot;+r&quot;(d31), &quot;+r&quot;(d32), &quot;+r&quot;(d33), &quot;+r&quot;(d34), &quot;+r&quot;(d35), &quot;+r&quot;(d36), &quot;+r&quot;(d37), &quot;+r&quot;(d38), &quot;+r&quot;(d39), &quot;+r&quot;(d40), &quot;+r&quot;(d41), &quot;+r&quot;(d42), &quot;+r&quot;(d43), &quot;+r&quot;(d44), &quot;+r&quot;(d45), &quot;+r&quot;(d46), &quot;+r&quot;(d47), &quot;+r&quot;(d48), &quot;+r&quot;(d49), &quot;+r&quot;(d50), &quot;+r&quot;(d51), &quot;+r&quot;(d52), &quot;+r&quot;(d53), &quot;+r&quot;(d54), &quot;+r&quot;(d55), &quot;+r&quot;(d56), &quot;+r&quot;(d57), &quot;+r&quot;(d58), &quot;+r&quot;(d59), &quot;+r&quot;(d60), &quot;+r&quot;(d61), &quot;+r&quot;(d62), &quot;+r&quot;(d63) : &quot;r&quot;(a00), &quot;r&quot;(a01), &quot;r&quot;(a02), &quot;r&quot;(a03), &quot;l&quot;(desc_b), &quot;r&quot;(int32_t(scale_D)), &quot;n&quot;(int32_t(scaleA)), &quot;n&quot;(int32_t(scaleB)), &quot;n&quot;(int32_t(tnspB)));#else CUTE_INVALID_CONTROL_PATH(&quot;Attempting to use MMA_64x256x16_F16F16F16_RS without CUTE_ARCH_MMA_SM90A_ENABLED&quot;);#endif }}; 这里多出了4个uint32的a的register，这是因为A矩阵是64x16，对于一个warpgroup，也就是128个thread来说，每个thread分配到了8个fp16的输入，也就是4个uint32的输入的register。输出是64x256，对于128个thread来说，每个thread是128个fp16，也就是64个uint32的d的register。 这两个MMA的Traits分别是： 123456789101112131415161718192021222324252627template &lt; GMMA::Major tnspA, GMMA::Major tnspB, GMMA::ScaleIn scaleA = GMMA::ScaleIn::One, GMMA::ScaleIn scaleB = GMMA::ScaleIn::One&gt;using SM90_64x256x16_F16F16F16_SS = SM90::GMMA::MMA_64x256x16_F16F16F16_SS&lt;tnspA, tnspB, scaleA, scaleB&gt;;template &lt;GMMA::Major tnspA, GMMA::Major tnspB, GMMA::ScaleIn scaleA, GMMA::ScaleIn scaleB&gt;struct MMA_Traits&lt;SM90_64x256x16_F16F16F16_SS&lt;tnspA, tnspB, scaleA, scaleB&gt;&gt;{ using ValTypeD = half_t; using ValTypeA = half_t; using ValTypeB = half_t; using ValTypeC = half_t; using FrgTypeA = GMMA::smem_desc&lt;tnspA&gt;; using FrgTypeB = GMMA::smem_desc&lt;tnspB&gt;; using Shape_MNK = Shape&lt;_64,_256,_16&gt;; using ThrID = Layout&lt;_128&gt;; using ALayout = GMMA::ABLayout&lt; 64, 16&gt;; using BLayout = GMMA::ABLayout&lt;256, 16&gt;; using CLayout = GMMA::CLayout_64x256; GMMA::ScaleOut accumulate_ = GMMA::ScaleOut::One;}; 1234567891011121314151617181920212223242526template &lt; GMMA::Major tnspA, GMMA::Major tnspB, GMMA::ScaleIn scaleA = GMMA::ScaleIn::One, GMMA::ScaleIn scaleB = GMMA::ScaleIn::One&gt;using SM90_64x256x16_F16F16F16_RS = SM90::GMMA::MMA_64x256x16_F16F16F16_RS&lt;tnspA, tnspB, scaleA, scaleB&gt;;template &lt;GMMA::Major tnspA, GMMA::Major tnspB, GMMA::ScaleIn scaleA, GMMA::ScaleIn scaleB&gt;struct MMA_Traits&lt;SM90_64x256x16_F16F16F16_RS&lt;tnspA, tnspB, scaleA, scaleB&gt;&gt;{ using ValTypeD = half_t; using ValTypeA = half_t; using ValTypeB = half_t; using ValTypeC = half_t; using FrgTypeB = GMMA::smem_desc&lt;tnspB&gt;; using Shape_MNK = Shape&lt;_64,_256,_16&gt;; using ThrID = Layout&lt;_128&gt;; using ALayout = GMMA::ALayout_64x16; using BLayout = GMMA::ABLayout&lt;256, 16&gt;; using CLayout = GMMA::CLayout_64x256; GMMA::ScaleOut accumulate_ = GMMA::ScaleOut::One;};","link":"/2024/12/24/0x04-atom-mma-ss-rs/"},{"title":"0x05 CUTLASS make_tiled_mma and MMA_Atom","text":"This article explains make_tiled_mma function and MMA_Atom class implementation, understanding how to build TiledMMA from Atom MMA. 示例代码: 0x05_make_tiled_mma.cu 1. make_tiled_mma 函数12345678910111213141516171819202122232425262728293031323334353637383940//// These tile the MMA_Atom as a whole//template &lt;class MMA_Op, class MMAThrLayout = Layout&lt;Shape&lt;_1,_1,_1&gt;&gt;, class Permutations = Tile&lt;Underscore,Underscore,Underscore&gt;&gt;CUTE_HOST_DEVICE constexprautomake_tiled_mma(MMA_Atom&lt;MMA_Op&gt; const&amp; mma_atom, MMAThrLayout const&amp; thr_layout = {}, Permutations const&amp; permutations = {}){ // append&lt;3&gt;: 确保 layout 有 3 个维度 // 如果不足 3 维，用 Layout&lt;_1,_0&gt;{} 填充 // ═══════════════════════════════════════════════════════════════════════════ // Layout&lt;_1, _0&gt; 含义 // ═══════════════════════════════════════════════════════════════════════════ auto thr_layout_mnk = append&lt;3&gt;(thr_layout, Layout&lt;_1,_0&gt;{}); auto permutation_mnk = append&lt;3&gt;(permutations, _); return TiledMMA&lt;MMA_Atom&lt;MMA_Op&gt;, decltype(thr_layout_mnk), decltype(permutation_mnk)&gt;{mma_atom, thr_layout_mnk};}template &lt;class MMA_Op, class MMAThrLayout = Layout&lt;Shape&lt;_1,_1,_1&gt;&gt;, class Permutations = Tile&lt;Underscore,Underscore,Underscore&gt;&gt;CUTE_HOST_DEVICE constexprautomake_tiled_mma(MMA_Op const&amp;, MMAThrLayout const&amp; thr_layout = {}, Permutations const&amp; permutations = {}){ // Attempt to wrap in an MMA_Atom&lt;&gt; and forward return make_tiled_mma(MMA_Atom&lt;MMA_Op&gt;{}, thr_layout, permutations);} 代码入口就是上面两种，第二种中间调用了第一种，只是为了额外支持单独的MMA_Op的输入，在内部裹成了MMA_Atom类型。输入进make_tiled_mma。 可以看到第一个代码内部需要将thr_layout转换成三维的layout。permutation也是一样。这里append&lt;3&gt;(thr_layout, Layout&lt;_1,_0&gt;{});后面的layout的1是shape，0是stride。 MMA_Atom class类的定义class的定义如下： 1234567891011template &lt;class... Args&gt;struct MMA_Atom;template &lt;class MMAOperation&gt;struct MMA_Atom&lt;MMAOperation&gt; : MMA_Atom&lt;MMA_Traits&lt;MMAOperation&gt;&gt;{};template &lt;class MMAOperation, class... Args&gt;struct MMA_Atom&lt;MMA_Traits&lt;MMAOperation, Args...&gt;&gt; : MMA_Traits&lt;MMAOperation, Args...&gt;{ 这里的类非常复杂，看最后这里的类定义，首先是一个full specialization，输入的类型是MMA_Traits&lt;MMAOperation, Args...&gt;, 这里对于MMA_Traits类用MMA_Operation 进行了实例化。然后还继承了这个traits类。这是一个常用的用template的参数class作为基类的例子。 这样，MMA_Atom就继承了Traits类，这样可以拿到Traits内部的很多MMA Atom的信息。 Type Alias然后是一堆Type Alias，把Traits类内部的参数用using的方式取了别名。 123456789101112131415161718// Element value types from the MMA_Traitsusing ValTypeD = typename Traits::ValTypeD;using ValTypeA = typename Traits::ValTypeA;using ValTypeB = typename Traits::ValTypeB;using ValTypeC = typename Traits::ValTypeC;// Thr-Val layouts from the MMA_Traitsusing Shape_MNK = typename Traits::Shape_MNK;using ThrID = typename Traits::ThrID;using LayoutC_TV = typename Traits::CLayout;using LayoutA_TV = typename Traits::ALayout;using LayoutB_TV = typename Traits::BLayout;// Fragment value types from the MMA_Traits (optional, defaults to Val type)using FrgTypeD = typename detail::FrgTypeC_or_Default&lt;Traits&gt;::type;using FrgTypeA = typename detail::FrgTypeA_or_Default&lt;Traits&gt;::type;using FrgTypeB = typename detail::FrgTypeB_or_Default&lt;Traits&gt;::type;using FrgTypeC = typename detail::FrgTypeC_or_Default&lt;Traits&gt;::type; With函数然后定义了一个with函数，用来对于MMA_Atom中的某些参数进行修改之后，返回一个新的类，这样就不需要重复构建atom类。其他的变种都可以用with接口进行重用。 12345678// Additional Trait parameters/transformationstemplate &lt;class... TraitsArgs&gt;CUTE_HOST_DEVICEautowith(TraitsArgs&amp;&amp;... args) const { auto traits = Traits::with(static_cast&lt;TraitsArgs&amp;&amp;&gt;(args)...); return MMA_Atom&lt;decltype(traits)&gt;{traits};} Call函数然后是定义了AtomMMA的调用函数: 1234567891011121314151617181920212223242526272829303132// Cast, check, and call fmatemplate &lt;class TD, class DLayout, class TA, class ALayout, class TB, class BLayout, class TC, class CLayout&gt;CUTE_HOST_DEVICE constexprvoidcall(Tensor&lt;TD, DLayout&gt; &amp; D, Tensor&lt;TA, ALayout&gt; const&amp; A, Tensor&lt;TB, BLayout&gt; const&amp; B, Tensor&lt;TC, CLayout&gt; const&amp; C) const{ static_assert(DLayout::rank == 1, &quot;Expected rank-1 D tensor&quot;); static_assert(ALayout::rank == 1, &quot;Expected rank-1 A tensor&quot;); static_assert(BLayout::rank == 1, &quot;Expected rank-1 B tensor&quot;); static_assert(CLayout::rank == 1, &quot;Expected rank-1 C tensor&quot;); return mma_unpack(static_cast&lt;Traits const&amp;&gt;(*this), D, A, B, C);}// Three arguments reproduces Ctemplate &lt;class TA, class ALayout, class TB, class BLayout, class TC, class CLayout&gt;CUTE_HOST_DEVICE constexprvoidcall(Tensor&lt;TA, ALayout&gt; const&amp; A, Tensor&lt;TB, BLayout&gt; const&amp; B, Tensor&lt;TC, CLayout&gt; &amp; C) const{ return call(C, A, B, C);} 注意上面的两种variation。 构建fragment A/B/C然后是对于fragmentA和fragmentB以及fragmentC的构建函数。","link":"/2024/12/24/0x05-make-tiled-mma/"},{"title":"0x02 elect_one_sync and Warp-Level Leader Election","text":"This article compares elect.sync instruction with the traditional threadIdx % 32 == 0 method for warp-level leader election. 示例代码: 0x02_elect_one_sync.cu 1. IntroductionIn CUDA programming, we often need to select a “leader” thread in a warp to execute certain operations (like barrier arrive, initialization, etc.). This article compares elect.sync instruction with the traditional threadIdx % 32 == 0 method. 2. 两种方法对比12345// 方法 1: 简单判断 (可能有问题!)bool is_leader = (threadIdx.x % 32) == 0;// 方法 2: elect.sync (SM90+, 更健壮!)bool is_leader = cute::elect_one_sync(); 3. 核心问题：部分线程不活跃3.1 问题场景1234567if (some_condition) { // 假设只有部分线程满足条件 // Lane 0 可能不在这里！ if (threadIdx.x % 32 == 0) { // ❌ Lane 0 不活跃，没人执行! do_something(); }} 3.2 图示12345678910Warp 32 threads:Lane: 0 1 2 3 4 5 6 7 ... 31 x o o x o o o x ... o | | | +-- active +------ inactive (Lane 0 not in this branch!)threadIdx % 32 == 0: no one satisfies!elect.sync: picks from active threads (e.g., Lane 1) 4. elect.sync PTX 指令4.1 CuTe 实现12345678910111213141516171819202122CUTE_HOST_DEVICE uint32_t elect_one_sync(){#if defined(CUTE_ARCH_ELECT_ONE_SM90_ENABLED) uint32_t pred = 0; uint32_t laneid = 0; asm volatile( &quot;{\\n&quot; &quot;.reg .b32 %%rx;\\n&quot; &quot;.reg .pred %%px;\\n&quot; &quot; elect.sync %%rx|%%px, %2;\\n&quot; // mask = 0xFFFFFFFF &quot;@%%px mov.s32 %1, 1;\\n&quot; // 被选中的线程设 pred = 1 &quot; mov.s32 %0, %%rx;\\n&quot; // 返回被选中的 lane id &quot;}\\n&quot; : &quot;+r&quot;(laneid), &quot;+r&quot;(pred) : &quot;r&quot;(0xFFFFFFFF)); return pred;#elif defined(__CUDA_ARCH__) return (threadIdx.x % 32) == 0; // Fallback#else return true;#endif} 4.2 输出含义 输出 含义 %%rx 被选中的 lane ID %%px 当前线程是否被选中 (predicate) 5. 对比示例场景 1：所有线程活跃1234567Lane: 0 1 2 3 ... 31活跃: ✓ ✓ ✓ ✓ ... ✓threadIdx % 32 == 0: Lane 0 执行 ✓elect.sync: Lane 0 执行 ✓ (选最小活跃 lane)结果相同 ✓ 场景 2：Lane 0 不活跃1234567Lane: 0 1 2 3 ... 31活跃: ✗ ✓ ✓ ✓ ... ✓threadIdx % 32 == 0: 没人执行! ✗elect.sync: Lane 1 执行 ✓ (选最小活跃 lane)elect.sync 更健壮! 场景 3：只有少数线程活跃1234567Lane: 0 1 2 3 4 5 ... 31活跃: ✗ ✗ ✗ ✗ ✓ ✓ ... ✗threadIdx % 32 == 0: 没人执行! ✗elect.sync: Lane 4 执行 ✓elect.sync 总能选出一个 leader! 6. 实际使用场景6.1 Barrier 初始化12345678void initialize_barrier_array_aligned(T ptr, int arv_cnt) { // 只需要一个线程执行初始化 if (cute::elect_one_sync()) { for (int i = 0; i &lt; Stages; i++) { ptr[i].init(arv_cnt); } }} 6.2 条件分支中的 Leader 选举123456789if (should_do_work) { // 这个分支里可能只有部分线程 // Lane 0 可能不在这里 if (cute::elect_one_sync()) { // 保证有一个线程执行 barrier.arrive(); }} 6.3 TMA 操作1234if (cute::elect_one_sync()) { // 只有一个线程发起 TMA tma_load_multicast(...);} 7. 额外好处：隐式同步123elect.sync %%rx|%%px, mask;// ↑// sync! 参与的线程会同步 elect.sync 自带 warp 同步，确保： 所有活跃线程到达这个点 选举结果对所有线程一致 避免竞争条件 8. Fallback 行为12345678910#if defined(CUTE_ARCH_ELECT_ONE_SM90_ENABLED) // SM90+: 使用 elect.sync 指令 asm volatile(&quot;elect.sync ...&quot;); return pred;#elif defined(__CUDA_ARCH__) // 旧架构: 没有 elect.sync，假设所有线程活跃 return (threadIdx.x % 32) == 0;#else return true;#endif 说明： elect.sync 是 SM90 (Hopper) 新增指令 旧架构只能用 threadIdx % 32 == 0 旧代码通常保证所有线程都活跃，所以可以工作 9. 常见使用模式9.1 Warp 级别选举1234// 每个 warp 选一个 leaderif (cute::elect_one_sync()) { // 一个 warp 一个线程执行} 9.2 Warpgroup 级别选举 (4 warps)1234// 只在 warp 0 中选举if (warp_idx == 0 &amp;&amp; cute::elect_one_sync()) { // 一个 warpgroup 一个线程执行} 9.3 Block 级别选举1234// 只在 warp 0 中选举if (threadIdx.x / 32 == 0 &amp;&amp; cute::elect_one_sync()) { // 一个 block 一个线程执行} 10. 性能考虑 方面 threadIdx % 32 == 0 elect.sync 指令数 1 条比较 ~3 条指令 同步 无 隐式 warp sync 健壮性 低 高 架构支持 所有 SM90+ 结论：性能差异微小，elect.sync 的健壮性优势远大于微小的性能开销。 11. 总结 特性 threadIdx % 32 == 0 elect.sync Lane 0 不活跃时 ❌ 没人执行 ✓ 选其他活跃线程 部分线程活跃 ❌ 可能没人执行 ✓ 总能选一个 隐式同步 ❌ 无 ✓ 有 架构要求 任意 SM90+ 适用场景 保证全活跃 通用 核心理解：elect.sync 从当前活跃线程中选举一个 leader，即使 Lane 0 不活跃也能正常工作，比硬编码 threadIdx % 32 == 0 更健壮。在 SM90+ 架构上应优先使用 elect_one_sync()。","link":"/2024/12/24/0x02-elect-one-sync/"},{"title":"0x06 CUTLASS mma_unpack Deep Dive","text":"This article explains SM90 GMMA specialized mma_unpack function, understanding how to unpack Tensor into WGMMA register parameters. 1. 函数签名123456789101112template &lt;class MMA_Op, class... MMA_Args, class TD, class DLayout, class TA, class ALayout, class TB, class BLayout, class TC, class CLayout&gt;CUTE_HOST_DEVICE constexprvoidmma_unpack(MMA_Traits&lt;MMA_Op, MMA_Args...&gt; const&amp; traits, Tensor&lt;TD, DLayout&gt; &amp; D, Tensor&lt;TA, ALayout&gt; const&amp; A, Tensor&lt;TB, BLayout&gt; const&amp; B, Tensor&lt;TC, CLayout&gt; const&amp; C) Step 1: 静态检查 - 必须是寄存器1234static_assert(is_rmem&lt;TD&gt;::value, &quot;Expected registers in MMA_Atom::call&quot;);static_assert(is_rmem&lt;TA&gt;::value, &quot;Expected registers in MMA_Atom::call&quot;);static_assert(is_rmem&lt;TB&gt;::value, &quot;Expected registers in MMA_Atom::call&quot;);static_assert(is_rmem&lt;TC&gt;::value, &quot;Expected registers in MMA_Atom::call&quot;); is_rmem = is register memory 确保所有 tensor 都在寄存器中（不是 GMEM/SMEM 指针） Step 2: 提取寄存器类型123using RegTypeA = typename remove_extent&lt;typename MMA_Op::ARegisters&gt;::type;using RegTypeB = typename remove_extent&lt;typename MMA_Op::BRegisters&gt;::type;using RegTypeC = typename remove_extent&lt;typename MMA_Op::CRegisters&gt;::type; MMA_Op 定义了寄存器数组类型： 寄存器 定义 remove_extent 后 ARegisters uint32_t[4] uint32_t BRegisters uint64_t[1] uint64_t CRegisters uint32_t[8] uint32_t Step 3: GMMA 特殊性 - C 和 D 必须相同1234static_assert(is_same&lt;typename TD::value_type, typename TC::value_type&gt;::value, &quot;GMMA C and D value_type must match.&quot;);static_assert(is_same&lt;DLayout, CLayout&gt;::value, &quot;GMMA C and D layouts must match.&quot;); 关键区别： 普通 MMA 四操作数：D = C + A × B SM90 GMMA 三操作数：D = D + A × B（原地累加） 因此 C 和 D 必须是同一个 tensor。 Step 4: Recast tensor 到寄存器类型123Tensor rA = recast&lt;RegTypeA&gt;(A); // 重新解释 A 为 uint32_t 数组Tensor rB = recast&lt;RegTypeB&gt;(B); // 重新解释 B 为 uint64_t 数组Tensor rC = recast&lt;RegTypeC&gt;(D); // 用 D (可变) 而非 C 注意：用 D 而非 C，因为 C 是 const，而 GMMA 需要原地修改。 Step 5: 获取寄存器数量并检查1234567constexpr int RegNumA = extent&lt;typename MMA_Op::ARegisters&gt;::value; // 4constexpr int RegNumB = extent&lt;typename MMA_Op::BRegisters&gt;::value; // 1constexpr int RegNumC = extent&lt;typename MMA_Op::CRegisters&gt;::value; // 8CUTE_STATIC_ASSERT_V(size(rA) == Int&lt;RegNumA&gt;{});CUTE_STATIC_ASSERT_V(size(rB) == Int&lt;RegNumB&gt;{});CUTE_STATIC_ASSERT_V(size(rC) == Int&lt;RegNumC&gt;{}); Tensor extent 含义 rA 4 4 个 32-bit 寄存器 rB 1 1 个 64-bit descriptor rC 8 8 个 32-bit 寄存器 Step 6: explode 展开并调用 fma12345detail::explode(MMA_Op::fma, rA, make_int_sequence&lt;RegNumA&gt;{}, rB, make_int_sequence&lt;RegNumB&gt;{}, rC, make_int_sequence&lt;RegNumC&gt;{}, &amp;(traits.accumulate_), seq&lt;0&gt;{}); explode 将 tensor 元素展开为函数参数： 123456789// 输入explode(fma, rA, &lt;0,1,2,3&gt;, rB, &lt;0&gt;, rC, &lt;0,1,2,3,4,5,6,7&gt;, &amp;scale, &lt;0&gt;)// 展开为fma(rA[0], rA[1], rA[2], rA[3], // A 的 4 个寄存器 rB[0], // B 的 1 个 descriptor rC[0], rC[1], rC[2], rC[3], // C 的 8 个寄存器 rC[4], rC[5], rC[6], rC[7], scale) // scale 参数 总结mma_unpack 的核心工作： 类型检查：确保数据在寄存器中 类型转换：recast 成底层寄存器类型 展开调用：explode 将 tensor 元素展开为 fma() 参数 执行指令：调用 PTX WGMMA 指令 注意：GMMA 是三操作数指令（D = D + A × B），所以 C 和 D 必须是同一个 tensor。","link":"/2024/12/24/0x06-mma-unpack/"},{"title":"0x07 CUTLASS ss_op_selector MMA Atom Selection","text":"This article explains ss_op_selector function, understanding how CUTLASS selects appropriate GMMA Atom based on data types and TileShape. 1. 简介这个函数用于选择 Atom MMA operation，调用在 MMA的CollectiveBuilder。 一些用这个function进行Atom MMA的选择的例子: ElementA ElementB TileShape GMMA Atom half half (128,128,64) SM90_64x128x16_SS half half (128,256,64) SM90_64x256x16_SS bf16 bf16 (128,128,64) SM90_64x128x16_SS tfloat32 tfloat32 (128,128,32) SM90_64x128x8_SS fp8_e4m3 fp8_e4m3 (128,128,128) SM90_64x128x32_SS 定义首先，看一下这个函数的定义（include/cute/arch/mma_sm90.hpp）： 12345678910111213template &lt;class ElementA,class ElementB,class ElementC,class TileShape_MNK,GMMA::Major MajorA = GMMA::Major::K,GMMA::Major MajorB = GMMA::Major::K,auto... Args // e.g. GMMA::ScaleOut::One, [GMMA::ScaleIn::One, GMMA::ScaleIn::One] // But most commonly leave empty for defaults&gt;CUTE_HOST_DEVICE constexprautoss_op_selector() 可以看到这个函数没有接受参数。template的parameter list中，有一个auto... Args，这个是什么意思： auto... Args 是 C++17 的可变参数 auto 语法，这个代表的是可变长的参数列表，但是这里的列表本身的内容还是编译期常量，只是可以处理不同长度的编译期常量列表。 一个简单的例子： 123456template &lt;auto... Values&gt;struct ValueList {};ValueList&lt;1, 2, 3&gt; // 三个 intValueList&lt;1, 'a', true&gt; // 混合: int, char, boolValueList&lt;&gt; // 空 其他的参数就是一些对于Atom MMA的类型进行选择的选项。包括A/B/C的数据格式，TileShape_MNK也就是CollectiveMma中单个stage计算的矩阵大小。MajorA和MajorB是A和B的cutlass语境下的layout格式。 代码走读 首先是一个assertion，对于gmma，M必须是64的整数倍。 1static_assert(size&lt;0&gt;(TileShape_MNK{}) % 64 == 0, &quot;Tile_M must be a multiple of 64.&quot;); 后续就是大量的conditon branch，来选择不同的Atom MMA的函数： 123456789101112131415161718auto Tile_N = size&lt;1&gt;(TileShape_MNK{}); // F16 accumulator if constexpr (is_same_v&lt;ElementC, half_t&gt;) { // Input A: half_t ; Input B: half_t if constexpr (is_same_v&lt;ElementA, half_t&gt; &amp;&amp; is_same_v&lt;ElementB, half_t&gt;) { static_assert(size&lt;2&gt;(TileShape_MNK{}) % 16 == 0, &quot;Tile_K must be a multiple of 16.&quot;); if constexpr (Tile_N % 256 == 0) { return SM90::GMMA::MMA_64x256x16_F16F16F16_SS&lt;MajorA, MajorB, Args...&gt;{}; }#if defined(CUTE_SM90_EXTENDED_MMA_SHAPES_ENABLED) else if constexpr (Tile_N % 248 == 0) { return SM90::GMMA::MMA_64x248x16_F16F16F16_SS&lt;MajorA, MajorB, Args...&gt;{}; }#endif... 可以看到这里同样有Args...，这里是把Arg参数解包传入下面的template进行实例化。","link":"/2024/12/24/0x07-ss-op-selector/"},{"title":"0x08 CuTe Tensor Engine Types","text":"This article explains CuTe Tensor Engine types: pointer Engine, ArrayEngine, and ViewEngine - their differences and use cases. 1. Tensor 模板结构12345678910template &lt;class Engine, class Layout&gt;struct Tensor { Engine engine_; // 数据存储/访问方式 Layout layout_; // 形状和步长};// TA 就是 Engine 类型Tensor&lt;TA, ALayout&gt;// ↑// Engine: 决定数据如何存储/访问 Engine 的三种主要类型1. 指针 Engine (指向外部内存)123456// 指向 GMEMTensor&lt;float*, Layout&gt; // 可写Tensor&lt;float const*, Layout&gt; // 只读// 指向 SMEMTensor&lt;smem_ptr&lt;half_t&gt;, Layout&gt; 2. ArrayEngine (寄存器存储)123456789// 数据存储在寄存器数组中Tensor&lt;ArrayEngine&lt;float, 32&gt;, Layout&gt;// ↑ ↑// 类型 数量// 使用auto tensor = make_tensor&lt;float&gt;(make_shape(4, 8)); // 32 个 float// tensor.data() 返回 ArrayEngine&lt;float, 32&gt;&amp;// 实际是 float[32] 在寄存器中 3. ViewEngine (视图/Descriptor)123456// 不拥有数据，只是视图Tensor&lt;ViewEngine&lt;Tensor&lt;...&gt;&gt;, Layout&gt;// SMEM Descriptor (WGMMA SS 模式)Tensor&lt;smem_desc&lt;half_t&gt;, Layout&gt;// 64-bit descriptor 指向 SMEM Engine 决定 .data() 返回什么12345678Tensor&lt;float*, Layout&gt; t1;t1.data(); // 返回 float*Tensor&lt;ArrayEngine&lt;float, 8&gt;, Layout&gt; t2;t2.data(); // 返回 float(&amp;)[8]，寄存器数组引用Tensor&lt;smem_desc&lt;half_t&gt;, Layout&gt; t3;t3.data(); // 返回 uint64_t，SMEM descriptor MMA 中的 Engine 类型累加器 (C/D)12345// 累加器在寄存器中using CRegisters = uint32_t[8]; // MMA_Op 定义Tensor&lt;ArrayEngine&lt;uint32_t, 8&gt;, Layout&gt; accum;// 8 个 32-bit 寄存器存储累加结果 SS 模式 (A/B 在 SMEM)123456// A, B 用 SMEM descriptorusing ARegisters = uint64_t[1]; // 1 个 64-bit descriptorTensor&lt;smem_desc&lt;half_t&gt;, Layout&gt; tCrA;// tCrA.data() 返回 64-bit SMEM descriptor// WGMMA 硬件直接从 descriptor 指向的 SMEM 读取 RS 模式 (A 在 Register)12345// A 需要加载到寄存器using ARegisters = uint32_t[4]; // 4 个 32-bit 寄存器Tensor&lt;ArrayEngine&lt;uint32_t, 4&gt;, Layout&gt; tCrA;// 数据从 SMEM 复制到这 4 个寄存器 Engine 类型 存储位置 .data() 返回 典型用途 T* GMEM/SMEM T* 指针 内存访问 ArrayEngine&lt;T,N&gt; Register T(&amp;)[N] 数组引用 累加器 smem_desc&lt;T&gt; - (描述 SMEM) uint64_t WGMMA SS 模式 ViewEngine&lt;...&gt; 取决于内部 视图 不拷贝数据 对比不同创建方式 12345678910111213141516// 1. 寄存器 Tensor (无指针参数)auto reg_tensor = make_tensor&lt;float&gt;(make_shape(4, 8));// Engine = ArrayEngine&lt;float, 32&gt;// 位置 = 寄存器/local memory (线程私有)// 2. GMEM Tensor (有指针参数)float* gmem_ptr = ...;auto gmem_tensor = make_tensor(gmem_ptr, make_shape(4, 8));// Engine = float*// 位置 = Global Memory (指针指向的位置)// 3. SMEM Tensor (有 SMEM 指针)__shared__ float smem[32];auto smem_tensor = make_tensor(make_smem_ptr(smem), make_shape(4, 8));// Engine = smem_ptr&lt;float&gt;// 位置 = Shared Memory","link":"/2024/12/24/0x08-tensor-engine-types/"},{"title":"0x09 CUTLASS SM90 Cooperative Kernel Pipeline","text":"This article explains CUTLASS SM90 Cooperative Kernel Pipeline mechanisms including Mainloop Pipeline, Epilogue Load/Store Pipeline, TileScheduler Pipeline, and LoadWarpOrderBarrier. 核心要点速览 Warp 分工：Producer WarpGroup 分为 Mainloop/Epilogue/Scheduler/Aux 四个角色 Mainloop Pipeline：PipelineTmaAsync，Producer TMA 加载，Consumer MMA 计算 Epilogue Load/Store Pipeline：独立的加载和存储同步 LoadWarpOrderBarrier：确保 Mainloop 先于 Epilogue 执行 无 cluster_sync：使用 cluster_arrive_relaxed + 延迟 cluster_wait 前置知识：Pipeline 与 mbarrier 深度解析、TMA Descriptor 深度解析 1. Warp Specialization 架构1.1 线程组织Cooperative Kernel 使用 Warp Specialization 技术，将线程分为不同角色： 123456789101112Thread Block (384 threads = 12 warps = 3 warp groups)+-- WarpGroup 0: Producer (128 threads = 4 warps)| +-- Warp 0: Mainloop Producer (TMA Load A/B)| +-- Warp 1: Scheduler Producer (CLC Query)| +-- Warp 2: Epilogue Producer (TMA Load C)| +-- Warp 3: MainloopAux Producer (auxiliary load)|+-- WarpGroup 1: Consumer0 (128 threads = 4 warps)| +-- Execute MMA compute + Epilogue Store|+-- WarpGroup 2: Consumer1 (128 threads = 4 warps) +-- Cooperate with Consumer0 on same tile 1.2 角色定义12345678910111213// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:366-376enum class WarpGroupRole { Producer = 0, Consumer0 = 1, Consumer1 = 2};enum class ProducerWarpRole { Mainloop = 0, // Warp 0: TMA Load A/B Warp1 = 1, // Warp 1: Scheduler (CLC query) Epilogue = 2, // Warp 2: TMA Load C MainloopAux = 3 // Warp 3: 辅助加载（可选）}; 1.3 寄存器动态分配12345// Producer: 低寄存器需求cutlass::arch::warpgroup_reg_dealloc&lt;LoadRegisterRequirement&gt;(); // 24-40 regs// Consumer: 高寄存器需求（累加器）cutlass::arch::warpgroup_reg_alloc&lt;MmaRegisterRequirement&gt;(); // 232-240 regs 2. Pipeline 类型总览Cooperative Kernel 中使用了 5 种 Pipeline： Pipeline 类型 Producer Consumer 用途 Mainloop Pipeline PipelineTmaAsync Mainloop Warp Consumer0/1 A/B tile 加载 Epilogue Load Pipeline PipelineTransactionAsync Epilogue Warp Consumer0/1 C 矩阵加载 Epilogue Store Pipeline PipelineTmaStore Consumer0/1 TMA Unit D 矩阵写回 Scheduler Pipeline PipelineAsync Scheduler Warp All Warps Tile 调度 LoadWarpOrderBarrier OrderedSequenceBarrier Mainloop Warp Epilogue Warp 加载顺序 2.1 Pipeline 存储结构1234567891011// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:148-157struct SharedStorage { struct PipelineStorage : cute::aligned_struct&lt;16, _1&gt; { MainloopPipelineStorage mainloop; // Mainloop barriers EpiLoadPipelineStorage epi_load; // Epilogue load barriers typename LoadWarpOrderBarrier::SharedStorage load_order; // 顺序控制 } pipelines; TileSchedulerStorage scheduler; // Scheduler barriers TensorStorage tensors; // SMEM 数据缓冲区}; 3. Mainloop Pipeline3.1 初始化123456789101112131415161718192021222324// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:444-458using MainloopPipeline = typename CollectiveMainloop::MainloopPipeline;typename MainloopPipeline::Params mainloop_pipeline_params;// Producer: Mainloop Warp 或 MainloopAux Warpif (warp_group_role == WarpGroupRole::Producer &amp;&amp; (producer_warp_role == ProducerWarpRole::Mainloop || producer_warp_role == ProducerWarpRole::MainloopAux)) { mainloop_pipeline_params.role = MainloopPipeline::ThreadCategory::Producer;}// Consumer: Consumer0 和 Consumer1if (warp_group_role == WarpGroupRole::Consumer0 || warp_group_role == WarpGroupRole::Consumer1) { mainloop_pipeline_params.role = MainloopPipeline::ThreadCategory::Consumer;}mainloop_pipeline_params.is_leader = warp_group_thread_idx == 0;mainloop_pipeline_params.num_consumers = NumMMAThreads; // 256mainloop_pipeline_params.num_producers = NumProducerThreads; // 32 或 64mainloop_pipeline_params.transaction_bytes = params.mainloop.tma_transaction_bytes;MainloopPipeline mainloop_pipeline(shared_storage.pipelines.mainloop, mainloop_pipeline_params, ClusterShape{}); 3.2 Producer 端（Mainloop Warp）12345678910111213141516171819202122232425262728293031// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:585-650if (producer_warp_role == ProducerWarpRole::Mainloop) { while (work_tile_info.is_valid()) { // 调用 CollectiveMainloop::load() collective_mainloop.load( params.mainloop, mainloop_pipeline, // Pipeline 接口 mainloop_pipe_producer_state, // 当前 stage load_inputs, // A/B tensors blk_coord, k_tile_iter, work_k_tile_count, lane_idx, block_rank_in_cluster, shared_storage.tensors.mainloop ); mainloop_pipe_producer_state.advance(work_k_tile_count); // 通知 Epilogue Warp 可以开始 if (do_load_order_arrive) { load_order_barrier.arrive(); do_load_order_arrive = false; } // 获取下一个 tile work_tile_info = scheduler.fetch_next_work(...); } // 通知所有 Consumer 结束 collective_mainloop.load_tail(mainloop_pipeline, mainloop_pipe_producer_state);} 3.3 Consumer 端（CollectiveMma::mma）Consumer 在 CollectiveMainloop::mma() 中使用 pipeline： 123456789101112131415161718192021222324252627282930313233343536373839404142// 源码: sm90_mma_tma_gmma_ss_warpspecialized.hpp:470-577void mma(MainloopPipeline pipeline, PipelineState smem_pipe_read, Tensor&amp; accum, int k_tile_count, ...) { PipelineState smem_pipe_release = smem_pipe_read; // Prologue: 预取前几个 tile for (int k = 0; k &lt; prologue_mma_count; ++k) { auto barrier_token = pipeline.consumer_try_wait(smem_pipe_read); pipeline.consumer_wait(smem_pipe_read, barrier_token); // 执行 GMMA cute::gemm(tiled_mma, tCrA[read_stage], tCrB[read_stage], accum); ++smem_pipe_read; } // Main loop: 边计算边释放 for (; k_tile_count &gt; 0; --k_tile_count) { auto barrier_token = pipeline.consumer_try_wait(smem_pipe_read); pipeline.consumer_wait(smem_pipe_read, barrier_token); cute::gemm(tiled_mma, tCrA[read_stage], tCrB[read_stage], accum); warpgroup_wait&lt;K_PIPE_MMAS&gt;(); // 等待前几个 MMA 完成 // 释放已计算完的 buffer pipeline.consumer_release(smem_pipe_release); ++smem_pipe_read; ++smem_pipe_release; }}// mma_tail: 释放剩余 buffervoid mma_tail(MainloopPipeline pipeline, PipelineState smem_pipe_release, int k_tile_count) { warpgroup_wait&lt;0&gt;(); // 等待所有 MMA 完成 for (int count = 0; count &lt; prologue_mma_count; ++count) { pipeline.consumer_release(smem_pipe_release); ++smem_pipe_release; }} 3.4 同步流程图12345678910111213141516171819202122232425sequenceDiagram participant ML as Mainloop Warp participant MP as MainloopPipeline participant C0 as Consumer0 participant C1 as Consumer1 Note over ML,C1: Stage 0 ML-&gt;&gt;MP: producer_acquire (wait EmptyBarrier) ML-&gt;&gt;MP: TMA Load A/B + expect_tx ML-&gt;&gt;MP: producer_commit par Consumer0 and Consumer1 C0-&gt;&gt;MP: consumer_wait (FullBarrier) C1-&gt;&gt;MP: consumer_wait (FullBarrier) end Note over C0,C1: GMMA 计算 par Consumer Release C0-&gt;&gt;MP: consumer_release (EmptyBarrier) C1-&gt;&gt;MP: consumer_release (EmptyBarrier) end ML-&gt;&gt;MP: 可以加载下一个 stage 4. Epilogue Load Pipeline4.1 特点 类型：PipelineTransactionAsync（与 Mainloop 相同） Producer：Epilogue Warp（Warp 2） Consumer：Consumer0 和 Consumer1 用途：加载 C 矩阵用于 betaC + alphaA*B 融合 4.2 初始化12345678910111213141516171819// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:460-475using EpiLoadPipeline = typename CollectiveEpilogue::LoadPipeline;typename EpiLoadPipeline::Params epi_load_pipeline_params;if (warp_group_role == WarpGroupRole::Producer &amp;&amp; producer_warp_role == ProducerWarpRole::Epilogue) { epi_load_pipeline_params.role = EpiLoadPipeline::ThreadCategory::Producer;}if (warp_group_role == WarpGroupRole::Consumer0 || warp_group_role == WarpGroupRole::Consumer1) { epi_load_pipeline_params.role = EpiLoadPipeline::ThreadCategory::Consumer;}epi_load_pipeline_params.dst_blockid = cute::block_rank_in_cluster();epi_load_pipeline_params.producer_arv_count = NumEpilogueLoadThreads; // 32epi_load_pipeline_params.consumer_arv_count = NumMMAThreads; // 256epi_load_pipeline_params.transaction_bytes = params.epilogue.tma_transaction_bytes;EpiLoadPipeline epi_load_pipeline(shared_storage.pipelines.epi_load, epi_load_pipeline_params); 4.3 Epilogue Producer12345678910111213141516171819202122// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:700-750if (producer_warp_role == ProducerWarpRole::Epilogue &amp;&amp; is_epi_load_needed) { // 等待 Mainloop 先开始 if (work_tile_info.is_valid()) { load_order_barrier.wait(); } while (work_tile_info.is_valid()) { if (TileScheduler::compute_epilogue(work_tile_info, params.scheduler)) { epi_load_pipe_producer_state = collective_epilogue.load( epi_load_pipeline, epi_load_pipe_producer_state, problem_shape_MNKL, blk_coord, ... ); } work_tile_info = scheduler.fetch_next_work(...); } collective_epilogue.load_tail(epi_load_pipeline, epi_load_pipe_producer_state);} 5. Epilogue Store Pipeline5.1 特点 类型：PipelineTmaStore 无 mbarrier：使用 TMA scoreboarding 机制 同步方式：tma_store_arrive() + tma_store_wait&lt;N&gt;() 5.2 初始化12345// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:477-481using EpiStorePipeline = typename CollectiveEpilogue::StorePipeline;typename EpiStorePipeline::Params epi_store_pipeline_params;epi_store_pipeline_params.always_wait = true; // 始终等待EpiStorePipeline epi_store_pipeline(epi_store_pipeline_params); 5.3 使用（Consumer 端）123456789101112131415161718192021222324// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:810-830if (TileScheduler::compute_epilogue(work_tile_info, params.scheduler)) { auto [epi_load_pipe_consumer_state_next, epi_store_pipe_producer_state_next] = collective_epilogue.store( epi_load_pipeline, epi_load_pipe_consumer_state, epi_store_pipeline, // Store pipeline epi_store_pipe_producer_state, problem_shape_MNKL, blk_shape, blk_coord, accumulators, ... ); epi_load_pipe_consumer_state = epi_load_pipe_consumer_state_next; epi_store_pipe_producer_state = epi_store_pipe_producer_state_next;}// 结束时collective_epilogue.store_tail( epi_load_pipeline, epi_load_pipe_consumer_state, epi_store_pipeline, epi_store_pipe_producer_state); 5.4 Store Pipeline 内部实现123// PipelineTmaStore 使用 TMA scoreboarding 而非 mbarrier// tma_store_arrive(): 提交一组 TMA store// tma_store_wait&lt;N&gt;(): 等待直到最多 N 组未完成 6. LoadWarpOrderBarrier6.1 目的确保 Mainloop Producer 先于 Epilogue Producer 执行，避免 Epilogue 加载 C 矩阵时 Mainloop 还未开始。 6.2 定义1234// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:140using LoadWarpOrderBarrier = cutlass::OrderedSequenceBarrier&lt;1, 2&gt;;// SequenceDepth = 1: 只有 1 个 stage// SequenceLength = 2: 2 个参与者（Mainloop 和 Epilogue） 6.3 初始化1234567// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:483-486typename LoadWarpOrderBarrier::Params params_load_order_barrier;params_load_order_barrier.group_id = producer_warp_role == ProducerWarpRole::Mainloop ? 0 : 1;params_load_order_barrier.group_size = NumThreadsPerWarp; // 32LoadWarpOrderBarrier load_order_barrier(shared_storage.pipelines.load_order, params_load_order_barrier); 6.4 使用12345678910// Mainloop Producer: 完成第一次加载后 arriveif (do_load_order_arrive) { load_order_barrier.arrive(); do_load_order_arrive = false;}// Epilogue Producer: 等待 Mainloop 先开始if (work_tile_info.is_valid()) { load_order_barrier.wait();} 6.5 工作原理1234567891011sequenceDiagram participant ML as Mainloop (group_id=0) participant OSB as OrderedSequenceBarrier participant EL as Epilogue (group_id=1) ML-&gt;&gt;ML: 开始加载 A/B ML-&gt;&gt;OSB: arrive() → signal group_id=1 EL-&gt;&gt;OSB: wait() → 等待 group_id=0 的 arrive Note over EL: wait 解除 EL-&gt;&gt;EL: 开始加载 C 7. TileScheduler Pipeline7.1 Dynamic Persistent Scheduler当使用 Dynamic Persistent 调度时，需要额外的 pipeline 来协调： 123456789101112// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:402-436if constexpr (IsSchedDynamicPersistent) { // Scheduler Pipeline: 用于 CLC 查询结果 scheduler_pipeline_params.producer_blockid = 0; scheduler_pipeline_params.producer_arv_count = 1; scheduler_pipeline_params.consumer_arv_count = NumSchedThreads + NumMainloopLoadThreads + NumMMAThreads; scheduler_pipeline_params.transaction_bytes = sizeof(typename TileScheduler::CLCResponse); // Throttle Pipeline: 控制 CLC 查询频率 scheduler_throttle_pipeline_params.producer_arv_count = NumMainloopLoadThreads; scheduler_throttle_pipeline_params.consumer_arv_count = NumSchedThreads;} 7.2 Scheduler Warp（Warp 1）123456789101112131415161718192021// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:548-580if (producer_warp_role == ProducerWarpRole::Warp1) { if constexpr (IsSchedDynamicPersistent) { while (work_tile_info.is_valid()) { // Throttle: 等待 Mainloop 准备好 scheduler_throttle_pipeline.consumer_wait(scheduler_pipe_throttle_consumer_state); scheduler_throttle_pipeline.consumer_release(scheduler_pipe_throttle_consumer_state); ++scheduler_pipe_throttle_consumer_state; // Query next work tile scheduler_pipe_producer_state = scheduler.advance_to_next_work( scheduler_pipeline, scheduler_pipe_producer_state); // Fetch and distribute auto [next_work_tile_info, increment_pipe] = scheduler.fetch_next_work(...); work_tile_info = next_work_tile_info; } scheduler_pipeline.producer_tail(scheduler_pipe_producer_state); }} 8. 无 cluster_sync 的初始化8.1 传统方式 vs Cooperative 方式 初始化后同步 问题 传统 cluster_sync() 所有 CTA 同步，开销大 Cooperative cluster_arrive_relaxed + 延迟 cluster_wait 更灵活 8.2 实现123456789101112131415// 源码: sm90_gemm_tma_warpspecialized_cooperative.hpp:500-511auto cluster_wait_fn = [] () { if constexpr (size(ClusterShape{}) &gt; 1) { cute::cluster_arrive_relaxed(); // 非阻塞 arrive return [] () { cute::cluster_wait(); }; // 返回延迟执行的 wait } else { __syncthreads(); return [] () {}; // 单 CTA 无需 cluster 同步 }} ();// ... pipeline 初始化 ...cluster_wait_fn(); // 延迟执行 cluster_wait 8.3 为什么这样设计 减少同步开销：cluster_arrive_relaxed 不等待其他 CTA Pipeline 初始化可见性：延迟的 cluster_wait 确保所有 CTA 看到初始化完成 灵活性：单 CTA 情况下直接使用 __syncthreads() 9. Pipeline 初始化为 Empty9.1 Producer/Consumer Start State123// Producer 从 phase=1 开始，Consumer 从 phase=0 开始PipelineState mainloop_pipe_producer_state = cutlass::make_producer_start_state&lt;MainloopPipeline&gt;();typename CollectiveMainloop::PipelineState mainloop_pipe_consumer_state; // phase=0 初始时 buffer 为空，Producer 可直接 acquire，Consumer 需等待数据就绪。 Phase 机制详解：参见 Pipeline 与 mbarrier - PipelineState 10. 完整 Pipeline 交互图1234567891011121314151617181920212223242526272829303132333435363738graph TB subgraph Producer_WG[&quot;Producer WarpGroup&quot;] MW[Mainloop Warp&lt;br/&gt;TMA A/B] SW[Scheduler Warp&lt;br/&gt;CLC Query] EW[Epilogue Warp&lt;br/&gt;TMA C] AW[Aux Warp] end subgraph Consumer_WGs[&quot;Consumer WarpGroups&quot;] C0[Consumer0&lt;br/&gt;MMA + Epilogue] C1[Consumer1&lt;br/&gt;MMA + Epilogue] end subgraph Pipelines[&quot;Pipelines&quot;] MLP[Mainloop Pipeline&lt;br/&gt;A/B Sync] ELP[Epilogue Load Pipeline&lt;br/&gt;C Sync] ESP[Epilogue Store Pipeline&lt;br/&gt;D Sync] OSB[LoadWarpOrderBarrier&lt;br/&gt;顺序控制] end MW --&gt;|produce| MLP MLP --&gt;|consume| C0 MLP --&gt;|consume| C1 EW --&gt;|produce| ELP ELP --&gt;|consume| C0 ELP --&gt;|consume| C1 C0 --&gt;|produce| ESP C1 --&gt;|produce| ESP MW --&gt;|arrive| OSB OSB --&gt;|wait| EW style MW fill:#e3f2fd style EW fill:#fff3e0 style C0 fill:#c8e6c9 style C1 fill:#c8e6c9 11. 关键要点总结 Warp Specialization：Producer WarpGroup 分为 4 个专用角色 Mainloop Pipeline：PipelineTmaAsync，使用 mbarrier 同步 Epilogue Store Pipeline：使用 TMA scoreboarding，无 mbarrier LoadWarpOrderBarrier：确保 Mainloop 先于 Epilogue 开始 无阻塞初始化：cluster_arrive_relaxed + 延迟 cluster_wait 12. 相关文档 Pipeline 与 mbarrier 深度解析 - mbarrier 原理、PipelineState、PTX 指令 TMA Descriptor 深度解析 - TMA 指令、expect_tx/complete_tx TMA Multicast 深度解析 - Cluster 内数据广播 参考资料 CUTLASS GitHub 仓库 sm90_gemm_tma_warpspecialized_cooperative.hpp sm90_mma_tma_gmma_ss_warpspecialized.hpp sm90_epilogue_tma_warpspecialized.hpp sm90_pipeline.hpp","link":"/2024/12/24/0x09-cooperative-kernel-pipeline/"},{"title":"0x0A CUTLASS SM90 GEMM Loop Hierarchy","text":"This article explains CUTLASS SM90 GEMM M/N/K dimension partitioning and loop hierarchy, from ProblemShape to innermost MMA instructions. 1. 整体 Loop 层次结构CUTLASS SM90 GEMM 将 ProblemShape (M, N, K) 切分为多层循环： 12345Level 0: Batch Loop (L dimension)+-- Level 1: Tile Scheduler Loop (M_tiles x N_tiles) +-- Level 2: K-tile Loop (over K dimension) +-- Level 3: Warp MMA K Loop (within one K-tile) +-- Level 4: MMA Atom (hardware instruction) 1.1 伪代码总览123456789101112131415161718192021222324252627// === Level 0: Batch Loop ===for (int l = 0; l &lt; L; ++l) { // Batch dimension // === Level 1: TileScheduler Loop (M/N tiles) === while (work_tile_info.is_valid()) { // Persistent kernel style auto [m_tile, n_tile] = work_tile_info; // 由 TileScheduler 分配 // === Level 2: K-tile Loop === for (int k_tile = 0; k_tile &lt; K_tiles; ++k_tile) { // Producer: TMA load A[m_tile, k_tile], B[n_tile, k_tile] → SMEM // Consumer: MMA compute // === Level 3: Warp MMA K Loop (within SMEM tile) === for (int k_block = 0; k_block &lt; MMA_K; ++k_block) { // === Level 4: MMA Atom === for (int mma_m = 0; mma_m &lt; MMA_M; ++mma_m) { for (int mma_n = 0; mma_n &lt; MMA_N; ++mma_n) { mma(D[mma_m,mma_n], A[mma_m,k_block], B[mma_n,k_block], C[mma_m,mma_n]); } } } } work_tile_info = scheduler.fetch_next_work(); // 获取下一个 tile }} 2. 维度切分关系2.1 从 ProblemShape 到各层循环123456789101112131415161718192021222324252627282930ProblemShape: (M, N, K, L) | | | | v v v v+-----------------------------------------------------------+| TileShape: (BLK_M, BLK_N, BLK_K) || e.g.: (128, 256, 64) || || M_tiles = ceil(M / BLK_M) // TileScheduler iterates || N_tiles = ceil(N / BLK_N) // TileScheduler iterates || K_tiles = ceil(K / BLK_K) // Mainloop K-tile loop |+-----------------------------------------------------------+ | v+-----------------------------------------------------------+| ClusterShape: (Cluster_M, Cluster_N, 1) || e.g.: (2, 1, 1) || || Multiple CTAs form a Cluster, sharing data (TMA Multicast)|| CTAs per Cluster = Cluster_M x Cluster_N |+-----------------------------------------------------------+ | v+-----------------------------------------------------------+| MMA Atom Shape: shape processed per MMA instruction || e.g. SM90 GMMA: M16N8K16 (FP16) || || MMA_M = BLK_M / atom_M // M iterations within warp || MMA_N = BLK_N / atom_N // N iterations within warp || MMA_K = BLK_K / atom_K // K iterations within warp |+-----------------------------------------------------------+ 2.2 数值示例假设： ProblemShape = (4096, 4096, 4096, 1) TileShape = (128, 256, 64) ClusterShape = (2, 1, 1) MMA Atom = M64N256K16 (GMMA) 123456789M_tiles = 4096 / 128 = 32N_tiles = 4096 / 256 = 16K_tiles = 4096 / 64 = 64总 Cluster 数 = 32/2 × 16/1 = 16 × 16 = 256 clusters每 Cluster 内 CTA 数 = 2 × 1 = 2总 CTA 数 = 256 × 2 = 512每个 K-tile 内的 MMA K 迭代 = 64 / 16 = 4 3. Level 1: TileScheduler Loop3.1 代码位置源码：sm90_gemm_tma_warpspecialized_cooperative.hpp:591-647 12345678910111213141516171819202122232425// Mainloop Producer Warpif (producer_warp_role == ProducerWarpRole::Mainloop) { while (work_tile_info.is_valid()) { // 从 WorkTileInfo 获取 M/N/L 坐标 auto m_coord = idx2crd(work_tile_info.M_idx, shape&lt;2&gt;(gA_mkl)); auto n_coord = idx2crd(work_tile_info.N_idx, shape&lt;2&gt;(gB_nkl)); auto l_coord = idx2crd(work_tile_info.L_idx, shape&lt;4&gt;(gB_nkl)); auto blk_coord = make_coord(m_coord, n_coord, _, l_coord); // 获取 K-tile 数量 auto work_k_tile_count = TileScheduler::get_work_k_tile_count( work_tile_info, problem_shape_MNKL, blk_shape); // K-tile 迭代器 auto k_tile_iter = cute::make_coord_iterator( idx2crd(work_k_tile_start, shape&lt;3&gt;(gA_mkl)), shape&lt;3&gt;(gA_mkl)); // 调用 mainloop.load() 执行 K-tile 循环 collective_mainloop.load(..., k_tile_iter, work_k_tile_count, ...); // 获取下一个 work tile auto [next_work_tile_info, increment_pipe] = scheduler.fetch_next_work(...); work_tile_info = next_work_tile_info; }} 3.2 WorkTileInfo 结构源码：static_tile_scheduler.hpp:55-84 123456struct WorkTileInfo { int32_t M_idx = 0; // M tile 索引 (以 CTA 为单位) int32_t N_idx = 0; // N tile 索引 (以 CTA 为单位) int32_t L_idx = 0; // Batch 索引 bool is_valid_tile = false;}; 3.3 Tile 总数计算源码：static_tile_scheduler.hpp:251-260 1234567891011template&lt;class ProblemShapeMNKL, class BlockShape, class ClusterShape&gt;static dim3 get_tiled_cta_shape_mnl(...) { // M 方向的 CTA 数量 auto cta_m = cute::size(cute::ceil_div(cute::shape&lt;0&gt;(problem_shape_mnkl), cute::shape&lt;0&gt;(cta_shape))); // N 方向的 CTA 数量 auto cta_n = cute::size(cute::ceil_div(cute::shape&lt;1&gt;(problem_shape_mnkl), cute::shape&lt;1&gt;(cta_shape))); // 返回 (cta_m, cta_n, L) return Params::get_tiled_cta_shape_mnl(..., cta_m, cta_n);} 4. Level 2: K-tile Loop (Producer)4.1 代码位置源码：sm90_mma_tma_gmma_ss_warpspecialized.hpp:370-391 12345678910111213141516171819// Producer: TMA 加载 A 和 B 到 SMEMCUTLASS_PRAGMA_NO_UNROLLfor ( ; k_tile_count &gt; 0; --k_tile_count) { // 等待 SMEM buffer 可写 pipeline.producer_acquire(smem_pipe_write); // 获取当前 stage 的 barrier BarrierType* tma_barrier = pipeline.producer_get_barrier(smem_pipe_write); int write_stage = smem_pipe_write.index(); // TMA 加载 A[m, k_tile] 和 B[n, k_tile] 到 SMEM copy(mainloop_params.tma_load_a.with(*tma_barrier, mcast_mask_a), tAgA(_,_,_,*k_tile_iter), tAsA(_,_,_,write_stage)); copy(mainloop_params.tma_load_b.with(*tma_barrier, mcast_mask_b), tBgB(_,_,_,*k_tile_iter), tBsB(_,_,_,write_stage)); ++k_tile_iter; // 下一个 K tile ++smem_pipe_write; // 下一个 pipeline stage} 4.2 K-tile 数量计算源码：static_tile_scheduler.hpp:445-452 12345678template &lt;class ProblemShape, class TileShape&gt;static int get_work_k_tile_count(WorkTileInfo const&amp; work_tile_info, ProblemShape problem_shape, TileShape tile_shape) { // K_tiles = ceil(K / BLK_K) return cute::size(cute::ceil_div(cute::get&lt;2&gt;(problem_shape), cute::get&lt;2&gt;(tile_shape)));} 5. Level 2: K-tile Loop (Consumer)5.1 代码位置源码：sm90_mma_tma_gmma_ss_warpspecialized.hpp:528-556 12345678910111213141516171819202122232425// Consumer: MMA 计算CUTLASS_PRAGMA_NO_UNROLLfor ( ; k_tile_count &gt; 0; --k_tile_count) { // 等待 SMEM 数据就绪 auto barrier_token = pipeline.consumer_try_wait(smem_pipe_read); pipeline.consumer_wait(smem_pipe_read, barrier_token); int read_stage = smem_pipe_read.index(); warpgroup_fence_operand(accum); warpgroup_arrive(); // Level 3: 调用 cute::gemm 执行 Warp MMA K Loop cute::gemm(tiled_mma, tCrA(_,_,_,read_stage), tCrB(_,_,_,read_stage), accum); warpgroup_commit_batch(); warpgroup_wait&lt;K_PIPE_MMAS&gt;(); warpgroup_fence_operand(accum); // 释放 SMEM buffer pipeline.consumer_release(smem_pipe_release); ++smem_pipe_read; ++smem_pipe_release;} 6. Level 3: Warp MMA K Loop6.1 代码位置源码：cute/algorithm/gemm.hpp:388-416 12345678910111213141516// Dispatch [5]: (V,M,K) x (V,N,K) =&gt; (V,M,N)template &lt;...&gt;void gemm(MMA_Atom&lt;MMA&gt; const&amp; mma, Tensor&lt;TD, DLayout&gt;&amp; D, // (V,M,N) Tensor&lt;TA, ALayout&gt; const&amp; A, // (V,M,K) Tensor&lt;TB, BLayout&gt; const&amp; B, // (V,N,K) Tensor&lt;TC, CLayout&gt; const&amp; C) // (V,M,N){ auto K = size&lt;2&gt;(A); // K 维度大小 (MMA_K iterations) CUTE_UNROLL for (int k = 0; k &lt; K; ++k) { // 对每个 k_block 调用 Level 4 的 (V,M) x (V,N) =&gt; (V,M,N) gemm(mma, D, A(_,_,k), B(_,_,k), C); }} 这个循环在 SMEM tile 内部迭代 K 方向的 MMA blocks。 7. Level 4: MMA M/N Loop7.1 代码位置源码：cute/algorithm/gemm.hpp:263-386 1234567891011121314151617181920212223// Dispatch [4]: (V,M) x (V,N) =&gt; (V,M,N)// 使用 serpentine 遍历优化寄存器复用template &lt;...&gt;void gemm(MMA_Atom&lt;MMA&gt; const&amp; mma, Tensor&lt;TD, DLayout&gt;&amp; D, // (V,M,N) Tensor&lt;TA, ALayout&gt; const&amp; A, // (V,M) Tensor&lt;TB, BLayout&gt; const&amp; B, // (V,N) Tensor&lt;TC, CLayout&gt; const&amp; C) // (V,M,N){ auto M = size&lt;1&gt;(A); auto N = size&lt;1&gt;(B); // Row-major serpentine iteration (优化寄存器复用) CUTE_UNROLL for (int m = 0; m &lt; M; ++m) { CUTE_UNROLL for (int n = 0; n &lt; N; ++n) { int ns = (m &amp; 1) ? N-1-n : n; // Serpentine 坐标 // Level 4: 调用单个 MMA 指令 gemm(mma, D(_,m,ns), A(_,m), B(_,ns), C(_,m,ns)); } }} 7.2 Serpentine 遍历模式1234m=0: n → 0, 1, 2, 3 (正向)m=1: n → 3, 2, 1, 0 (反向)m=2: n → 0, 1, 2, 3 (正向)m=3: n → 3, 2, 1, 0 (反向) 这种遍历模式最大化了 A 寄存器的复用（同一行 m 复用 A 值）。 8. 完整循环层次图12345678910111213141516171819202122232425262728293031323334353637graph TB subgraph &quot;Level 0: Batch&quot; L[L Loop] end subgraph &quot;Level 1: TileScheduler&quot; MN[M×N Tiles Loop&lt;br/&gt;fetch_next_work] end subgraph &quot;Level 2: K-tile&quot; K[K tiles Loop&lt;br/&gt;k_tile_count iterations] end subgraph &quot;Level 3: Warp MMA K&quot; WK[Warp K Loop&lt;br/&gt;cute::gemm K dimension] end subgraph &quot;Level 4: MMA M×N&quot; MMA_MN[MMA M×N Loop&lt;br/&gt;serpentine iteration] end subgraph &quot;Hardware&quot; GMMA[GMMA Instruction&lt;br/&gt;wgmma.mma_async] end L --&gt; MN MN --&gt; K K --&gt; WK WK --&gt; MMA_MN MMA_MN --&gt; GMMA style L fill:#e3f2fd style MN fill:#e8f5e9 style K fill:#fff3e0 style WK fill:#fce4ec style MMA_MN fill:#f3e5f5 style GMMA fill:#ffebee 9. 代码对照表 Loop Level 代码位置 循环变量 迭代次数 L0: Batch Kernel launch L_idx L L1: Tile M×N cooperative.hpp:591 work_tile_info M_tiles × N_tiles L2: K-tile warpspecialized.hpp:372 k_tile_count ceil(K / BLK_K) L3: Warp K gemm.hpp:412 k BLK_K / atom_K L4: MMA M×N gemm.hpp:294 m, n MMA_M × MMA_N 10. Pipeline 与 Loop 的交互1234567891011121314Producer (TMA Load) Consumer (MMA Compute)--------------------- -------------------------for k_tile in K_tiles: for k_tile in K_tiles: | | +- producer_acquire(stage) +- consumer_wait(stage) | | +- TMA_load_A[k_tile] +- for k in MMA_K: +- TMA_load_B[k_tile] | for m in MMA_M: | | for n in MMA_N: +- (implicit commit via TMA) | GMMA(A,B,C) | | +- ++stage +- consumer_release(stage) +- ++stage 11. 关键要点总结 五层循环结构：Batch → Tile M×N → K-tile → Warp K → MMA M×N TileScheduler 管理 M×N：persistent kernel 风格，动态获取 work tiles K-tile 循环在 Mainloop：Producer/Consumer 异步执行，Pipeline 同步 Warp MMA K 在 cute::gemm：展开循环，处理 SMEM tile 内的 K blocks Serpentine 遍历：优化寄存器复用，减少 register spilling 12. 相关文档 Pipeline 与 mbarrier 深度解析 - Pipeline 同步机制 TMA Descriptor 深度解析 - TMA 数据加载 Cooperative Kernel Pipeline 深度解析 - Warp Specialization 参考资料 CUTLASS GitHub 仓库 sm90_gemm_tma_warpspecialized_cooperative.hpp sm90_mma_tma_gmma_ss_warpspecialized.hpp cute/algorithm/gemm.hpp static_tile_scheduler.hpp","link":"/2024/12/24/0x0A-gemm-loop-hierarchy/"},{"title":"0x0B CUTLASS SM90 Pipeline and mbarrier PTX Mapping","text":"This article explains CUTLASS SM90 Pipeline mechanism and its underlying mbarrier PTX instruction mapping. All code references are from NVIDIA CUTLASS official repository. 示例代码: 0x0B_pipeline_barrier_ptx.cu 1. mbarrier 原理1.1 什么是 mbarriermbarrier（Memory Barrier）是 NVIDIA Hopper (SM90) 架构引入的硬件同步原语，存储在共享内存（SMEM）中。它是一个 64-bit 的硬件对象，支持： 到达计数（Arrival Counting）：追踪有多少线程已经到达 barrier 事务计数（Transaction Counting）：追踪 TMA 传输了多少字节（仅 ClusterTransactionBarrier） Phase 位：用于区分不同轮次的同步 1.2 mbarrier 64-bit 结构 字段 位数 描述 Phase Bit 1 完成时翻转，用于区分不同轮次 Pending TX Count ~20 期望传输的字节数（TMA 完成时递减） Arrival Count ~20 剩余需要 arrive 的线程数 完成条件： 1Pending TX Count == 0 AND Arrival Count == 0 → Phase Bit 翻转 1.3 Phase-Parity 机制Phase 位是 mbarrier 实现循环复用的关键： Barrier 初始化时 phase = 0 当所有条件满足（到达计数和事务计数都归零），phase 翻转（0→1 或 1→0） try_wait.parity 指令检查当前 phase 是否匹配期望值 这样同一个 barrier 可以在不同迭代中重复使用 1.4 两种 Barrier 类型CUTLASS 定义了两种 barrier 类型： 类型 用途 完成条件 ClusterBarrier 纯到达计数 Arrival Count == 0 ClusterTransactionBarrier 到达 + 事务计数 Arrival Count == 0 AND TX Count == 0 源码位置：barrier.h 1.5 双 Barrier 架构Pipeline 使用双 Barrier 实现生产者-消费者同步： 123456789101112131415161718192021222324252627282930graph LR subgraph &quot;Pipeline Stage [i]&quot; FB[FullBarrier&lt;br/&gt;ClusterTransactionBarrier] EB[EmptyBarrier&lt;br/&gt;ClusterBarrier] BUF[(SMEM Buffer)] end subgraph Producer P1[producer_acquire] P2[TMA Load] P3[TMA Complete] end subgraph Consumer C1[consumer_wait] C2[MMA Compute] C3[consumer_release] end P1 --&gt;|wait| EB P1 --&gt;|arrive_and_expect_tx| FB P2 --&gt;|write| BUF P3 --&gt;|complete_tx| FB C1 --&gt;|wait| FB BUF --&gt;|read| C2 C3 --&gt;|arrive| EB style FB fill:#e1f5fe style EB fill:#fff3e0 Barrier 类型 谁 Signal 谁 Wait 含义 FullBarrier ClusterTransactionBarrier Producer (TMA) Consumer “Data is ready” EmptyBarrier ClusterBarrier Consumer Producer “Buffer is free” 语义说明： 123456789+-------------------------------------------------------------+| Full Barrier (Data Ready): || - Producer arrives after writing data || - Consumer waits on this barrier before reading || || Empty Barrier (Buffer Free): || - Consumer arrives after consuming data || - Producer waits on this barrier before reusing buffer |+-------------------------------------------------------------+ 2. PipelineState 详解PipelineState 是 Pipeline 的状态追踪器，管理循环缓冲区中的当前位置。 2.1 数据结构定义12345678910111213141516171819// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:170-250template&lt;uint32_t Stages_&gt;struct PipelineState { static constexpr uint32_t Stages = Stages_; int index_ = 0; // 当前 stage 索引 (0 到 Stages-1) uint32_t phase_ = 0; // 当前 phase (0 或 1)，每绕回一次翻转 uint32_t count_ = 0; // 总迭代次数 CUTLASS_DEVICE PipelineState(): index_{}, phase_{}, count_{} {} CUTLASS_DEVICE PipelineState(int index, uint32_t phase, uint32_t count) : index_(index) , phase_(phase) , count_(count) {}}; 2.2 成员变量说明 成员 类型 描述 index_ int 当前 stage 在循环缓冲区中的索引，范围 [0, Stages-1] phase_ uint32_t 当前 phase 值，0 或 1，用于 barrier 同步 count_ uint32_t 总迭代计数，用于追踪已处理多少次 2.3 自增操作符123456789101112// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:203-213CUTLASS_DEVICEvoid operator++() { if constexpr (Stages &gt; 0) { ++index_; ++count_; if (index_ == Stages) { index_ = 0; // 回绕到开头 phase_ ^= 1; // 翻转 phase } }} 关键点：当 index_ 从 Stages-1 回绕到 0 时，phase_ 翻转。这确保了 barrier 能区分不同轮次的操作。 12345678910111213141516171819stateDiagram-v2 direction LR state &quot;Stage 0&quot; as S0 state &quot;Stage 1&quot; as S1 state &quot;Stage 2&quot; as S2 state &quot;Stage 3&quot; as S3 [*] --&gt; S0: index=0, phase=0 S0 --&gt; S1: ++index S1 --&gt; S2: ++index S2 --&gt; S3: ++index S3 --&gt; S0: index回绕, phase翻转 note right of S3 当 index == Stages 时: index = 0 phase ^= 1 end note 2.4 advance 方法12345678910111213141516// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:228-244CUTLASS_DEVICEPipelineState&amp; advance(uint32_t num_iterations) { if constexpr (Stages &gt; 0) { // 判断是否需要翻转 phase if ((num_iterations &lt; Stages) &amp;&amp; (index_ + num_iterations) &gt;= Stages ) { phase_ ^= 1; } if ((num_iterations &gt;= Stages) &amp;&amp; (((index_ + num_iterations) / Stages) % 2) == 1) { phase_ ^= 1; } index_ = (index_ + num_iterations) % Stages; count_ += num_iterations; } return *this;} 2.5 访问器方法123CUTLASS_DEVICE int index() const { return index_; }CUTLASS_DEVICE uint32_t phase() const { return phase_; }CUTLASS_DEVICE uint32_t count() const { return count_; } 2.6 Producer 起始状态12345678910// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:252-260template&lt;class Pipeline&gt;CUTLASS_DEVICEPipelineState&lt;Pipeline::Stages&gt; make_producer_start_state() { // Producer 以相反的 phase 开始，因为缓冲区初始为空 constexpr int InitialProducerStage = 0; constexpr uint32_t InitialProducerPhase = 1; // 注意：phase 为 1 constexpr uint32_t InitialProducerCount = 0; return {InitialProducerStage, InitialProducerPhase, InitialProducerCount};} 重要：Producer 初始 phase 为 1，而 Consumer 初始 phase 为 0。这是因为缓冲区一开始是空的，Producer 需要先填充数据。 3. PipelineTmaAsync 详解PipelineTmaAsync 是 SM90 上用于 TMA 异步加载的 Pipeline 类，实现了生产者-消费者同步模式。 源码位置：sm90_pipeline.hpp 3.1 类型定义123456789101112// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:270-278template &lt;int Stages_&gt;class PipelineTmaAsync {public: using FullBarrier = cutlass::arch::ClusterTransactionBarrier; using EmptyBarrier = cutlass::arch::ClusterBarrier; using ProducerBarrierType = FullBarrier::ValueType; // uint64_t using ConsumerBarrierType = EmptyBarrier::ValueType; // uint64_t static constexpr uint32_t Stages = Stages_; using PipelineState = cutlass::PipelineState&lt;Stages&gt;; // ...}; 类型别名 实际类型 用途 FullBarrier ClusterTransactionBarrier 数据就绪信号，支持事务计数 EmptyBarrier ClusterBarrier 空间释放信号，纯到达计数 ProducerBarrierType uint64_t Full barrier 的原始值类型 ConsumerBarrierType uint64_t Empty barrier 的原始值类型 3.2 SharedStorage 结构12345// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:280-283struct SharedStorage { FullBarrier full_barrier_[Stages]; // 每个 stage 一个 full barrier EmptyBarrier empty_barrier_[Stages]; // 每个 stage 一个 empty barrier}; 每个 Pipeline stage 都有一对 barrier： full_barrier_：Producer 填充数据后 signal，Consumer 等待 empty_barrier_：Consumer 使用完毕后 signal，Producer 等待 12345678910111213141516171819202122232425262728293031graph TB subgraph &quot;SharedStorage (SMEM)&quot; subgraph &quot;Stage 0&quot; FB0[full_barrier_0] EB0[empty_barrier_0] end subgraph &quot;Stage 1&quot; FB1[full_barrier_1] EB1[empty_barrier_1] end subgraph &quot;Stage 2&quot; FB2[full_barrier_2] EB2[empty_barrier_2] end subgraph &quot;Stage N-1&quot; FBN[full_barrier_N-1] EBN[empty_barrier_N-1] end end FULL[full_barrier_ptr_] --&gt; FB0 EMPTY[empty_barrier_ptr_] --&gt; EB0 style FB0 fill:#e3f2fd style FB1 fill:#e3f2fd style FB2 fill:#e3f2fd style FBN fill:#e3f2fd style EB0 fill:#fff8e1 style EB1 fill:#fff8e1 style EB2 fill:#fff8e1 style EBN fill:#fff8e1 3.3 ThreadCategory 枚举1234567// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:285-290enum class ThreadCategory { NonParticipant, // 不参与 Pipeline 操作 Producer, // 仅作为生产者 Consumer, // 仅作为消费者 ProducerConsumer // 同时是生产者和消费者}; 3.4 Params 参数结构123456789// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:292-299struct Params { uint32_t transaction_bytes = 0; // 每次 TMA 传输的字节数 ThreadCategory role = ThreadCategory::NonParticipant; // 线程角色 uint32_t is_leader = 0; // 是否为 leader 线程（负责 barrier 操作） uint32_t num_consumers = 0; // Consumer 线程总数 uint32_t num_producers = 1; // Producer 线程总数 int initializing_warp = 0; // 负责初始化 barrier 的 warp}; 3.5 私有成员变量1234567// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:494-499private: uint32_t dst_blockid_ = 0; // 目标 CTA ID（用于 cluster 内通信） uint32_t is_signaling_thread_ = 0; // 是否负责发送 arrive 信号 FullBarrier *full_barrier_ptr_ = nullptr; // 指向 full barrier 数组 EmptyBarrier *empty_barrier_ptr_ = nullptr; // 指向 empty barrier 数组 Params params_; // 配置参数 成员 描述 dst_blockid_ 在 cluster 模式下，标识要发送 arrive 信号的目标 CTA is_signaling_thread_ 标记此线程是否负责发送 arrive 信号（避免重复发送） full_barrier_ptr_ 指向共享内存中 FullBarrier 数组的指针 empty_barrier_ptr_ 指向共享内存中 EmptyBarrier 数组的指针 params_ 保存构造时传入的配置参数 3.6 构造函数1234567891011121314151617181920212223242526272829303132// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:326-377template&lt;class ClusterShape, class InitBarriers, class InitMasks&gt;CUTLASS_DEVICEPipelineTmaAsync(SharedStorage&amp; storage, Params params, ClusterShape cluster_shape, InitBarriers = {}, InitMasks = {}) : params_(params) , full_barrier_ptr_(&amp;storage.full_barrier_[0]) , empty_barrier_ptr_(&amp;storage.empty_barrier_[0]) { int warp_idx = canonical_warp_idx_sync(); int thread_idx = threadIdx.x; // 初始化 barrier（如果需要） if constexpr (cute::is_same_v&lt;InitBarriers, cute::true_type&gt;) { init_barriers(storage, params_, cluster_shape); } // 初始化信号掩码（用于 cluster 内通信） if constexpr (cute::is_same_v&lt;InitMasks, cute::true_type&gt;) { dim3 block_id = cute::block_id_in_cluster(); auto cluster_size = cute::size(cluster_shape); if (cluster_size == 1) { is_signaling_thread_ = true; dst_blockid_ = 0; } else { // 在 warp group 内分配 arrive 职责 // ... } }} 3.7 Barrier 初始化12345678910111213141516171819202122232425// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:301-324template &lt;class ClusterShape&gt;static CUTLASS_DEVICE voidinit_barriers(SharedStorage&amp; storage, Params params, ClusterShape cluster_shape) { int warp_idx = canonical_warp_idx_sync(); bool is_initializing_warp = (warp_idx == params.initializing_warp); if (is_initializing_warp) { uint32_t const producer_arv_cnt = params.num_producers; uint32_t multicast_consumer_arrival_count = params.num_consumers; // Cluster 模式下调整 arrival count if (cute::size(cluster_shape) &gt; 1) { uint32_t const num_consumer_warpgroups_per_cluster = cute::ceil_div(params.num_consumers, NumThreadsPerWarpGroup); multicast_consumer_arrival_count = (cute::size&lt;0&gt;(cluster_shape) + cute::size&lt;1&gt;(cluster_shape) - 1) * num_consumer_warpgroups_per_cluster; } // 初始化 barrier 对 initialize_barrier_array_pair_aligned(...); } cutlass::arch::fence_barrier_init();} fence_barrier_init 的作用： 123456CUTLASS_DEVICEvoid fence_barrier_init() { asm volatile( &quot;fence.mbarrier_init.release.cluster;&quot; ::);} 确保 barrier 初始化对 cluster 内所有 CTA 可见： 1234567891011Thread 0 (init): Other threads / CTAs: | | v |mbarrier.init(...) |mbarrier.init(...) | waiting... | | v |fence.mbarrier_init.release ----------+---&gt; now visible | | v v safe to use barrier 3.8 Arrival Count 详解Cluster Size == 1（无 Multicast）： 1multicast_consumer_arrival_count = params.num_consumers; // 线程数 每个线程都执行 arrive。 Cluster Size &gt; 1（Multicast）： 12multicast_consumer_arrival_count = (ClusterM + ClusterN - 1) * num_consumer_warpgroups_per_cluster; 2×2 Cluster 计算示例： 1234567891011// ClusterM = 2, ClusterN = 2// num_consumer_warpgroups = 2 (每 CTA)// 接收 multicast 的有效 CTA 数unique_cta_count = ClusterM + ClusterN - 1 = 2 + 2 - 1 = 3// 每个有效 CTA 的 warpgroupswarpgroups_per_cta = 2// 总 arrival countmulticast_consumer_arrival_count = 3 * 2 = 6 为什么是 ClusterM + ClusterN - 1？ 1234567891011121314151617181920Cluster = (2, 2), CTA(0,0) Producer example: N +-------------------------+ | CTA(0,0) | CTA(0,1) | &lt;- B multicast (same row) M | (self) | (B) | +-----------+------------+ | CTA(1,0) | CTA(1,1) | | (A) | (unrel.) | +-------------------------+ ^ A multicast (same col)Receiving CTAs: - CTA(0,0): receives A and B (self) - CTA(1,0): receives A - CTA(0,1): receives BTotal: 3 CTAs = 2 + 2 - 1CTA(1,1) does not receive any data from CTA(0,0)! 4. Producer API 详解4.1 producer_try_acquire非阻塞尝试获取缓冲区空间： 12345678910111213141516// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:419-422 (public)CUTLASS_DEVICEProducerToken producer_try_acquire(PipelineState state, uint32_t skip_wait = false) { return producer_try_acquire(state.index(), state.phase(), skip_wait);}// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:501-509 (private)CUTLASS_DEVICEProducerToken producer_try_acquire(uint32_t stage, uint32_t phase, uint32_t skip_wait) { detail::pipeline_check_is_producer(params_.role); if (skip_wait) { return {BarrierStatus::WaitDone}; } bool barrier_status = empty_barrier_ptr_[stage].try_wait(phase); return {static_cast&lt;BarrierStatus&gt;(barrier_status)};} PTX 指令： 12mbarrier.try_wait.parity.shared::cta.b64 P1, [smem_addr], phase;selp.b32 result, 1, 0, P1; 4.2 producer_acquire阻塞等待缓冲区空间，并设置期望传输字节数： 1234567891011121314151617// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:424-427 (public)CUTLASS_DEVICEvoid producer_acquire(PipelineState state) { producer_acquire(state.index(), state.phase());}// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:511-528 (private)CUTLASS_DEVICEvoid producer_acquire(uint32_t stage, uint32_t phase) { // Step 1: 等待 Consumer 释放空间 empty_barrier_ptr_[stage].wait(phase); // Step 2: Leader 线程设置期望传输字节数 if (params_.is_leader) { full_barrier_ptr_[stage].arrive_and_expect_tx(params_.transaction_bytes); }} PTX 指令： 等待部分（spin loop）： 12345LAB_WAIT: mbarrier.try_wait.parity.shared::cta.b64 P1, [smem_addr], phase, 0x989680; @P1 bra DONE; bra LAB_WAIT;DONE: 设置期望字节数： 1mbarrier.arrive.expect_tx.shared::cta.b64 _, [smem_addr], transaction_bytes; 4.3 producer_acquire (带 token)12345678910111213// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:530-550CUTLASS_DEVICEvoid producer_acquire(uint32_t stage, uint32_t phase, ProducerToken barrier_token) { detail::pipeline_check_is_producer(params_.role); // 如果 try_acquire 已经成功，跳过等待 if (barrier_token != BarrierStatus::WaitDone) { empty_barrier_ptr_[stage].wait(phase); } if (params_.is_leader) { full_barrier_ptr_[stage].arrive_and_expect_tx(params_.transaction_bytes); }} 4.4 producer_expect_transaction额外增加期望传输字节数（用于多次 TMA 操作）： 12345678// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:552-558CUTLASS_DEVICEvoid producer_expect_transaction(uint32_t stage, uint32_t transaction_bytes) { detail::pipeline_check_is_producer(params_.role); if (params_.is_leader) { full_barrier_ptr_[stage].expect_transaction(transaction_bytes); }} PTX 指令： 1mbarrier.expect_tx.shared::cta.b64 [smem_addr], transaction_bytes; 4.5 producer_commitTMA 完成后由硬件自动触发： 1234567891011// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:560-587CUTLASS_DEVICEvoid producer_commit(uint32_t stage, uint32_t bytes) { // 仅用于单元测试（无 TMA 时手动提交） #if CUTLASS_UNIT_TEST_PIPELINE if (params_.is_leader) { full_barrier_ptr_[stage].complete_transaction(bytes); // Cluster 模式下通知其他 CTA... } #endif} PTX 指令（TMA 硬件自动执行）： 1mbarrier.complete_tx.shared::cluster.relaxed.cluster.b64 [smem_addr], transaction_bytes; 4.6 producer_get_barrier返回 barrier 指针供 TMA 使用： 12345// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:638-641CUTLASS_DEVICEProducerBarrierType* producer_get_barrier(uint32_t stage) { return reinterpret_cast&lt;ProducerBarrierType*&gt;(&amp;full_barrier_ptr_[stage]);} 4.7 producer_tail防止 Producer block 过早退出： 123456789// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:447-454CUTLASS_DEVICEvoid producer_tail(PipelineState state) { detail::pipeline_check_is_producer(params_.role); for (int count = 0; count &lt; Stages; ++count) { empty_barrier_ptr_[state.index()].wait(state.phase()); ++state; }} 5. Consumer API 详解5.1 consumer_try_wait非阻塞尝试等待数据就绪： 12345678910// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:590-597CUTLASS_DEVICEConsumerToken consumer_try_wait(uint32_t stage, uint32_t phase, uint32_t skip_wait) { detail::pipeline_check_is_consumer(params_.role); if (skip_wait) { return {BarrierStatus::WaitDone}; } bool barrier_status = full_barrier_ptr_[stage].try_wait(phase); return {static_cast&lt;BarrierStatus&gt;(barrier_status)};} 返回值： BarrierStatus::WaitDone (1)：数据已就绪 BarrierStatus::WaitAgain (0)：数据未就绪 5.2 consumer_test_wait与 try_wait 类似，但使用 test_wait PTX 指令： 12345678910// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:599-607CUTLASS_DEVICEConsumerToken consumer_test_wait(uint32_t stage, uint32_t phase, uint32_t skip_wait) { detail::pipeline_check_is_consumer(params_.role); if (skip_wait) { return {BarrierStatus::WaitDone}; } bool barrier_status = full_barrier_ptr_[stage].test_wait(phase); return {static_cast&lt;BarrierStatus&gt;(barrier_status)};} 5.3 consumer_wait阻塞等待数据就绪： 12345678910111213141516// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:609-623CUTLASS_DEVICEvoid consumer_wait(uint32_t stage, uint32_t phase) { detail::pipeline_check_is_consumer(params_.role); full_barrier_ptr_[stage].wait(phase);}// 带 token 版本CUTLASS_DEVICEvoid consumer_wait(uint32_t stage, uint32_t phase, ConsumerToken barrier_token) { detail::pipeline_check_is_consumer(params_.role); if (barrier_token == BarrierStatus::WaitAgain) { full_barrier_ptr_[stage].wait(phase); } // 如果已经 WaitDone，直接返回} 5.4 consumer_release通知 Producer 空间已释放： 123456// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:625-636CUTLASS_DEVICEvoid consumer_release(uint32_t stage, uint32_t skip = false) { detail::pipeline_check_is_consumer(params_.role); empty_barrier_ptr_[stage].arrive(dst_blockid_, is_signaling_thread_ &amp; (!skip));} PTX 指令： 本地 CTA arrive： 1mbarrier.arrive.shared::cta.b64 _, [smem_addr]; 远程 Cluster arrive： 12mapa.shared::cluster.u32 remAddr32, smem_addr, cta_id;mbarrier.arrive.shared::cluster.b64 _, [remAddr32]; 6. Pipeline API 到 PTX 映射总览 Pipeline API Barrier 类型 PTX 指令 producer_try_acquire EmptyBarrier mbarrier.try_wait.parity (单次) producer_acquire (wait) EmptyBarrier mbarrier.try_wait.parity (spin) producer_acquire (leader) FullBarrier mbarrier.arrive.expect_tx producer_expect_transaction FullBarrier mbarrier.expect_tx producer_commit FullBarrier mbarrier.complete_tx (TMA 自动) consumer_try_wait FullBarrier mbarrier.try_wait.parity (单次) consumer_test_wait FullBarrier mbarrier.test_wait.parity consumer_wait FullBarrier mbarrier.try_wait.parity (spin) consumer_release EmptyBarrier mbarrier.arrive 7. ClusterBarrier 实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 源码: include/cutlass/arch/barrier.h:341-532struct ClusterBarrier { using ValueType = uint64_t;protected: ValueType barrier_; // SMEM 中的 64-bit mbarrier 对象public: // 初始化 barrier static void init(ValueType const* smem_ptr, uint32_t arrive_count) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); asm volatile( &quot;mbarrier.init.shared::cta.b64 [%1], %0;&quot; : : &quot;r&quot;(arrive_count), &quot;r&quot;(smem_addr)); } // 阻塞等待（spin loop） static void wait(ValueType const* smem_ptr, uint32_t phase) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); uint32_t ticks = 0x989680; // ~10M cycles 超时后重试 asm volatile( &quot;.reg .pred P1;\\n&quot; &quot;LAB_WAIT:\\n&quot; &quot;mbarrier.try_wait.parity.shared::cta.b64 P1, [%0], %1, %2;\\n&quot; &quot;@P1 bra DONE;\\n&quot; &quot;bra LAB_WAIT;\\n&quot; &quot;DONE:&quot; : : &quot;r&quot;(smem_addr), &quot;r&quot;(phase), &quot;r&quot;(ticks)); } // 非阻塞尝试等待 static bool try_wait(ValueType const* smem_ptr, uint32_t phase) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); uint32_t waitComplete; asm volatile( &quot;.reg .pred P1;\\n&quot; &quot;mbarrier.try_wait.parity.shared::cta.b64 P1, [%1], %2;\\n&quot; &quot;selp.b32 %0, 1, 0, P1;&quot; : &quot;=r&quot;(waitComplete) : &quot;r&quot;(smem_addr), &quot;r&quot;(phase)); return static_cast&lt;bool&gt;(waitComplete); } // 本地 arrive static void arrive(ValueType const* smem_ptr) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); asm volatile( &quot;mbarrier.arrive.shared::cta.b64 _, [%0];&quot; : : &quot;r&quot;(smem_addr)); } // 远程 cluster arrive static void arrive(ValueType const* smem_ptr, uint32_t cta_id, uint32_t pred) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); if (pred) { asm volatile( &quot;.reg .b32 remAddr32;\\n&quot; &quot;mapa.shared::cluster.u32 remAddr32, %0, %1;\\n&quot; &quot;mbarrier.arrive.shared::cluster.b64 _, [remAddr32];&quot; : : &quot;r&quot;(smem_addr), &quot;r&quot;(cta_id)); } }}; 8. ClusterTransactionBarrier 实现1234567891011121314151617181920212223242526272829303132// 源码: include/cutlass/arch/barrier.h:538-693struct ClusterTransactionBarrier : public ClusterBarrier { // Arrive + 设置期望传输字节数 static void arrive_and_expect_tx(ValueType const* smem_ptr, uint32_t transaction_bytes) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); asm volatile( &quot;mbarrier.arrive.expect_tx.shared::cta.b64 _, [%1], %0;&quot; : : &quot;r&quot;(transaction_bytes), &quot;r&quot;(smem_addr)); } // 仅设置期望字节数（不 arrive） static void expect_transaction(ValueType const* smem_ptr, uint32_t transaction_bytes) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); asm volatile( &quot;mbarrier.expect_tx.shared::cta.b64 [%1], %0;&quot; : : &quot;r&quot;(transaction_bytes), &quot;r&quot;(smem_addr)); } // 完成传输（减少 pending 字节数）- TMA 硬件自动调用 static void complete_transaction( ValueType const* smem_ptr, uint32_t dst_cta_id, uint32_t transaction_bytes, uint32_t pred = 1) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); smem_addr = cute::set_block_rank(smem_addr, dst_cta_id); asm volatile( &quot;.reg .pred p;\\n&quot; &quot;setp.eq.u32 p, %2, 1;\\n&quot; &quot;@p mbarrier.complete_tx.shared::cluster.relaxed.cluster.b64 [%1], %0;&quot; : : &quot;r&quot;(transaction_bytes), &quot;r&quot;(smem_addr), &quot;r&quot;(pred)); }}; 9. 完整工作流程示例下图展示了 Producer 和 Consumer 之间的交互时序： 12345678910111213141516171819202122232425262728sequenceDiagram participant P as Producer participant FB as FullBarrier participant EB as EmptyBarrier participant BUF as SMEM Buffer participant C as Consumer Note over P,C: Stage i 的一次完整迭代 rect rgb(232, 245, 233) Note over P: Producer Phase P-&gt;&gt;EB: wait(phase) - 等待空间 EB--&gt;&gt;P: 空间已释放 P-&gt;&gt;FB: arrive_and_expect_tx(bytes) P-&gt;&gt;BUF: TMA Load (异步) Note over FB: TMA完成后自动&lt;br/&gt;complete_tx end rect rgb(227, 242, 253) Note over C: Consumer Phase C-&gt;&gt;FB: wait(phase) - 等待数据 FB--&gt;&gt;C: 数据已就绪 BUF-&gt;&gt;C: 读取数据 Note over C: MMA 计算 C-&gt;&gt;EB: arrive() - 释放空间 end Note over P,C: Phase 翻转后进入下一轮 12345678910111213141516171819202122232425262728293031323334353637383940// Producer 线程（TMA 加载器）PipelineState producer_state = make_producer_start_state&lt;Pipeline&gt;();for (int k = 0; k &lt; num_tiles; ++k) { // 1. 获取缓冲区（等待空间 + 设置期望字节数） pipeline.producer_acquire(producer_state); // 2. 获取 barrier 指针供 TMA 使用 auto* barrier = pipeline.producer_get_barrier(producer_state); // 3. 发起 TMA 加载（硬件自动完成 barrier） copy(tma_load, gmem_tensor, smem_tensor, barrier); ++producer_state;}// 4. 退出前等待所有 Consumer 完成pipeline.producer_tail(producer_state);// Consumer 线程（MMA 计算）PipelineState consumer_state{0, 0, 0}; // phase = 0for (int k = 0; k &lt; num_tiles; ++k) { // 1. 尝试等待（非阻塞） auto token = pipeline.consumer_try_wait(consumer_state); // 2. 可以做一些其他工作... // 3. 如果需要，阻塞等待 pipeline.consumer_wait(consumer_state, token); // 4. 使用 SMEM 数据进行 MMA 计算 gemm(smem_tensor, accumulators); // 5. 释放缓冲区给 Producer pipeline.consumer_release(consumer_state); ++consumer_state;} 10. 关键要点总结 SM90 专属：Pipeline 机制专为 NVIDIA Hopper 架构（SM90+）设计 双 Barrier 架构： FullBarrier（ClusterTransactionBarrier）：Producer → Consumer，数据就绪信号 EmptyBarrier（ClusterBarrier）：Consumer → Producer，空间释放信号 硬件加速：TMA 完成时自动 signal barrier，无需软件干预 Phase 机制：phase 位使 barrier 可跨迭代复用 Cluster 支持：通过 mapa 指令实现跨 CTA 的 barrier 操作 Leader 模式：只有 leader 线程执行 barrier 的 arrive/expect_tx 操作，避免重复 参考资料 CUTLASS GitHub 仓库 sm90_pipeline.hpp barrier.h NVIDIA PTX ISA - mbarrier CUDA Programming Guide - Asynchronous Barrier","link":"/2024/12/24/0x0B-pipeline-barrier-ptx/"},{"title":"0x0C CUTLASS SM90 TMA Descriptor Deep Dive","text":"This article explains NVIDIA Hopper (SM90) TMA (Tensor Memory Accelerator) mechanism, including Descriptor construction, Prefetch, Copy instructions, and CuTe Tensor integration. 示例代码: 0x0C_tma_descriptor.cu 核心要点速览 TMA Descriptor：128 字节结构，Host 端创建，Device 端使用 Transaction Barrier：用 expect_tx + complete_tx 跟踪字节数，非 arrive 次数 A/B 共享 Barrier：GEMM 中 A 和 B 共用一个 barrier，complete_tx 自动累加 Prefetch：预取 Descriptor 和数据到 L2 可提升性能 PTX 指令：cp.async.bulk.tensor.Xd.shared::cluster.global.mbarrier::complete_tx::bytes 1. TMA 概述1.1 什么是 TMATMA（Tensor Memory Accelerator）是 NVIDIA Hopper (SM90) 架构引入的硬件加速单元，专门用于高效的张量数据传输。它的主要特点： 异步执行：TMA 操作与 SM 计算完全异步，由专用硬件单元执行 硬件地址生成：自动计算多维张量的内存地址，无需软件计算 支持复杂布局：原生支持 swizzle、padding、stride 等复杂内存模式 Multicast 支持：单次操作可向 cluster 内多个 CTA 广播数据 1.2 TMA vs 传统 Copy 特性 传统 Copy TMA 地址计算 软件计算，占用寄存器 硬件计算，基于 Descriptor 执行单元 SM (CUDA Cores) 专用 TMA 硬件单元 同步方式 __syncthreads() mbarrier Swizzle 软件实现 硬件原生支持 多维支持 需要手动展开 原生 1D-5D 1.3 TMA 工作流程概览1234567891011121314151617181920212223242526graph LR subgraph Host[&quot;Host (Kernel Launch)&quot;] GT[GMEM Tensor] SL[SMEM Layout] MK[make_tma_copy] end subgraph Descriptor[&quot;TMA Descriptor (128B)&quot;] DESC[cuTensorMapEncodeTiled] end subgraph Device[&quot;Device (SM90)&quot;] PF[Prefetch Descriptor] LOAD[TMA Load/Store] MBAR[mbarrier sync] end GT --&gt; MK SL --&gt; MK MK --&gt; DESC DESC --&gt; PF PF --&gt; LOAD LOAD --&gt; MBAR style DESC fill:#e1f5fe style MBAR fill:#fff3e0 2. TMA Descriptor 结构2.1 类型定义TMA Descriptor 是一个 128 字节的数据结构，存储了完整的张量传输信息： 12345678// 源码: include/cute/arch/copy_sm90_desc.hpp:291-297#if (__CUDACC_VER_MAJOR__ &gt;= 12) &amp;&amp; !defined(__CUDACC_RTC__) using TmaDescriptor = CUtensorMap; // CUDA 12.0+ 使用原生类型 using Im2ColTmaDescriptor = CUtensorMap;#else using TmaDescriptor = struct alignas(64) { char bytes[128]; }; // 128字节，64字节对齐 using Im2ColTmaDescriptor = struct alignas(64) { char bytes[128]; };#endif 关键约束： 大小：128 字节 对齐：64 字节对齐（硬件要求） 存储位置：通常在 constant memory 或 global memory 2.2 Descriptor 编码参数Descriptor 通过 cuTensorMapEncodeTiled() CUDA Driver API 创建： 123456789101112131415// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:1024-1053CUresult result = cuTensorMapEncodeTiled( tma_desc, // 输出：TMA descriptor 指针 tma_format, // 数据类型 (FP32, FP16, BF16, INT8, etc.) tma_dim, // 维度数 (1-5) gmem_address, // 全局内存基地址 (16字节对齐) gmem_prob_shape, // 各维度大小 (uint32_t[5]) gmem_prob_stride + 1, // 各维度步长 (uint64_t[5], 字节为单位) smem_box_shape, // SMEM tile 各维度大小 (uint32_t[5], 最大256) smem_box_stride, // SMEM tile 步长 (uint32_t[5]) tma_interleave, // 交错模式 tma_swizzle, // Swizzle 模式 (32B, 64B, 128B) tma_l2Promotion, // L2 缓存策略 tma_oobFill // 越界填充值 (ZERO 或 CONSTANT)); 2.3 参数详解 参数 类型 约束 描述 gmem_address void* 16 字节对齐 全局内存基地址 gmem_prob_shape[i] uint32_t 1 ~ 2^32 第 i 维的元素数 gmem_prob_stride[i] uint64_t 16 字节对齐，最大 2^40 第 i 维的字节步长 smem_box_shape[i] uint32_t 1 ~ 256 SMEM tile 第 i 维大小 smem_box_stride[i] uint32_t 1 ~ 8 SMEM tile 第 i 维步长 2.4 Swizzle 模式Swizzle 用于优化 shared memory bank conflict： 12345// Swizzle 模式选项CU_TENSOR_MAP_SWIZZLE_NONE // 无 swizzleCU_TENSOR_MAP_SWIZZLE_32B // 32 字节 swizzleCU_TENSOR_MAP_SWIZZLE_64B // 64 字节 swizzleCU_TENSOR_MAP_SWIZZLE_128B // 128 字节 swizzle 2.5 数据类型映射1234567891011// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:906-918// TMA 数据格式映射CU_TENSOR_MAP_DATA_TYPE_UINT8 // uint8_t, int8_tCU_TENSOR_MAP_DATA_TYPE_UINT16 // uint16_t, int16_t, half_t, bfloat16_tCU_TENSOR_MAP_DATA_TYPE_UINT32 // uint32_t, int32_t, floatCU_TENSOR_MAP_DATA_TYPE_UINT64 // uint64_t, int64_t, doubleCU_TENSOR_MAP_DATA_TYPE_FLOAT16 // half_t (FP16 专用)CU_TENSOR_MAP_DATA_TYPE_FLOAT32 // floatCU_TENSOR_MAP_DATA_TYPE_FLOAT64 // doubleCU_TENSOR_MAP_DATA_TYPE_BFLOAT16 // bfloat16_tCU_TENSOR_MAP_DATA_TYPE_FLOAT32_FTZ // TF32 (Tensor Float 32) 3. TMA Descriptor 创建流程3.1 make_tma_copy APICUTLASS/CuTe 提供了高层 API 来创建 TMA copy atom： 12345678910111213141516171819202122232425262728// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:1221-1336template &lt;class TmaInternalType = void, class CopyOp, class GEngine, class GLayout, class SLayout, class CTA_Tiler, class Cluster_Size&gt;CUTE_HOST_RTCautomake_tma_copy(CopyOp const&amp; copy_op, // SM90_TMA_LOAD 等 Tensor&lt;GEngine,GLayout&gt; const&amp; gtensor, // 全局内存张量 SLayout const&amp; slayout, // 共享内存布局 CTA_Tiler const&amp; cta_tiler, // CTA tile 大小 Cluster_Size const&amp; cluster_size) // Cluster 大小{ // 1. 创建 CTA tile 和 cluster tile 的布局 auto cta_v_tile = make_identity_layout(shape(gtensor)).compose(cta_tiler); auto cta_t_tile = make_layout(cluster_size); // 2. 推导 TMA 内部数据类型 using TmaType = conditional_t&lt;is_same&lt;void, TmaInternalType&gt;::value, typename GEngine::value_type, TmaInternalType&gt;; // 3. 调用内部实现创建 TiledCopy return detail::make_tma_copy_tiled&lt;TmaType&gt;(copy_op, gtensor, slayout, cta_t_tile, cta_v_tile);} 3.2 使用示例123456789101112131415161718192021// 创建全局内存张量 (M x K 矩阵)auto gmem_tensor = make_tensor( make_gmem_ptr(ptr_A), make_shape(M, K), make_stride(K, Int&lt;1&gt;{}));// 定义共享内存布局 (128 x 64 tile，带 swizzle)auto smem_layout = make_layout( make_shape(Int&lt;128&gt;{}, Int&lt;64&gt;{}), GenColMajor{});// 创建 TMA copy atomauto tma_load_a = make_tma_copy( SM90_TMA_LOAD{}, // TMA Load 操作 gmem_tensor, // 源：全局内存张量 smem_layout, // 目标：共享内存布局 make_shape(Int&lt;128&gt;{}, Int&lt;64&gt;{}), // CTA tile 大小 Int&lt;1&gt;{} // Cluster 大小); 3.3 内部创建流程12345678910111213141516171819202122232425262728293031323334graph TB subgraph Input[&quot;输入&quot;] GT[GMEM Tensor&lt;br/&gt;shape, stride, ptr] SL[SMEM Layout&lt;br/&gt;shape, stride, swizzle] CT[CTA Tiler] CS[Cluster Size] end subgraph Process[&quot;处理流程&quot;] V1[提取 GMEM 参数] V2[计算 SMEM box 参数] V3[确定 Swizzle 模式] V4[创建 Basis Mapping] end subgraph Output[&quot;输出&quot;] DESC[TmaDescriptor&lt;br/&gt;128 bytes] AUX[AuxTmaParams&lt;br/&gt;g_stride_, TmaGmemBasis] TC[TiledCopy Atom] end GT --&gt; V1 SL --&gt; V2 SL --&gt; V3 V1 --&gt; V4 V2 --&gt; V4 V4 --&gt; DESC V4 --&gt; AUX DESC --&gt; TC AUX --&gt; TC style DESC fill:#e1f5fe style TC fill:#e8f5e9 3.4 参数验证TMA 对参数有严格要求，CUTLASS 在创建时会验证： 123456789101112// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:949-979// 地址对齐检查assert((reinterpret_cast&lt;uint64_t&gt;(gmem_address) &amp; 0b1111) == 0); // 16B 对齐// Shape 范围检查assert(gmem_prob_shape[i] &gt;= 1 &amp;&amp; gmem_prob_shape[i] &lt;= (1u &lt;&lt; 32));// Stride 对齐检查assert((gmem_prob_stride[i] &amp; 0b1111) == 0); // 16B 对齐// SMEM box 大小检查assert(smem_box_shape[i] &gt;= 1 &amp;&amp; smem_box_shape[i] &lt;= 256); 4. TMA Prefetch 机制4.1 Descriptor Prefetch将 TMA Descriptor 预取到 L2 缓存，加速后续 TMA 操作： 12345678910111213// 源码: include/cute/arch/copy_sm90_desc.hpp:302-317CUTE_HOST_DEVICE voidprefetch_tma_descriptor(TmaDescriptor const* desc_ptr){#if defined(CUTE_ARCH_TMA_SM90_ENABLED) uint64_t gmem_int_desc = reinterpret_cast&lt;uint64_t&gt;(desc_ptr); asm volatile ( &quot;prefetch.tensormap [%0];&quot; : : &quot;l&quot;(gmem_int_desc) : &quot;memory&quot;);#endif} PTX 指令： 1prefetch.tensormap [desc_addr]; 4.2 Data Prefetch预取数据到 L2 缓存（不写入 SMEM）： 12345678910111213141516// 源码: include/cute/arch/copy_sm90_tma.hpp:81-100struct SM90_TMA_LOAD_1D::PREFETCH{ CUTE_HOST_DEVICE static void copy(void const* desc_ptr, int32_t const&amp; crd0) {#if defined(CUTE_ARCH_TMA_SM90_ENABLED) uint64_t gmem_int_desc = reinterpret_cast&lt;uint64_t&gt;(desc_ptr); asm volatile ( &quot;cp.async.bulk.prefetch.tensor.1d.L2.global [%0, {%1}];&quot; : : &quot;l&quot;(gmem_int_desc), &quot;r&quot;(crd0) : &quot;memory&quot;);#endif }}; PTX 指令（1D-5D）： 1234567// 1Dcp.async.bulk.prefetch.tensor.1d.L2.global [desc], {crd0};// 2Dcp.async.bulk.prefetch.tensor.2d.L2.global [desc], {crd0, crd1};// 3D-5D 类似... 4.3 Prefetch 使用时机12345678910111213141516sequenceDiagram participant K as Kernel Start participant PF as Prefetch participant L2 as L2 Cache participant TMA as TMA Load K-&gt;&gt;PF: prefetch_tma_descriptor() PF-&gt;&gt;L2: 预取 Descriptor Note over L2: Descriptor 缓存 K-&gt;&gt;PF: TMA_LOAD::PREFETCH PF-&gt;&gt;L2: 预取第一批数据 K-&gt;&gt;TMA: TMA Load (实际传输) L2--&gt;&gt;TMA: Descriptor 命中 Note over TMA: 快速启动传输 5. TMA Copy 指令5.1 TMA Load从全局内存加载数据到共享内存： 12345678910111213141516171819202122232425262728// 源码: include/cute/arch/copy_sm90_tma.hpp:49-79struct SM90_TMA_LOAD_1D{ CUTE_HOST_DEVICE static void copy(void const* desc_ptr, // TMA Descriptor 指针 uint64_t* mbar_ptr, // mbarrier 指针 uint64_t cache_hint, // L2 缓存提示 void* smem_ptr, // 目标 SMEM 地址 int32_t const&amp; crd0) // 坐标 {#if defined(CUTE_ARCH_TMA_SM90_ENABLED) uint64_t gmem_int_desc = reinterpret_cast&lt;uint64_t&gt;(desc_ptr); uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr); uint32_t smem_int_ptr = cast_smem_ptr_to_uint(smem_ptr); asm volatile ( &quot;cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint&quot; &quot; [%0], [%1, {%3}], [%2], %4;&quot; : : &quot;r&quot;(smem_int_ptr), // 目标 SMEM &quot;l&quot;(gmem_int_desc), // TMA Descriptor &quot;r&quot;(smem_int_mbar), // mbarrier &quot;r&quot;(crd0), // 坐标 &quot;l&quot;(cache_hint) // 缓存提示 : &quot;memory&quot;);#endif }}; PTX 指令格式： 123456// 基本格式cp.async.bulk.tensor.{dim}d.{dst}.{src}.mbarrier::complete_tx::bytes [smem], [desc, {coords}], [mbar];// 完整示例 (2D)cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint [smem_ptr], [tma_desc, {crd0, crd1}], [mbar_ptr], cache_hint; 5.2 TMA Store从共享内存存储数据到全局内存： 12345678910111213141516171819202122// 源码: include/cute/arch/copy_sm90_tma.hpp:980-1001struct SM90_TMA_STORE_2D{ CUTE_HOST_DEVICE static void copy(void const* desc_ptr, void const* smem_ptr, int32_t const&amp; crd0, int32_t const&amp; crd1) {#if defined(CUTE_ARCH_TMA_SM90_ENABLED) uint64_t gmem_int_desc = reinterpret_cast&lt;uint64_t&gt;(desc_ptr); uint32_t smem_int_ptr = cast_smem_ptr_to_uint(smem_ptr); asm volatile ( &quot;cp.async.bulk.tensor.2d.global.shared::cta.bulk_group [%0, {%2, %3}], [%1];&quot; : : &quot;l&quot;(gmem_int_desc), // TMA Descriptor &quot;r&quot;(smem_int_ptr), // 源 SMEM &quot;r&quot;(crd0), &quot;r&quot;(crd1) // 坐标 : &quot;memory&quot;);#endif }}; PTX 指令格式： 1cp.async.bulk.tensor.{dim}d.global.shared::cta.bulk_group [desc, {coords}], [smem]; 5.3 TMA Multicast向 cluster 内多个 CTA 同时广播数据： 12345678910111213141516171819// 源码: include/cute/arch/copy_sm90_tma.hpp:275-306struct SM90_TMA_LOAD_MULTICAST_1D{ CUTE_HOST_DEVICE static void copy(void const* desc_ptr, uint64_t* mbar_ptr, uint64_t cache_hint, uint16_t multicast_mask, // 目标 CTA 掩码 void* smem_ptr, int32_t const&amp; crd0) { asm volatile ( &quot;cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.L2::cache_hint&quot; &quot; [%0], [%1, {%4}], [%2], %3, %5;&quot; : : &quot;r&quot;(smem_int_ptr), &quot;l&quot;(gmem_int_desc), &quot;r&quot;(smem_int_mbar), &quot;h&quot;(multicast_mask), &quot;r&quot;(crd0), &quot;l&quot;(cache_hint) : &quot;memory&quot;); }}; 5.4 TMA Fence/Commit/Wait用于 TMA Store 的同步控制： 1234567891011121314151617181920// 源码: include/cute/arch/copy_sm90_tma.hpp:1213-1274// Fence: 确保之前的 SMEM 写入完成CUTE_HOST_DEVICE static voidtma_store_fence() { asm volatile (&quot;fence.proxy.async.shared::cta;&quot;);}// Commit: 标记一组 TMA store 完成CUTE_HOST_DEVICE static voidtma_store_arrive() { asm volatile(&quot;cp.async.bulk.commit_group;&quot;);}// Wait: 等待最多 Count 个未完成的 TMA storetemplate &lt;int Count&gt;CUTE_HOST_DEVICE static voidtma_store_wait() { asm volatile(&quot;cp.async.bulk.wait_group.read %0;&quot; : : &quot;n&quot;(Count) : &quot;memory&quot;);} 5.5 TMA Load vs Store 对比 特性 TMA Load TMA Store 方向 GMEM → SMEM SMEM → GMEM 同步机制 mbarrier bulk_group + wait Multicast 支持 不支持 Scope shared::cluster shared::cta 6. TMA Transaction Barrier 与 expect_tx 机制6.1 Transaction Barrier 概述TMA 使用 Transaction Barrier（也称 ClusterTransactionBarrier）来同步异步数据传输。与普通 barrier 不同，Transaction Barrier 跟踪的是传输的字节数而非 arrive 次数。 123456789101112131415161718graph LR subgraph &quot;普通 Barrier&quot; A1[Thread 1 Arrive] --&gt; B1[Count++] A2[Thread 2 Arrive] --&gt; B1 B1 --&gt; C1{Count == Expected?} C1 --&gt;|Yes| D1[Barrier Complete] end subgraph &quot;Transaction Barrier&quot; E1[expect_tx 128B] --&gt; F1[Expected += 128] E2[TMA complete_tx 128B] --&gt; G1[Completed += 128] F1 --&gt; H1{Completed &gt;= Expected?} G1 --&gt; H1 H1 --&gt;|Yes| I1[Barrier Complete] end style F1 fill:#e3f2fd style G1 fill:#c8e6c9 6.2 expect_tx 与 complete_tx 配对Transaction Barrier 使用 expect_tx 和 complete_tx 两个操作来跟踪传输进度： 操作 执行者 作用 PTX 指令 expect_tx(bytes) Producer (CPU thread) 声明期望接收的字节数 mbarrier.arrive.expect_tx.shared::cta.b64 complete_tx(bytes) TMA 硬件 报告已完成传输的字节数 TMA 指令中 .mbarrier::complete_tx::bytes 修饰符 关键点：当 completed_bytes &gt;= expected_bytes 时，barrier 翻转（phase 变化），Consumer 的 wait 解除。 6.3 expect_tx 的 PTX 指令123456789101112// 源码: include/cutlass/arch/barrier.h:580-593static void arrive_and_expect_tx(ValueType const* smem_ptr, uint32_t transaction_bytes) {#if CUDA_BARRIER_ENABLED uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); asm volatile( &quot;{\\n\\t&quot; &quot;mbarrier.arrive.expect_tx.shared::cta.b64 _, [%1], %0; \\n\\t&quot; &quot;}&quot; : : &quot;r&quot;(transaction_bytes), &quot;r&quot;(smem_addr));#endif} PTX 指令解析： 12345678mbarrier.arrive.expect_tx.shared::cta.b64 _, [mbar_addr], tx_bytes;// 组成部分：// - mbarrier.arrive : barrier arrive 操作// - .expect_tx : 同时设置期望的传输字节数// - .shared::cta : barrier 在 CTA 本地共享内存// - .b64 : 64-bit barrier// - tx_bytes : 期望接收的字节数 6.4 TMA 指令中的 complete_txTMA Load 指令通过 .mbarrier::complete_tx::bytes 修饰符自动完成 complete_tx： 1234567// TMA Load 指令格式cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [smem_ptr], [tma_desc, {crd0, crd1}], [mbar_ptr];// 修饰符解析：// - .mbarrier::complete_tx::bytes : TMA 完成后自动对 mbarrier 调用 complete_tx// 传输的实际字节数由 TMA descriptor 中的 box_shape 决定 硬件自动完成：TMA 硬件在数据传输完成后，会自动向指定的 mbarrier 报告完成的字节数，无需软件干预。 6.5 expect_tx 在 Pipeline 中的使用在 CUTLASS Pipeline 中，Producer 在 acquire 阶段设置 expect_tx： 123456789101112// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:508-517CUTLASS_DEVICEvoid producer_acquire(uint32_t stage, uint32_t phase) { detail::pipeline_check_is_producer(params_.role); // 等待 Consumer 释放 buffer empty_barrier_ptr_[stage].wait(phase); // 设置期望接收的字节数 if (params_.is_leader) { full_barrier_ptr_[stage].arrive_and_expect_tx(params_.transaction_bytes); }} 6.6 transaction_bytes 的计算transaction_bytes 是每个 stage 期望传输的总字节数，由 tile 大小决定： 1234567// 示例：计算 TMA transaction bytesstatic constexpr uint32_t TmaTransactionBytes = size(SmemLayoutA) * sizeof(ElementA) + // A tile 字节数 size(SmemLayoutB) * sizeof(ElementB); // B tile 字节数// 例如：A = 128x64 FP16, B = 64x128 FP16// TmaTransactionBytes = (128*64 + 64*128) * 2 = 32768 bytes 6.7 A 和 B 共享同一个 Barrier：累加机制关键设计：在 GEMM 中，A tile 和 B tile 的 TMA 操作共享同一个 barrier，而非各自使用独立的 barrier。 6.7.1 为什么 A 和 B 共享 barrier？从 CUTLASS 源码可以看到，TmaTransactionBytes 是 A 和 B 的总和： 1234// 源码: include/cutlass/gemm/collective/sm90_mma_tma_warpspecialized_cooperative.hppstatic constexpr uint32_t TmaTransactionBytes = cutlass::bits_to_bytes(size&lt;0&gt;(SmemLayoutA{}) * size&lt;1&gt;(SmemLayoutA{}) * sizeof_bits&lt;ElementA&gt;::value) + cutlass::bits_to_bytes(size&lt;0&gt;(SmemLayoutB{}) * size&lt;1&gt;(SmemLayoutB{}) * sizeof_bits&lt;ElementB&gt;::value); 原因： GEMM 语义需要：Consumer 必须等 A 和 B 都到齐才能执行 MMA，单独等待 A 没有意义 减少 barrier 开销：每个 barrier 占用 SMEM，减少同步点提高效率 硬件支持累加：Transaction barrier 天然支持多次 complete_tx 累加 6.7.2 complete_tx 的累加机制TMA 硬件的 complete_tx 支持累加，多个 TMA 操作的完成字节数会自动累加： 12345678910111213141516171819202122sequenceDiagram participant P as Producer participant MB as mbarrier participant TMA_A as TMA (A) participant TMA_B as TMA (B) participant C as Consumer P-&gt;&gt;MB: expect_tx(A_bytes + B_bytes) Note over MB: expected = 128KB par TMA A and B 可并行 P-&gt;&gt;TMA_A: TMA Load A P-&gt;&gt;TMA_B: TMA Load B end TMA_A-&gt;&gt;MB: complete_tx(64KB) Note over MB: completed = 64KB TMA_B-&gt;&gt;MB: complete_tx(64KB) Note over MB: completed = 128KB ≥ expected C-&gt;&gt;MB: wait() 解除 6.7.3 代码体现12345678// Producerpipeline.producer_acquire(state); // expect_tx(A_bytes + B_bytes)copy(tma_load_A.with(*mbar), ...); // complete_tx(A_bytes)copy(tma_load_B.with(*mbar), ...); // complete_tx(B_bytes)// Consumerpipeline.consumer_wait(state); // 等待 completed &gt;= expected// A 和 B 都已就绪 6.8 expect_tx vs 普通 arrive 对比 维度 普通 Barrier (arrive) Transaction Barrier (expect_tx) 跟踪单位 arrive 次数 字节数 完成条件 arrive_count == expected completed_bytes &gt;= expected_bytes 典型用途 线程同步 异步数据传输 信号方 软件线程 硬件 (TMA) PTX 指令 mbarrier.arrive mbarrier.arrive.expect_tx + TMA .complete_tx 6.9 expect_tx 关键要点 必须在 TMA 发起前调用 字节数必须匹配：expect_tx 的字节数 = TMA 实际传输的字节数 只有 leader 线程执行 支持累加：多个 TMA 的 complete_tx 自动累加 7. CuTe Tensor 与 TMA 集成7.1 Copy_Traits 结构TMA 操作通过 Copy_Traits 封装： 1234567891011121314151617181920212223242526272829// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:98-166template &lt;class NumBitsPerTMA, class AuxParams_&gt;struct Copy_Traits&lt;SM90_TMA_LOAD, NumBitsPerTMA, AuxParams_&gt;{ using ThrID = Layout&lt;_1&gt;; // 单线程执行 using SrcLayout = Layout&lt;Shape&lt;_1,NumBitsPerTMA&gt;&gt;; using DstLayout = Layout&lt;Shape&lt;_1,NumBitsPerTMA&gt;&gt;; using RefLayout = SrcLayout; // TMA Descriptor 存储 TmaDescriptor tma_desc_; // 辅助参数（stride 映射等） using AuxParams = AuxParams_; AuxParams aux_params_; // 获取 Descriptor 指针 CUTE_HOST_DEVICE constexpr TmaDescriptor const* get_tma_descriptor() const { return &amp;tma_desc_; } // 生成坐标张量 template &lt;class GShape&gt; CUTE_HOST_DEVICE constexpr auto get_tma_tensor(GShape const&amp; g_shape) const { return make_coord_tensor(make_layout(g_shape, aux_params_.g_stride_)); }}; 7.2 AuxTmaParams 辅助参数存储 GMEM 到 TMA 坐标的映射关系： 123456789101112// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:50-58template &lt;class GmemTmaBasisStrides_, class TmaGmemBasis_, class TmaSwizzle_&gt;struct AuxTmaParams { using GmemStrides = GmemTmaBasisStrides_; GmemStrides g_stride_; // GMEM mode → TMA coord 的映射 using TmaGmemBasis = TmaGmemBasis_; // 静态 basis 信息 static_assert(is_static&lt;TmaGmemBasis&gt;::value); using TmaSwizzle = TmaSwizzle_; // Swizzle 模式 static_assert(is_static&lt;TmaSwizzle&gt;::value);}; 7.3 可执行 TMA Copy运行时携带 barrier 和 cache hint： 12345678910111213141516// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:169-198template &lt;class NumBitsPerTMA&gt;struct Copy_Traits&lt;SM90_TMA_LOAD_OP, NumBitsPerTMA&gt; : TMA_LOAD_Unpack&lt;SM90_TMA_LOAD_OP, NumBitsPerTMA&gt;{ // 运行时参数 tuple&lt; TmaDescriptor const*, // Descriptor 指针 uint64_t*, // mbarrier 指针 uint64_t // L2 cache hint &gt; const opargs_; CUTE_HOST_DEVICE Copy_Traits(TmaDescriptor const* desc, uint64_t* mbar, uint64_t cache) : opargs_(desc, mbar, cache) {}}; 7.4 Copy Unpack 执行将 CuTe tensor copy 转换为 TMA 指令： 12345678910111213141516171819202122232425262728// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:60-87template &lt;class CopyOp, class... Args&gt;struct TMA_LOAD_Unpack{ template &lt;class TS, class SLayout, class TD, class DLayout&gt; CUTE_HOST_DEVICE friend constexpr void copy_unpack(Copy_Traits&lt;CopyOp, Args...&gt; const&amp; traits, Tensor&lt;TS,SLayout&gt; const&amp; src, // GMEM 坐标张量 Tensor&lt;TD,DLayout&gt; &amp; dst) // SMEM 数据张量 { // 验证目标是共享内存 static_assert(is_smem&lt;TD&gt;::value, &quot;SM90_TMA_LOAD requires the destination be shared memory.&quot;); // 提取源坐标 auto src_coord = src.data().coord_; // 获取目标指针 void* dst_ptr = cute::raw_pointer_cast(dst.data()); // 展开并调用 TMA 指令 return detail::explode_tuple(detail::CallCOPY&lt;CopyOp&gt;{}, traits.opargs_, ..., make_tuple(dst_ptr), ..., src_coord, ...); }}; 7.5 完整数据流123456789101112131415161718192021222324252627282930313233343536graph TB subgraph CuTe[&quot;CuTe Layer&quot;] GS[gmem_tensor&lt;br/&gt;shape + stride + ptr] CT[TiledCopy&lt;br/&gt;Copy_Traits] CR[coord_tensor&lt;br/&gt;get_tma_tensor] ST[smem_tensor] end subgraph TMA[&quot;TMA Layer&quot;] DESC[TmaDescriptor] MBAR[mbarrier] PTX[PTX Instructions] end subgraph HW[&quot;Hardware&quot;] TU[TMA Unit] GMEM[(Global Memory)] SMEM[(Shared Memory)] end GS --&gt; CT CT --&gt; DESC CT --&gt; CR CR --&gt;|src coords| PTX ST --&gt;|dst ptr| PTX DESC --&gt;|desc ptr| PTX MBAR --&gt;|mbar ptr| PTX PTX --&gt; TU TU --&gt; GMEM TU --&gt; SMEM style DESC fill:#e1f5fe style MBAR fill:#fff3e0 style TU fill:#e8f5e9 8. 完整使用示例8.1 创建 TMA Copy1234567891011121314151617181920212223242526272829// 步骤 1: 定义矩阵参数constexpr int M = 4096;constexpr int K = 4096;constexpr int TILE_M = 128;constexpr int TILE_K = 64;// 步骤 2: 创建全局内存张量half_t* ptr_A = ...; // 设备内存指针auto gmem_A = make_tensor( make_gmem_ptr(ptr_A), make_shape(M, K), make_stride(K, Int&lt;1&gt;{}) // Row-major);// 步骤 3: 定义共享内存布局（带 swizzle）auto smem_layout_A = composition( Swizzle&lt;3, 3, 3&gt;{}, make_layout(make_shape(Int&lt;TILE_M&gt;{}, Int&lt;TILE_K&gt;{}), make_stride(Int&lt;TILE_K&gt;{}, Int&lt;1&gt;{})));// 步骤 4: 创建 TMA copy atomauto tma_load_A = make_tma_copy( SM90_TMA_LOAD{}, gmem_A, smem_layout_A, make_shape(Int&lt;TILE_M&gt;{}, Int&lt;TILE_K&gt;{}), Int&lt;1&gt;{}); 8.2 Kernel 中使用 TMA123456789101112131415161718192021222324252627282930313233343536373839404142__global__ void kernel(TmaLoadA tma_load_A, ...) { // 共享内存 extern __shared__ char smem[]; half_t* smem_A = reinterpret_cast&lt;half_t*&gt;(smem); // mbarrier __shared__ uint64_t mbar; // 初始化 barrier (仅 thread 0) if (threadIdx.x == 0) { mbarrier_init(&amp;mbar, 1); // 期望 1 次 arrive // Prefetch descriptor prefetch_tma_descriptor(tma_load_A.get_tma_descriptor()); } __syncthreads(); // 计算 tile 坐标 int tile_m = blockIdx.x; int tile_k = 0; // 获取坐标张量 auto coord_tensor = tma_load_A.get_tma_tensor(make_shape(M, K)); auto coord = coord_tensor(tile_m, tile_k); // 创建 SMEM 张量 auto smem_tensor = make_tensor(make_smem_ptr(smem_A), smem_layout_A); // 发起 TMA load (仅 thread 0) if (threadIdx.x == 0) { // 设置期望传输字节数 mbarrier_arrive_expect_tx(&amp;mbar, size(smem_tensor) * sizeof(half_t)); // 发起 TMA copy(tma_load_A.with(&amp;mbar, 0), coord, smem_tensor); } // 等待 TMA 完成 mbarrier_wait(&amp;mbar, 0); // phase = 0 // 使用 smem_A 进行计算...} 8.3 与 Pipeline 配合1234567891011121314151617181920212223242526272829// 多 stage pipelinefor (int k = 0; k &lt; num_k_tiles; ++k) { // 获取当前 stage auto stage = k % Stages; auto&amp; smem_A = smem_A_stages[stage]; auto&amp; mbar = mbar_full[stage]; // Producer: TMA load if (is_producer_thread) { pipeline.producer_acquire(pipe_state); auto coord = coord_tensor(tile_m, k); copy(tma_load_A.with(pipeline.producer_get_barrier(pipe_state), 0), coord, smem_A); ++pipe_state; } // Consumer: MMA compute if (is_consumer_thread) { pipeline.consumer_wait(pipe_state_c); // 使用 smem_A 进行 MMA gemm(smem_A, smem_B, accum); pipeline.consumer_release(pipe_state_c); ++pipe_state_c; }} 9. TMA API 到 PTX 映射总览 CuTe/CUTLASS API PTX 指令 prefetch_tma_descriptor() prefetch.tensormap [addr] SM90_TMA_LOAD::PREFETCH cp.async.bulk.prefetch.tensor.Xd.L2.global SM90_TMA_LOAD_Xd::copy() cp.async.bulk.tensor.Xd.shared::cluster.global.mbarrier::complete_tx::bytes SM90_TMA_LOAD_MULTICAST 同上 + .multicast::cluster SM90_TMA_STORE_Xd::copy() cp.async.bulk.tensor.Xd.global.shared::cta.bulk_group tma_store_fence() fence.proxy.async.shared::cta tma_store_arrive() cp.async.bulk.commit_group tma_store_wait&lt;N&gt;() cp.async.bulk.wait_group.read N arrive_and_expect_tx(bytes) mbarrier.arrive.expect_tx.shared::cta.b64 expect_transaction(bytes) mbarrier.expect_tx.shared::cta.b64 complete_transaction(bytes) mbarrier.complete_tx.shared::cluster.b64 10. 关键要点总结 TMA Descriptor 是核心：128 字节结构，编码完整的张量传输信息 Host 端创建，Device 端使用：Descriptor 在 kernel launch 前创建 Transaction Barrier 同步：expect_tx + complete_tx 跟踪字节数 A/B 共享 Barrier：GEMM 中 A 和 B 共用一个 barrier，complete_tx 累加 Prefetch 提升性能：预取 descriptor 和数据到 L2 缓存 11. 相关文档 Pipeline 与 mbarrier 深度解析 - mbarrier 原理、DSMEM、PTX 指令 TMA Multicast 深度解析 - Cluster 内数据广播 Cooperative Kernel Pipeline 深度解析 - GEMM 中的 Pipeline 使用 参考资料 CUTLASS GitHub 仓库 copy_sm90_tma.hpp copy_sm90_desc.hpp copy_traits_sm90_tma.hpp NVIDIA PTX ISA - TMA CUDA Programming Guide - TMA","link":"/2024/12/24/0x0C-tma-descriptor/"},{"title":"0x0D CUTLASS SM90 TMA Multicast Deep Dive","text":"This article explains NVIDIA Hopper (SM90) TMA Multicast mechanism, including multicast mask calculation, Cluster-Multicast relationship, and practical GEMM applications. 核心要点速览 Multicast Mask：16-bit，每位对应 Cluster 内一个 CTA A/B 广播方向：A 沿 N 方向（同行），B 沿 M 方向（同列） EmptyBarrier arrival count：(cluster_x + cluster_y - 1) × warpgroups Consumer release 跨 CTA：使用 DSMEM + mbarrier.arrive.shared::cluster FullBarrier：TMA 硬件自动对所有目标 CTA 的 mbarrier 调用 complete_tx 1. TMA Multicast 概述1.1 什么是 TMA MulticastTMA Multicast 是 Hopper 架构 TMA 的高级特性，允许单次 TMA 操作将数据广播到 Cluster 内的多个 CTA（Thread Block）。这对于 GEMM 等需要数据复用的场景非常有价值。 1.2 为什么需要 Multicast在 GEMM 中，矩阵 A 和 B 的数据被多个输出 tile 共享： 1234567891011121314151617181920212223242526graph TB subgraph &quot;Cluster (2x2)&quot; subgraph &quot;Row 0&quot; CTA00[&quot;CTA(0,0)&lt;br/&gt;C[0,0]&quot;] CTA01[&quot;CTA(0,1)&lt;br/&gt;C[0,1]&quot;] end subgraph &quot;Row 1&quot; CTA10[&quot;CTA(1,0)&lt;br/&gt;C[1,0]&quot;] CTA11[&quot;CTA(1,1)&lt;br/&gt;C[1,1]&quot;] end end A0[&quot;A row 0&quot;] --&gt; CTA00 A0 --&gt; CTA01 A1[&quot;A row 1&quot;] --&gt; CTA10 A1 --&gt; CTA11 B0[&quot;B col 0&quot;] --&gt; CTA00 B0 --&gt; CTA10 B1[&quot;B col 1&quot;] --&gt; CTA01 B1 --&gt; CTA11 style A0 fill:#e3f2fd style A1 fill:#e3f2fd style B0 fill:#fff3e0 style B1 fill:#fff3e0 矩阵 A：同一行的 CTA 共享相同的 A tile（沿 N 方向广播） 矩阵 B：同一列的 CTA 共享相同的 B tile（沿 M 方向广播） 1.3 CTA 数据共享关系12345678910111213C[M, N] = A[M, K] x B[K, N]2x2 Cluster CTA assignment: N direction N_tile_0 N_tile_1 +-----------+-----------+M_tile_0| CTA(0,0) | CTA(0,1) | &lt;- both need same A[M_tile_0] +-----------+-----------+M_tile_1| CTA(1,0) | CTA(1,1) | &lt;- both need same A[M_tile_1] +-----------+-----------+ ^ ^ need same need same B[N_tile_0] B[N_tile_1] Matrix Sharing Rule Example A same-row CTAs share CTA(0,0), CTA(0,1) share A[M_tile_0] B same-col CTAs share CTA(0,0), CTA(1,0) share B[N_tile_0] 1.4 Multicast Loading Strategy1234567891011121314151617181920212223242526272829Matrix A (first column CTAs load):+-------------------------------------------------------------+| || CTA(0,0) loads A[M_tile_0, K] || | || +--&gt; multicast --&gt; CTA(0,0), CTA(0,1) SMEM || || CTA(1,0) loads A[M_tile_1, K] || | || +--&gt; multicast --&gt; CTA(1,0), CTA(1,1) SMEM || || CTA(0,1), CTA(1,1): don't load A (receive from multicast) || |+-------------------------------------------------------------+Matrix B (first row CTAs load):+-------------------------------------------------------------+| || CTA(0,0) loads B[K, N_tile_0] || | || +--&gt; multicast --&gt; CTA(0,0), CTA(1,0) SMEM || || CTA(0,1) loads B[K, N_tile_1] || | || +--&gt; multicast --&gt; CTA(0,1), CTA(1,1) SMEM || || CTA(1,0), CTA(1,1): don't load B (receive from multicast) || |+-------------------------------------------------------------+ 每个 CTA 的工作量： CTA 加载 A 加载 B 说明 CTA(0,0) ✓ ✓ 最忙，加载两份数据 CTA(0,1) ✗ ✓ 只加载 B CTA(1,0) ✓ ✗ 只加载 A CTA(1,1) ✗ ✗ 不加载，全部 multicast 带宽节省：A 加载 2 次 (vs 4 次)，B 加载 2 次 (vs 4 次) → 节省 50%！ 1.5 数据流向图示1234567891011121314151617 Global Memory | +--------------+---------------+ | | | v v v A[M_tile_0] A[M_tile_1] B[N_tile_0] B[N_tile_1] | | | | | | | | CTA(0,0) CTA(1,0) CTA(0,0) CTA(0,1) load A0 load A1 load B0 load B1 | | | | | | | | +-----+-----+ +-----+-----+ +-----+-----+ +-----+-----+ | | | | | | | | v v v v v v v vCTA(0,0) CTA(0,1) CTA(1,0) CTA(1,1) CTA(0,0) CTA(1,0) CTA(0,1) CTA(1,1) SMEM_A SMEM_A SMEM_A SMEM_A SMEM_B SMEM_B SMEM_B SMEM_B 1.6 Multicast vs 独立加载 方式 带宽消耗 延迟 每个 CTA 独立加载 N × 数据量 竞争 L2/HBM Multicast 1 × 数据量 一次加载，硬件广播 对于 2×2 Cluster，Multicast 可节省约 50% 的内存带宽。 2. Multicast Mask 原理2.1 Mask 结构Multicast mask 是一个 16-bit 整数，每一位对应 Cluster 内的一个 CTA： 1234567891011Cluster 内 CTA 编号 (block_rank_in_cluster): Cluster 2x2 示例: +-------+-------+ | CTA 0 | CTA 1 | (row 0) +-------+-------+ | CTA 2 | CTA 3 | (row 1) +-------+-------+ multicast_mask = 0b0011 表示广播到 CTA 0 和 CTA 1 multicast_mask = 0b0101 表示广播到 CTA 0 和 CTA 2 2.2 CTA 编号计算CTA 在 Cluster 内的编号由 Layout&lt;ClusterShape&gt; 决定： 123456// 源码: include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp:331constexpr uint32_t cluster_shape_x = get&lt;0&gt;(typename DispatchPolicy::ClusterShape());uint2 cluster_local_block_id = { block_rank_in_cluster % cluster_shape_x, // x = M 方向 block_rank_in_cluster / cluster_shape_x // y = N 方向}; 对于 ClusterShape = Shape&lt;_2, _2&gt;： block_rank = 0 → (x=0, y=0) block_rank = 1 → (x=1, y=0) block_rank = 2 → (x=0, y=1) block_rank = 3 → (x=1, y=1) 2.3 Mask 计算逻辑12345678910111213141516171819202122graph LR subgraph Input CS[ClusterShape] BR[block_rank_in_cluster] end subgraph Process LO[Layout ClusterShape] LC[local_block_id&lt;br/&gt;x, y] MA[Mask for A&lt;br/&gt;沿 N 方向] MB[Mask for B&lt;br/&gt;沿 M 方向] end CS --&gt; LO BR --&gt; LC LO --&gt; MA LO --&gt; MB LC --&gt; MA LC --&gt; MB style MA fill:#e3f2fd style MB fill:#fff3e0 3. Multicast Mask 实现详解3.1 矩阵 A 的 Mask（沿 N 方向广播）矩阵 A 的同一行数据被 N 方向的所有 CTA 共享： 12345678// 源码: include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp:357-362uint16_t mcast_mask_a = 0;auto block_layout = Layout&lt;typename DispatchPolicy::ClusterShape&gt;{}; // (m,n) -&gt; block_id// 固定 M 坐标（当前 CTA 的 x），遍历所有 N 坐标for (int n = 0; n &lt; size&lt;1&gt;(block_layout); ++n) { mcast_mask_a |= (uint16_t(1) &lt;&lt; block_layout(cluster_local_block_id.x, n, Int&lt;0&gt;{}));} 示例：ClusterShape = (2, 2)，当前 CTA 的 x = 0 n block_layout(0, n) 累加 mask 0 0 0b0001 1 2 0b0101 最终 mcast_mask_a = 0b0101（广播到 CTA 0 和 CTA 2） 3.2 矩阵 B 的 Mask（沿 M 方向广播）矩阵 B 的同一列数据被 M 方向的所有 CTA 共享： 12345678// 源码: include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp:364-368uint16_t mcast_mask_b = 0;auto block_layout = Layout&lt;typename DispatchPolicy::ClusterShape&gt;{};// 固定 N 坐标（当前 CTA 的 y），遍历所有 M 坐标for (int m = 0; m &lt; size&lt;0&gt;(block_layout); ++m) { mcast_mask_b |= (uint16_t(1) &lt;&lt; block_layout(m, cluster_local_block_id.y, Int&lt;0&gt;{}));} 示例：ClusterShape = (2, 2)，当前 CTA 的 y = 0 m block_layout(m, 0) 累加 mask 0 0 0b0001 1 1 0b0011 最终 mcast_mask_b = 0b0011（广播到 CTA 0 和 CTA 1） 3.3 完整 Mask 计算图示1234567891011121314151617181920212223242526272829303132graph TB subgraph &quot;Cluster 2x2&quot; CTA0[&quot;CTA 0&lt;br/&gt;(0,0)&quot;] CTA1[&quot;CTA 1&lt;br/&gt;(1,0)&quot;] CTA2[&quot;CTA 2&lt;br/&gt;(0,1)&quot;] CTA3[&quot;CTA 3&lt;br/&gt;(1,1)&quot;] end subgraph &quot;Matrix A Masks&quot; MA0[&quot;CTA 0: mask_a=0b0101&lt;br/&gt;广播到 CTA 0,2&quot;] MA1[&quot;CTA 1: mask_a=0b1010&lt;br/&gt;广播到 CTA 1,3&quot;] end subgraph &quot;Matrix B Masks&quot; MB0[&quot;CTA 0: mask_b=0b0011&lt;br/&gt;广播到 CTA 0,1&quot;] MB2[&quot;CTA 2: mask_b=0b1100&lt;br/&gt;广播到 CTA 2,3&quot;] end CTA0 -.-&gt; MA0 CTA1 -.-&gt; MA1 CTA2 -.-&gt; MA0 CTA3 -.-&gt; MA1 CTA0 -.-&gt; MB0 CTA1 -.-&gt; MB0 CTA2 -.-&gt; MB2 CTA3 -.-&gt; MB2 style MA0 fill:#e3f2fd style MA1 fill:#e3f2fd style MB0 fill:#fff3e0 style MB2 fill:#fff3e0 4. create_tma_multicast_mask 函数4.1 通用版本CuTe 提供了通用的 multicast mask 创建函数： 123456789101112131415161718192021222324252627282930313233// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:1439-1469template &lt;class CTA_Layout, class CTA_Coord&gt;CUTE_HOST_DEVICE constexpruint16_tcreate_tma_multicast_mask(CTA_Layout const&amp; cta_layout, // Cluster 布局 CTA_Coord const&amp; cta_coord) // 当前 CTA 坐标{ // 将坐标分解为参与 multicast 的部分和不参与的部分 auto [rest_coord, mcast_coord] = slice_and_offset(cta_coord, cta_layout); // 获取 multicast 相关的子布局 auto mcast_layout = cta_layout.slice(rest_coord); uint16_t mcast_mask = 0; // 优化路径：rank-1 且深度 &lt;= 1 if constexpr (rank(mcast_layout) == 1 &amp;&amp; depth(mcast_layout) &lt;= 1) { // 位 smearing 技术 mcast_mask = uint16_t(1) &lt;&lt; mcast_coord; mcast_mask |= mcast_mask &lt;&lt; (1 * stride&lt;0&gt;(mcast_layout)); mcast_mask |= mcast_mask &lt;&lt; (2 * stride&lt;0&gt;(mcast_layout)); mcast_mask |= mcast_mask &lt;&lt; (4 * stride&lt;0&gt;(mcast_layout)); mcast_mask |= mcast_mask &lt;&lt; (8 * stride&lt;0&gt;(mcast_layout)); } else { // 通用路径：遍历所有位置 for (int i = 0; i &lt; size(mcast_layout); ++i) { mcast_mask |= (uint16_t(1) &lt;&lt; mcast_layout(i)); } } return mcast_mask;} 4.2 带 Mode 参数的版本用于指定沿哪个维度进行 multicast： 1234567891011121314// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:1475-1479template &lt;int... Mode, class CTA_Layout, class CTA_Coord&gt;CUTE_HOST_DEVICE constexpruint16_tcreate_tma_multicast_mask(CTA_Layout const&amp; cta_layout, CTA_Coord const&amp; cta_coord){ // 只保留指定 Mode 的坐标，其他设为 0 auto proj_coord = cute::make_tuple( cute::conditional_return&lt;cute::is_one_of&lt;Int&lt;Mode&gt;...&gt;::template eval&lt;Int&lt;I&gt;&gt;()&gt;( get&lt;I&gt;(cta_coord), Int&lt;0&gt;{})... ); return create_tma_multicast_mask(cta_layout, proj_coord);} 使用示例： 123456789// 源码: include/cutlass/gemm/collective/sm90_sparse_mma_tma_gmma_ss_warpspecialized.hpp:425-431Layout cta_layout_mnk = make_layout(ClusterShape{});auto cta_coord_mnk = cta_layout_mnk.get_flat_coord(block_rank_in_cluster);// Mode&lt;1&gt; = N 方向，用于矩阵 Auint16_t mcast_mask_a = create_tma_multicast_mask&lt;1&gt;(cta_layout_mnk, cta_coord_mnk);// Mode&lt;0&gt; = M 方向，用于矩阵 Buint16_t mcast_mask_b = create_tma_multicast_mask&lt;0&gt;(cta_layout_mnk, cta_coord_mnk); 5. TMA Multicast 指令5.1 PTX 指令格式12cp.async.bulk.tensor.{dim}d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.L2::cache_hint [smem_ptr], [tma_desc, {coords}], [mbar_ptr], multicast_mask, cache_hint; 关键修饰符： .multicast::cluster：启用 cluster 内 multicast multicast_mask：16-bit 掩码，指示目标 CTA 5.2 实现代码123456789101112131415161718192021222324252627282930// 源码: include/cute/arch/copy_sm90_tma.hpp:626-652struct SM90_TMA_LOAD_MULTICAST_1D{ CUTE_HOST_DEVICE static void copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask, // &lt;-- multicast 掩码 uint64_t cache_hint, void* smem_ptr, int32_t const&amp; crd0) {#if defined(CUTE_ARCH_TMA_SM90_ENABLED) uint64_t gmem_int_desc = reinterpret_cast&lt;uint64_t&gt;(desc_ptr); uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr); uint32_t smem_int_ptr = cast_smem_ptr_to_uint(smem_ptr); asm volatile ( &quot;cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.L2::cache_hint&quot; &quot; [%0], [%1, {%4}], [%2], %3, %5;&quot; : : &quot;r&quot;(smem_int_ptr), // 目标 SMEM 地址 &quot;l&quot;(gmem_int_desc), // TMA Descriptor &quot;r&quot;(smem_int_mbar), // mbarrier 地址 &quot;h&quot;(multicast_mask), // 16-bit multicast 掩码 &quot;r&quot;(crd0), // 坐标 &quot;l&quot;(cache_hint) // L2 缓存提示 : &quot;memory&quot;);#endif }}; 5.3 Copy_Traits 集成123456789101112131415// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:278-296// .with() 方法绑定运行时参数CUTE_HOST_DEVICE constexprCopy_Traits&lt;SM90_TMA_LOAD_MULTICAST_OP, NumBitsPerTMA&gt;with(uint64_t&amp; tma_load_mbar, uint16_t const&amp; multicast_mask, TMA::CacheHintSm90 const&amp; cache_hint = TMA::CacheHintSm90::EVICT_NORMAL) const{ return { &amp;tma_desc_, &amp;tma_load_mbar, multicast_mask, static_cast&lt;uint64_t&gt;(cache_hint) };} 6. 在 GEMM Collective 中的应用6.1 Warp-Specialized Mainloop1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// 源码: include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp:310-390template &lt;class... Args&gt;CUTLASS_DEVICE voidload(Params const&amp; mainloop_params, MainloopPipeline pipeline, PipelineState smem_pipe_write, cute::tuple&lt;Args...&gt; const&amp; load_inputs, BlockCoord const&amp; blk_coord, KTileIterator k_tile_iter, int k_tile_count, int lane_idx, uint32_t block_rank_in_cluster, SharedStorage&amp; shared_storage){ // 步骤 1: 计算本地 CTA 坐标 constexpr uint32_t cluster_shape_x = get&lt;0&gt;(ClusterShape{}); uint2 cluster_local_block_id = { block_rank_in_cluster % cluster_shape_x, block_rank_in_cluster / cluster_shape_x }; // 步骤 2: 根据 CTA 坐标分区 TMA descriptor auto [tAgA, tAsA] = tma_partition_A(cluster_local_block_id.y); auto [tBgB, tBsB] = tma_partition_B(cluster_local_block_id.x); // 步骤 3: 计算 multicast mask uint16_t mcast_mask_a = 0; uint16_t mcast_mask_b = 0; auto block_layout = Layout&lt;ClusterShape&gt;{}; // A: 沿 N 方向广播（固定 M，遍历 N） CUTLASS_PRAGMA_UNROLL for (int n = 0; n &lt; size&lt;1&gt;(block_layout); ++n) { mcast_mask_a |= (uint16_t(1) &lt;&lt; block_layout(cluster_local_block_id.x, n, Int&lt;0&gt;{})); } // B: 沿 M 方向广播（固定 N，遍历 M） CUTLASS_PRAGMA_UNROLL for (int m = 0; m &lt; size&lt;0&gt;(block_layout); ++m) { mcast_mask_b |= (uint16_t(1) &lt;&lt; block_layout(m, cluster_local_block_id.y, Int&lt;0&gt;{})); } // 步骤 4: Mainloop - TMA Load with Multicast CUTLASS_PRAGMA_NO_UNROLL for ( ; k_tile_count &gt; 0; --k_tile_count) { // 获取 pipeline stage pipeline.producer_acquire(smem_pipe_write); BarrierType* tma_barrier = pipeline.producer_get_barrier(smem_pipe_write); int write_stage = smem_pipe_write.index(); // 发起 TMA Multicast Load copy(mainloop_params.tma_load_a.with(*tma_barrier, mcast_mask_a), tAgA(_,_,_,*k_tile_iter), tAsA(_,_,_,write_stage)); copy(mainloop_params.tma_load_b.with(*tma_barrier, mcast_mask_b), tBgB(_,_,_,*k_tile_iter), tBsB(_,_,_,write_stage)); ++k_tile_iter; ++smem_pipe_write; }} 6.2 执行流程时序图1234567891011121314151617181920sequenceDiagram participant CTA0 as CTA 0 (Producer) participant CTA1 as CTA 1 participant TMA as TMA Unit participant GMEM as Global Memory participant SMEM0 as SMEM (CTA 0) participant SMEM1 as SMEM (CTA 1) Note over CTA0,CTA1: Cluster 2x1, 加载矩阵 B (mask=0b11) CTA0-&gt;&gt;CTA0: 计算 mcast_mask_b = 0b11 CTA0-&gt;&gt;TMA: copy(tma_load_b.with(mbar, 0b11), ...) TMA-&gt;&gt;GMEM: 读取 B tile TMA-&gt;&gt;SMEM0: 写入数据 (multicast) TMA-&gt;&gt;SMEM1: 写入数据 (multicast) TMA-&gt;&gt;CTA0: signal mbar (CTA 0) TMA-&gt;&gt;CTA1: signal mbar (CTA 1) Note over CTA0,CTA1: 两个 CTA 同时收到数据 7. Multicast 性能优化7.1 最佳实践 选择合适的 Cluster Shape 太小：multicast 收益有限 太大：受限于 SM 资源，可能降低 occupancy 对齐数据 TMA 要求 16 字节对齐 Multicast 不会改变对齐要求 平衡 A 和 B 的 multicast ClusterShape = (M, N) A multicast 因子 = N B multicast 因子 = M 7.2 Cluster Shape 选择指南 Cluster Shape A Multicast B Multicast 总带宽节省 (1, 1) 1x 1x 0% (2, 1) 1x 2x ~25% (1, 2) 2x 1x ~25% (2, 2) 2x 2x ~50% (4, 1) 1x 4x ~37.5% (2, 4) 4x 2x ~62.5% 7.3 限制条件 Cluster 最大支持 16 个 CTA（由 16-bit mask 限制） Multicast 只在同一 Cluster 内有效 所有目标 CTA 的 SMEM 布局必须相同 8. TMA Multicast 与 mbarrier 特殊处理TMA Multicast 场景下，mbarrier 的初始化和同步需要特殊处理，以确保跨 CTA 的正确协调。 8.1 EmptyBarrier Arrival Count 的计算在 Cluster 模式下，EmptyBarrier 的 arrival count 需要考虑数据共享关系： 123456789101112// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:311-321uint32_t const num_consumer_warpgroups_per_cluster = cute::ceil_div(params.num_consumers, static_cast&lt;uint32_t&gt;(NumThreadsPerWarpGroup));uint32_t multicast_consumer_arrival_count = params.num_consumers; // Cluster size = 1if (cute::size(cluster_shape) &gt; 1) { // 关键公式：需要信号的 CTA 数量 = (cluster_x + cluster_y - 1) multicast_consumer_arrival_count = (cute::size&lt;0&gt;(cluster_shape) + cute::size&lt;1&gt;(cluster_shape) - 1) * num_consumer_warpgroups_per_cluster;} 为什么是 (cluster_x + cluster_y - 1)？ 以 2×2 Cluster 为例，考虑 CTA(0,0) 的 EmptyBarrier： CTA(0,0) 自己使用该 buffer CTA(0,1) 共享 A 数据（同行） CTA(1,0) 共享 B 数据（同列） CTA(1,1) 不与 CTA(0,0) 共享任何数据 因此 arrival count = (2 + 2 - 1) = 3 个 CTA 需要对该 barrier arrive。 123456789101112Cluster 2x2 中 CTA(0,0) 的数据共享关系: Col 0 Col 1 +-------+-------+Row 0 | (0,0) | (0,1) | ← 共享 A (同行) +-------+-------+Row 1 | (1,0) | (1,1) | +-------+-------+ ↑ 共享 B (同列)需要 arrive 的 CTA: (0,0), (0,1), (1,0) → 共 3 个 8.2 is_same_row_or_col：确定信号目标每个 Consumer 线程需要对远程 CTA 的 barrier 进行 arrive，但只针对共享数据的 CTA： 1234567// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:392-397template &lt;class ClusterShape&gt;CUTLASS_DEVICEbool is_same_row_or_col(int dst_block_id, dim3 block_id, ClusterShape cluster_shape) { return (((dst_block_id % cute::size&lt;0&gt;(cluster_shape)) == block_id.x) || // 同列 ((dst_block_id / cute::size&lt;0&gt;(cluster_shape)) == block_id.y)); // 同行} 这确保了： 同一行的 CTA 互相 arrive（因为共享 A 数据） 同一列的 CTA 互相 arrive（因为共享 B 数据） 8.3 Consumer Release：跨 CTA 的 barrier arriveconsumer_release 使用 DSMEM 机制对远程 CTA 的 barrier 进行 arrive。 详细实现：参见 Pipeline 与 mbarrier 深度解析 - DSMEM 跨 CTA 同步 核心要点： 使用 mapa 指令映射远程 SMEM 地址 mbarrier.arrive.shared::cluster 执行跨 SM arrive is_same_row_or_col 函数确定需要 arrive 的目标 CTA Consumer Release 初始化： 12345678910// 构造函数中设置auto [is_signaling_thread, dst_blockid] = spread_arrivals_to_warpgroup(thread_idx % NumThreadsPerWarpGroup, warp_idx);is_signaling_thread_ &amp;= is_same_row_or_col(dst_blockid_, block_id, cluster_shape);// 运行时 arrivevoid consumer_release(stage) { empty_barrier.arrive(dst_blockid_, is_signaling_thread_);} spread_arrivals_to_warpgroup 实现： 12345678910111213141516cute::tuple&lt;bool, uint32_t&gt; spread_arrivals_to_warpgroup( int thread_idx_in_warpgroup, int warp_idx){ // 每 8 个线程选一个 signaling thread // 128 threads / 16 CTA = 8 bool is_signaling_thread = (thread_idx_in_warpgroup % 8) == 0; // 用 Swizzle Layout 分配 dst_blockid (0-15) auto layout = cute::composition(Swizzle&lt;2,0,-2&gt;{}, Layout&lt;Shape&lt;_4,_4&gt;,Stride&lt;_4,_1&gt;&gt;{}); uint32_t thread_row = warp_idx % 4; uint32_t thread_col = (thread_idx_in_warpgroup / 8) % 4; uint32_t dst_blockid = layout(thread_row, thread_col); return cute::make_tuple(is_signaling_thread, dst_blockid);} 2×2 Cluster Arrive 流程示例： 123456789101112131415161718CTA(0,0) Consumer Warpgroup executes consumer_release(): 16 signaling threads in Warpgroup | +---------------+---------------+ | | | v v v CTA(0,0) CTA(0,1) CTA(1,0) CTA(1,1) barrier barrier barrier barrier ^ ^ ^ X | | | (not same row/col) arrive arrive arrive is_same_row_or_col: Y (self) Y (same row) Y (same col) NEach warpgroup sends 1 arrive to 3 valid CTAs2 warpgroups -&gt; each valid CTA receives 2 arrives3 valid CTAs x 2 warpgroups = 6 total arrives 远程 Barrier Arrive PTX： 12345678910111213static void arrive(ValueType const* smem_ptr, uint32_t cta_id, uint32_t pred) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); if (pred) { asm volatile( &quot;{\\n\\t&quot; &quot;.reg .b32 remAddr32;\\n\\t&quot; &quot;mapa.shared::cluster.u32 remAddr32, %0, %1;\\n\\t&quot; // 映射远程地址 &quot;mbarrier.arrive.shared::cluster.b64 _, [remAddr32];\\n\\t&quot; &quot;}&quot; : : &quot;r&quot;(smem_addr), &quot;r&quot;(cta_id)); }} 关键指令： mapa.shared::cluster: 本地 SMEM 地址 → 远程 CTA SMEM 地址 mbarrier.arrive.shared::cluster: 对远程 barrier 执行 arrive 8.4 TMA Multicast 与 FullBarrierTMA Multicast 指令自动处理 FullBarrier。Producer 只需发起一次 TMA 调用，硬件会： 广播数据到 multicast_mask 指定的所有 CTA 对每个目标 CTA 的 mbarrier 调用 complete_tx expect_tx/complete_tx 详解：参见 TMA Descriptor 深度解析 - Transaction Barrier 8.5 完整的 Pipeline 同步流程1234567891011121314151617181920212223242526272829sequenceDiagram participant P0 as Producer (CTA 0) participant TMA as TMA Unit participant C0 as Consumer (CTA 0) participant C1 as Consumer (CTA 1) Note over P0,C1: Cluster 2x1, 加载 B (mcast_mask=0b11) P0-&gt;&gt;P0: producer_acquire(stage) - 等待 EmptyBarrier P0-&gt;&gt;TMA: TMA Multicast Load (mask=0b11) TMA-&gt;&gt;C0: 写入 SMEM[stage] + complete_tx(mbar) TMA-&gt;&gt;C1: 写入 SMEM[stage] + complete_tx(mbar) par Consumer Wait C0-&gt;&gt;C0: consumer_wait(stage) - FullBarrier C1-&gt;&gt;C1: consumer_wait(stage) - FullBarrier end Note over C0,C1: Consumers 执行计算... par Consumer Release C0-&gt;&gt;C0: arrive(local EmptyBarrier) C0-&gt;&gt;C1: arrive(remote EmptyBarrier) via DSMEM C1-&gt;&gt;C1: arrive(local EmptyBarrier) C1-&gt;&gt;C0: arrive(remote EmptyBarrier) via DSMEM end P0-&gt;&gt;P0: EmptyBarrier wait 完成 (arrival_count=2) 8.6 TMA Multicast vs 普通 TMA 对比 维度 普通 TMA TMA Multicast 数据目标 单 CTA Cluster 内多 CTA 内存带宽 N × 数据量 1 × 数据量 Multicast Mask 不需要 16-bit EmptyBarrier arrival num_consumers (cluster_x + cluster_y - 1) × warpgroups Consumer release 本地 arrive 本地 + 远程 arrive TMA 调用 tma.with(*mbar) tma.with(*mbar, mask) A/B 共享 Barrier 详解：参见 TMA Descriptor - A 和 B 共享同一个 Barrier 9. 关键要点 Multicast Mask 是 16-bit：每位对应 cluster 内一个 CTA Layout 决定编号：Layout&lt;ClusterShape&gt; 将 (m,n) 映射到线性 block_id A/B 不同方向：A 沿 N 方向广播，B 沿 M 方向广播 由 Producer 发起：只需一个 CTA 发起 TMA，硬件负责广播 EmptyBarrier arrival count：(cluster_x + cluster_y - 1) × warpgroups Consumer Release 跨 CTA：通过 DSMEM 实现 10. 相关文档 Pipeline 与 mbarrier 深度解析 - DSMEM、mbarrier PTX TMA Descriptor 深度解析 - expect_tx/complete_tx、A/B 共享 Barrier 参考资料 CUTLASS GitHub 仓库 sm90_mma_tma_gmma_ss_warpspecialized.hpp copy_traits_sm90_tma.hpp copy_sm90_tma.hpp sm90_pipeline.hpp - Pipeline 和 mbarrier 处理 barrier.h - ClusterBarrier 实现 NVIDIA PTX ISA - TMA Multicast NVIDIA PTX ISA - mbarrier","link":"/2024/12/24/0x0D-tma-multicast/"},{"title":"0x0E CuTe Layout 分割操作：logical_divide 与 zipped_divide 详解","text":"本文详细解析 CuTe 的 Layout 分割操作：logical_divide 和 zipped_divide。理解这两个操作对于掌握 CUTLASS 中的 tiled tensor 操作和 Thread-Value Layout 至关重要。 示例代码: 0x0E_logical_divide.cu 0x0E_zipped_divide.cu Key Points logical_divide: Hierarchically divides layout, keeping original offset mapping zipped_divide: Divides and zips tile dimensions together for easier iteration Shape transformation: Both change shape but differ in stride organization Use case: zipped_divide is commonly used for thread-value decomposition in MMA 1. Original LayoutLet’s start with a simple 8x8 layout: 12auto layout_2d = make_layout(make_shape(Int&lt;8&gt;{}, Int&lt;8&gt;{}));// Layout: (_8,_8):(_1,_8) Visualization (click to view PDF): step0_original_8x8.pdf 12345678910111213141516171819Original 2D layout: (_8,_8):(_1,_8) 0 1 2 3 4 5 6 7 +----+----+----+----+----+----+----+----+ 0 | 0 | 8 | 16 | 24 | 32 | 40 | 48 | 56 | +----+----+----+----+----+----+----+----+ 1 | 1 | 9 | 17 | 25 | 33 | 41 | 49 | 57 | +----+----+----+----+----+----+----+----+ 2 | 2 | 10 | 18 | 26 | 34 | 42 | 50 | 58 | +----+----+----+----+----+----+----+----+ 3 | 3 | 11 | 19 | 27 | 35 | 43 | 51 | 59 | +----+----+----+----+----+----+----+----+ 4 | 4 | 12 | 20 | 28 | 36 | 44 | 52 | 60 | +----+----+----+----+----+----+----+----+ 5 | 5 | 13 | 21 | 29 | 37 | 45 | 53 | 61 | +----+----+----+----+----+----+----+----+ 6 | 6 | 14 | 22 | 30 | 38 | 46 | 54 | 62 | +----+----+----+----+----+----+----+----+ 7 | 7 | 15 | 23 | 31 | 39 | 47 | 55 | 63 | +----+----+----+----+----+----+----+----+ The offset formula is: offset = m * 1 + n * 8 2. Tile DefinitionWe define a tile to divide the layout: 123auto tile_2d = make_tile(make_layout(Int&lt;4&gt;{}), // M: groups of 4 make_layout(Int&lt;2&gt;{})); // N: groups of 2// Tile: (_4:_1, _2:_1) This means: Divide M dimension into groups of 4 rows Divide N dimension into groups of 2 columns Result: 2x4 = 8 tiles covering the 8x8 matrix 3. logical_divide12auto divided_2d = logical_divide(layout_2d, tile_2d);// Result: ((_4,_2),(_2,_4)):((_1,_4),(_8,_16)) Visualization: step1_logical_divide.pdf Shape Analysis Dimension Meaning (_4,_2) First dim: 4 rows per tile, 2 tile rows (_2,_4) Second dim: 2 cols per tile, 4 tile cols Stride Analysis Stride Meaning _1 Adjacent rows within tile (original M stride) _4 Jump between tile rows (4 rows apart) _8 Adjacent columns within tile (original N stride) _16 Jump between tile columns (2 cols * 8 stride) Key Property: Original Offset Preserved12345678After logical_divide: ((_4,_2),(_2,_4)):((_1,_4),(_8,_16)) 0 1 2 3 4 5 6 7 +----+----+----+----+----+----+----+----+ 0 | 0 | 8 | 16 | 24 | 32 | 40 | 48 | 56 | +----+----+----+----+----+----+----+----+ 1 | 1 | 9 | 17 | 25 | 33 | 41 | 49 | 57 | +----+----+----+----+----+----+----+----+ ... The offset mapping is identical to the original layout! logical_divide only restructures the logical view, not the physical layout. 4. zipped_divide12auto zipped = zipped_divide(layout_2d, tile_2d);// Result: ((_4,_2),(_2,_4)):((_1,_8),(_4,_16)) Visualization: step2_zipped_divide.pdf Different Stride OrganizationCompare the strides: Operation Shape Stride logical_divide ((_4,_2),(_2,_4)) ((_1,_4),(_8,_16)) zipped_divide ((_4,_2),(_2,_4)) ((_1,_8),(_4,_16)) Notice: zipped_divide zips the inner tile dimensions together! Offset Mapping Changes12345678910111213141516171819After zipped_divide: ((_4,_2),(_2,_4)):((_1,_8),(_4,_16)) 0 1 2 3 4 5 6 7 +----+----+----+----+----+----+----+----+ 0 | 0 | 4 | 16 | 20 | 32 | 36 | 48 | 52 | +----+----+----+----+----+----+----+----+ 1 | 1 | 5 | 17 | 21 | 33 | 37 | 49 | 53 | +----+----+----+----+----+----+----+----+ 2 | 2 | 6 | 18 | 22 | 34 | 38 | 50 | 54 | +----+----+----+----+----+----+----+----+ 3 | 3 | 7 | 19 | 23 | 35 | 39 | 51 | 55 | +----+----+----+----+----+----+----+----+ 4 | 8 | 12 | 24 | 28 | 40 | 44 | 56 | 60 | +----+----+----+----+----+----+----+----+ 5 | 9 | 13 | 25 | 29 | 41 | 45 | 57 | 61 | +----+----+----+----+----+----+----+----+ 6 | 10 | 14 | 26 | 30 | 42 | 46 | 58 | 62 | +----+----+----+----+----+----+----+----+ 7 | 11 | 15 | 27 | 31 | 43 | 47 | 59 | 63 | +----+----+----+----+----+----+----+----+ The offsets are now reordered - elements within each tile are now contiguous! 5. Understanding the DifferenceVisual Comparisonlogical_divide - Hierarchical grouping: 12345First 4x2 tile: Second 4x2 tile (offset by 16):[0, 8] [16, 24][1, 9] [17, 25][2, 10] [18, 26][3, 11] [19, 27] zipped_divide - Contiguous tiles: 12345First 4x2 tile: Second 4x2 tile (offset by 16):[0, 4] [16, 20][1, 5] [17, 21][2, 6] [18, 22][3, 7] [19, 23] Mathematical InterpretationFor coordinate (m, n) with tile size (TileM, TileN): logical_divide: Inner coord: (m % TileM, n % TileN) Outer coord (tile index): (m / TileM, n / TileN) Offset uses original strides zipped_divide: Same hierarchical decomposition But strides are adjusted so each tile’s elements are contiguous Tile index effectively becomes the major index 6. When to Use Which Use Case Recommended Preserving memory access pattern logical_divide Thread-Value decomposition zipped_divide Tiled iteration with contiguous tiles zipped_divide Hierarchical view without data movement logical_divide In CUTLASS, zipped_divide is heavily used for: Converting tensor layouts to Thread-Value (TV) format Creating fragment views for MMA operations Iterating over tiles in a regular pattern 7. Example Code1234567891011121314151617181920212223#include &lt;cute/tensor.hpp&gt;#include &lt;cute/layout.hpp&gt;using namespace cute;int main() { // Original 8x8 layout auto layout_2d = make_layout(make_shape(Int&lt;8&gt;{}, Int&lt;8&gt;{})); // Tile: 4x2 auto tile_2d = make_tile(make_layout(Int&lt;4&gt;{}), make_layout(Int&lt;2&gt;{})); // logical_divide - preserves offsets auto divided = logical_divide(layout_2d, tile_2d); print(&quot;logical_divide: &quot;); print(divided); print(&quot;\\n&quot;); // zipped_divide - contiguous tiles auto zipped = zipped_divide(layout_2d, tile_2d); print(&quot;zipped_divide: &quot;); print(zipped); print(&quot;\\n&quot;); return 0;} 8. Summary Property logical_divide zipped_divide Shape Same Same Stride Hierarchical (original) Zipped (reordered) Offset mapping Preserved Changed Use case View transformation Tile iteration Both operations are fundamental to CuTe’s layout algebra and enable efficient tiled tensor operations. Understanding their differences is key to mastering CUTLASS’s tensor abstractions. References CUTLASS CuTe Documentation Source: cute-examples/logical_divide.cu","link":"/2024/12/24/0x0E-cute-layout-divide/"},{"title":"0x11 CuTe TiledMMA 完全指南：从 Layout 到线程映射","text":"本文是 CuTe TiledMMA 的综合指南，将核心概念串联起来，深入剖析从 Layout 基础到线程映射的完整流程。 相关文章: 0x01 TV Layout Guide - TV Layout 基础 0x05 make_tiled_mma - TiledMMA 构建 0x0E Layout 分割操作 - logical_divide 与 zipped_divide 0x0F thrfrg_C 详解 - C Tensor 线程分片 1. 核心概念速查表 概念 本质 Layout 坐标到 offset 的映射函数 Tiler 坐标空间的分解规则 logical_divide 按 (inner, outer) 重组坐标层级 zipped_divide 按 (M方向, N方向) 重组坐标层级 compose 两个映射的复合：coord → offset₁ → offset₂ Shape 坐标的取值范围 Stride 每个坐标维度对 offset 的贡献系数 概念关系图1234567891011 Tiler (分解规则) │ ▼原始坐标 ──────────────────────────────► 分层坐标 (m, n) ((m_i,m_o), (n_i,n_o)) [zipped] ((m_i,n_i), (m_o,n_o)) [logical] │ │ │ Layout │ Layout' ▼ ▼ offset ◄──────────────────────────────── offset (相同) 2. Layout 基础Layout 本质是一个坐标到整数偏移量的映射函数： $$L: \\text{coord} \\rightarrow \\text{offset}$$ CuTe 表示法1Layout = Shape : Stride 123auto layout = make_layout(make_shape(8, 4), make_stride(1, 8));// (8, 4) : (1, 8)// L(m, n) = m × 1 + n × 8 详细的 TV Layout 解析请参考 0x01 TV Layout Guide 3. Divide 操作关于 logical_divide 和 zipped_divide 的详细解析，请参考 0x0E Layout 分割操作。 这里给出核心对比： Operation Shape Stride 特点 logical_divide ((_4,_2),(_2,_4)) ((_1,_4),(_8,_16)) 保持原始 offset zipped_divide ((_4,_2),(_2,_4)) ((_1,_8),(_4,_16)) Tile 内连续 关键理解：divide 操作后仍然是 Layout！ 只是 Shape 变成了嵌套结构。 4. TiledMMA 结构核心组成123456789101112template &lt;class MMA_Atom, class AtomLayoutMNK, class PermutationMNK&gt;struct TiledMMA : MMA_Atom{ // 从 MMA_Atom 继承 using AtomShape_MNK = typename MMA_Atom::Shape_MNK; // 如 (64, 16, 16) using AtomThrID = typename MMA_Atom::ThrID; // 如 Layout&lt;_128&gt; using AtomLayoutC_TV = typename MMA_Atom::LayoutC_TV; // Atom 内的 TV 映射 // TiledMMA 自己的成员 using ThrLayoutVMNK = decltype(tiled_product(AtomThrID{}, AtomLayoutMNK{})); ThrLayoutVMNK thr_layout_vmnk_; // 核心：线程的 (V,M,N,K) 布局}; 层级关系1234567891011TiledMMA├── MMA_Atom (单个 MMA 指令)│ ├── AtomShape_MNK: (64, 16, 16) -- 单个 Atom 覆盖的区域│ ├── AtomThrID: 128 -- Atom 内的线程数│ └── AtomLayoutC_TV: CLayout_64x16 -- Atom 内 (thr,val)→offset│├── AtomLayoutMNK: (2, 1, 1) -- Atom 的 tiling 方式│ └── M方向2个Atom, N方向1个, K方向1个│└── thr_layout_vmnk_: (128, 2, 1, 1) -- 全局线程布局 └── 128×2×1×1 = 256 总线程 关于 TiledMMA 的构建，请参考 0x05 make_tiled_mma 5. thrfrg_C 函数thrfrg_C 是 TiledMMA 的核心函数，将 C 矩阵 Layout 转换为线程-值视图。 详细的 thrfrg_C 解析请参考 0x0F thrfrg_C 详解 流程概览12345678910111213141516ctensor: (128, 128) │ ▼ zipped_divide by (64, 16)c_tensor: ((64, 16), (2, 8)) ((AtomM,AtomN), (RestM,RestN)) │ ▼ compose with AtomLayoutC_TVtv_tensor: ((128, 8), (2, 8)) ((ThrV,FrgV), (RestM,RestN)) │ ▼ zipped_divide by (_, (2, 1))thr_tensor: ((128, (2, 1)), (8, (1, 8))) ((ThrV,(ThrM,ThrN)), (FrgV,(RestM',RestN'))) 6. get_slice 与线程映射函数实现123456789template &lt;class ThrIdx&gt;auto get_slice(ThrIdx const&amp; thr_idx) const{ // Step 1: 线性 thread_id → 多维坐标 (v, m, n, k) auto thr_vmnk = thr_layout_vmnk_.get_flat_coord(thr_idx); // Step 2: 返回该线程的 MMA slice return ThrMMA&lt;TiledMMA, decltype(thr_vmnk)&gt;{*this, thr_vmnk};} get_flat_coord 详解本质：Layout 的逆映射，从 offset 反推坐标 12正向: coord → offset (Layout 正常用法)逆向: offset → coord (get_flat_coord) 示例12345678// thr_layout_vmnk_ = (128, 2, 1, 1) : (1, 128, 256, 256)thr_idx = 0 → thr_vmnk = (0, 0, 0, 0)thr_idx = 1 → thr_vmnk = (1, 0, 0, 0)thr_idx = 127 → thr_vmnk = (127, 0, 0, 0)thr_idx = 128 → thr_vmnk = (0, 1, 0, 0) // 第二个 Atomthr_idx = 129 → thr_vmnk = (1, 1, 0, 0)thr_idx = 255 → thr_vmnk = (127, 1, 0, 0) 计算公式123456789// Layout: (128, 2, 1, 1) : (1, 128, 256, 256)// thr_idx = 130v = (130 / 1) % 128 = 2m = (130 / 128) % 2 = 1n = (130 / 256) % 1 = 0k = (130 / 256) % 1 = 0→ thr_vmnk = (2, 1, 0, 0) 线程映射图示12345678910thr_idx (线性) thr_vmnk (多维坐标) │ │ │ get_flat_coord │ ▼ ▼ 130 ──────────────────────► (v=2, m=1, n=0, k=0) │ │ 含义 ▼ Atom 内第 2 号线程 第 1 个 Atom (M方向) ThrMMA 的作用12345678910111213141516template &lt;class TiledMMA, class ThrVMNK&gt;struct ThrMMA { TiledMMA mma_; ThrVMNK thr_vmnk_; // (v, m, n, k) 坐标 // 获取该线程负责的 C 数据 template &lt;class CTensor&gt; auto partition_C(CTensor&amp;&amp; ctensor) const { auto thr_tensor = mma_.thrfrg_C(ctensor); // thr_tensor: ((ThrV, (ThrM, ThrN)), (FrgV, (RestM, RestN))) // 用 thr_vmnk_ 选择该线程的 slice return thr_tensor(thr_vmnk_, _); // 结果: (FrgV, (RestM, RestN)) - 只属于这个线程的数据 }}; 7. get_layoutC_TV 解析函数目的返回一个 Layout，映射 (thr_idx, val_idx) → C 矩阵中的 offset 实现代码12345678910111213141516auto get_layoutC_TV() const{ // Step 1: 构造参考 C 矩阵 Layout auto ref_C = make_layout(make_shape(tile_size_mnk&lt;0&gt;(), tile_size_mnk&lt;1&gt;())); // 例如: (128, 128) : (1, 128) // Step 2: 构造 thr_idx → 层级坐标 的转换 auto thridx_2_thrid = composition( make_layout(make_shape (size(thr_layout_vmnk_), Int&lt;1&gt;{}), make_stride(Int&lt;1&gt;{}, Int&lt;0&gt;{})), right_inverse(make_layout(thr_layout_vmnk_, complement(thr_layout_vmnk_))) ); // Step 3: compose 得到最终 Layout return thrfrg_C(ref_C).compose(thridx_2_thrid, _);} 流程图1234567891011121314151617输入: (thr_idx, val_idx) │ ▼ ┌─────────────────┐ │ thridx_2_thrid │ thr_idx → (ThrV, (ThrM, ThrN)) └─────────────────┘ │ ▼((ThrV, (ThrM, ThrN)), val_idx) │ ▼ ┌─────────────────┐ │ thrfrg_C(ref_C)│ 线程坐标 + 值坐标 → offset └─────────────────┘ │ ▼输出: offset (在 M×N 矩阵中的位置) AtomLayoutC_TV vs get_layoutC_TV() 属性 AtomLayoutC_TV get_layoutC_TV() 级别 Atom TiledMMA (整个 Tile) 线程范围 Atom 内 (128) 全部线程 (256) 值范围 单 Atom 内 (8) 跨所有 Atom (64) 输出 offset Atom 内 offset Tile 内 offset 覆盖区域 64×16 128×128 使用示例1234567auto layoutC_TV = tiled_mma.get_layoutC_TV();// Thread 130 的 Value 3 在 C 矩阵中的位置int offset = layoutC_TV(130, 3);int m = offset % M;int n = offset / M;// 现在知道 Thread 130 的第 3 个值对应 C[m, n] 8. 完整示例代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;cute/tensor.hpp&gt;#include &lt;cute/layout.hpp&gt;using namespace cute;// CLayout_64x16 定义template &lt;int N&gt;using CLayout_64xN = Layout&lt;Shape &lt;Shape &lt; _4, _8, _4&gt;, Shape &lt; _2, _2, Int&lt;N/8&gt;&gt;&gt;, Stride&lt;Stride&lt;_128, _1, _16&gt;, Stride&lt;_64, _8, _512&gt;&gt;&gt;;using CLayout_64x16 = CLayout_64xN&lt;16&gt;;int main() { // 配置 using AtomShapeM = Int&lt;64&gt;; using AtomShapeN = Int&lt;16&gt;; using AtomLayoutMNK = Layout&lt;Shape&lt;_2, _1, _1&gt;&gt;; // M方向2个Atom using AtomLayoutC_TV = CLayout_64x16; // 原始 C 矩阵 auto ctensor = make_layout(make_shape(Int&lt;128&gt;{}, Int&lt;128&gt;{})); // Step 2: zipped_divide by AtomShape auto c_tile = make_tile(make_layout(AtomShapeM{}), make_layout(AtomShapeN{})); auto c_tensor = zipped_divide(ctensor, c_tile); // ((64, 16), (2, 8)) : ((1, 128), (64, 2048)) // Step 3: compose with AtomLayoutC_TV auto tv_tensor = c_tensor.compose(AtomLayoutC_TV{}, _); // ((128, 8), (2, 8)) : (...) // Step 4: zipped_divide for Thread Tiling auto AtomThrID = Layout&lt;Int&lt;128&gt;&gt;{}; auto thr_layout_vmnk = tiled_product(AtomThrID, AtomLayoutMNK{}); // (128, 2, 1, 1) : (1, 128, 256, 256) auto thr_tile = make_tile(_, make_tile(make_layout(size&lt;1&gt;(thr_layout_vmnk)), make_layout(size&lt;2&gt;(thr_layout_vmnk)))); auto thr_tensor = zipped_divide(tv_tensor, thr_tile); // ((128, (2, 1)), (8, (1, 8))) // 验证 int ThrV = size&lt;0,0&gt;(thr_tensor); // 128 int ThrM = size&lt;0,1,0&gt;(thr_tensor); // 2 int ThrN = size&lt;0,1,1&gt;(thr_tensor); // 1 int FrgV = size&lt;1,0&gt;(thr_tensor); // 8 int RestM = size&lt;1,1,0&gt;(thr_tensor);// 1 int RestN = size&lt;1,1,1&gt;(thr_tensor);// 8 // 总线程: 128 × 2 × 1 = 256 // 每线程值: 8 × 1 × 8 = 64 // 总元素: 256 × 64 = 16384 = 128 × 128 ✓ return 0;} 9. 总结核心理解 Layout 是坐标到 offset 的映射，divide 操作只是重组坐标层级，最终 offset 不变 TiledMMA 通过 thrfrg_C/A/B 将矩阵 Layout 转换为 ((线程坐标), (值坐标)) 的形式 thrfrg_C 的四个步骤： Permutation（可选重排） 按 Atom 大小切分 (M,N) → (Thread, Value) 变换 分配 Atom 给不同 Warpgroup get_slice 通过逆映射把线性 thread_id 转为多维坐标，返回线程视图 AtomLayoutC_TV 是 Atom 级别的映射，get_layoutC_TV() 是整个 Tile 级别的映射 一图总结12345678910C 矩阵 (M×N) │ │ thrfrg_C ▼((ThrV, (ThrM, ThrN)), (FrgV, (RestM, RestN))) └────── 线程坐标 ──────┘ └──── 值坐标 ────┘ │ │ │ get_slice(thr_idx) │ ▼ ▼ 特定线程的坐标 该线程负责的所有值 References 0x01 TV Layout Guide 0x05 make_tiled_mma 0x0E Layout 分割操作 0x0F thrfrg_C 详解 CUTLASS CuTe Documentation","link":"/2024/12/24/0x11-tiledmma-complete-guide/"},{"title":"0x10 CUTLASS SM90 WarpGroup MMA API 详解","text":"本文详解 CUTLASS SM90 WarpGroup MMA 的核心 API，基于 sm90_mma_tma_gmma_ss_warpspecialized.hpp 源码分析 WGMMA 异步执行模型和同步原语的使用方法。 示例代码: 0x10_warpgroup_mma_api.cu 核心 API 速览 warpgroup_arrive(): 发起 WGMMA fence，标记操作开始 warpgroup_commit_batch(): 提交一批 WGMMA 操作 warpgroup_wait&lt;N&gt;(): 等待直到最多 N 批操作在飞 warpgroup_fence_operand(): 防止累加器被优化重排 1. WGMMA 异步执行模型1.1 WarpGroup MMA 概述Hopper (SM90) 引入了 WarpGroup MMA (WGMMA)，允许 4 个 warp（128 线程）协作执行大规模矩阵乘法。关键特性： 异步执行: MMA 操作在后台异步执行 批处理: 多个 MMA 操作可组成一批 流水线: 支持多批操作同时在飞 12345678WarpGroup (128 threads = 4 warps)+-- Warp 0 (32 threads)+-- Warp 1 (32 threads)+-- Warp 2 (32 threads)+-- Warp 3 (32 threads) | v WGMMA Unit (异步执行 MMA) 1.2 异步执行流程1234567891011// 1. Fence - 标记 MMA 操作边界warpgroup_arrive();// 2. 发射 MMA 指令cute::gemm(tiled_mma, tCrA, tCrB, accum);// 3. Commit - 提交一批操作warpgroup_commit_batch();// 4. Wait - 等待完成warpgroup_wait&lt;0&gt;(); // 等待所有操作完成 2. 核心同步原语2.1 warpgroup_arrive()123456// 源码: cute/arch/mma_sm90_gmma.hpp:47-57CUTE_HOST_DEVICE void warpgroup_arrive() {#if defined(CUTE_ARCH_MMA_SM90A_ENABLED) asm volatile (&quot;wgmma.fence.sync.aligned;\\n&quot; ::: &quot;memory&quot;);#endif} 作用: 发射 wgmma.fence.sync.aligned PTX 指令 标记 WGMMA 操作的开始边界 确保之前的内存操作完成 使用时机: 在发射 gemm() 之前调用 2.2 warpgroup_commit_batch()123456// 源码: cute/arch/mma_sm90_gmma.hpp:74-84CUTE_HOST_DEVICE void warpgroup_commit_batch() {#if defined(CUTE_ARCH_MMA_SM90A_ENABLED) asm volatile(&quot;wgmma.commit_group.sync.aligned;\\n&quot; ::: &quot;memory&quot;);#endif} 作用: 发射 wgmma.commit_group.sync.aligned PTX 指令 将当前未提交的 WGMMA 操作打包成一批 批次编号自动递增 使用时机: 在 gemm() 之后、下一轮 warpgroup_arrive() 之前调用 2.3 warpgroup_wait&lt;N&gt;()12345678// 源码: cute/arch/mma_sm90_gmma.hpp:59-71template &lt;int N&gt;CUTE_HOST_DEVICE void warpgroup_wait() { static_assert(N &gt;= 0 &amp;&amp; N &lt;= 7, &quot;WGMMA wait: N must be in range [0, 7]&quot;);#if defined(CUTE_ARCH_MMA_SM90A_ENABLED) asm volatile(&quot;wgmma.wait_group.sync.aligned %0;\\n&quot; :: &quot;n&quot;(N) : &quot;memory&quot;);#endif} 作用: 发射 wgmma.wait_group.sync.aligned N PTX 指令 阻塞直到最多 N 批操作还在执行 N=0 表示等待所有操作完成 参数范围: N ∈ [0, 7] N 值 含义 0 等待所有批次完成 1 允许最多 1 批在飞 2 允许最多 2 批在飞 … … 7 允许最多 7 批在飞 2.4 warpgroup_fence_operand()123456789101112// 源码: cute/arch/mma_sm90_gmma.hpp:86-103CUTE_HOST_DEVICE void warpgroup_fence_operand(float&amp; reg) {#if defined(__CUDA_ARCH__) asm volatile(&quot;&quot; : &quot;+f&quot;(reg) :: &quot;memory&quot;);#endif}CUTE_HOST_DEVICE void warpgroup_fence_operand(uint32_t&amp; reg) {#if defined(__CUDA_ARCH__) asm volatile(&quot;&quot; : &quot;+r&quot;(reg) :: &quot;memory&quot;);#endif} 作用: 编译器屏障，防止累加器寄存器被优化重排 使用 &quot;+f&quot; (float) 或 &quot;+r&quot; (uint32) 约束 确保 WGMMA 操作看到正确的累加器值 使用时机: 在 warpgroup_arrive() 之前和 warpgroup_wait() 之后 3. WGMMA 批处理模型3.1 批处理流程图1234567891011时间 -&gt;Thread: fence -&gt; gemm -&gt; gemm -&gt; commit -&gt; fence -&gt; gemm -&gt; commit -&gt; wait&lt;1&gt; -&gt; wait&lt;0&gt; |______Batch 0______| |__Batch 1__| ^ |WGMMA: [===Batch 0===] [===Batch 1===] 执行中 执行中 ^ ^ | | commit后开始执行 wait&lt;0&gt;时全部完成 3.2 K_PIPE_MMAS 控制12345// 源码: sm90_mma_tma_gmma_ss_warpspecialized.hpp:264static constexpr int K_PIPE_MMAS = 1;// 使用: 在主循环中控制在飞批次数warpgroup_wait&lt;K_PIPE_MMAS&gt;(); // 允许 1 批在飞 K_PIPE_MMAS 控制了 MMA 操作的流水线深度： 较大值: 更多并行，更高吞吐 较小值: 更低延迟，更少资源占用 4. CollectiveMma::mma() 完整流程4.1 代码结构123456789101112131415161718192021222324252627282930// 源码: sm90_mma_tma_gmma_ss_warpspecialized.hpp:416-559template &lt;class FrgTensorC&gt;CUTLASS_DEVICE void mma( MainloopPipeline pipeline, PipelineState smem_pipe_read, FrgTensorC&amp; accum, int k_tile_count, int thread_idx, TensorStorage&amp; shared_tensors, Params const&amp; mainloop_params){ // Step 1: 创建 SMEM tensors Tensor sA = make_tensor(make_smem_ptr(...), SmemLayoutA{}); Tensor sB = make_tensor(make_smem_ptr(...), SmemLayoutB{}); // Step 2: 获取当前 warp group 的 MMA slice TiledMma tiled_mma; auto thread_mma = tiled_mma.get_slice(warp_group_idx); // Step 3: Partition A/B for this thread Tensor tCsA = thread_mma.partition_A(sA); Tensor tCsB = thread_mma.partition_B(sB); // Step 4: 创建 fragment/descriptor Tensor tCrA = thread_mma.make_fragment_A(tCsA); Tensor tCrB = thread_mma.make_fragment_B(tCsB); // Step 5: Prologue + Mainloop // ...} 4.2 Prologue (第一个 K-tile)12345678910111213141516171819202122232425// 源码: sm90_mma_tma_gmma_ss_warpspecialized.hpp:479-503tiled_mma.accumulate_ = GMMA::ScaleOut::Zero; // 清零累加器warpgroup_fence_operand(accum);{ // 等待 SMEM 数据就绪 auto barrier_token = pipeline.consumer_try_wait(smem_pipe_read); pipeline.consumer_wait(smem_pipe_read, barrier_token); int read_stage = smem_pipe_read.index(); warpgroup_arrive(); // &lt;- Fence tiled_mma.accumulate_ = GMMA::ScaleOut::Zero; // K-内层循环 CUTLASS_PRAGMA_UNROLL for (int k_block = 0; k_block &lt; size&lt;2&gt;(tCrA); ++k_block) { cute::gemm(tiled_mma, tCrA(_,_,k_block,read_stage), tCrB(_,_,k_block,read_stage), accum); tiled_mma.accumulate_ = GMMA::ScaleOut::One; // 后续累加 } warpgroup_commit_batch(); // &lt;- Commit ++smem_pipe_read;} 关键点: ScaleOut::Zero: 第一次 MMA 清零 D ScaleOut::One: 后续 MMA 累加到 D 内层 K 循环遍历 SMEM 中的 K blocks 4.3 Mainloop (剩余 K-tiles)12345678910111213141516171819202122232425262728// 源码: sm90_mma_tma_gmma_ss_warpspecialized.hpp:528-556CUTLASS_PRAGMA_NO_UNROLLfor ( ; k_tile_count &gt; 0; --k_tile_count) { // 等待 SMEM 数据 auto barrier_token = pipeline.consumer_try_wait(smem_pipe_read); pipeline.consumer_wait(smem_pipe_read, barrier_token); int read_stage = smem_pipe_read.index(); warpgroup_fence_operand(accum); warpgroup_arrive(); // 执行 MMA cute::gemm(tiled_mma, tCrA(_,_,_,read_stage), tCrB(_,_,_,read_stage), accum); warpgroup_commit_batch(); // 等待前面的批次完成，释放 SMEM warpgroup_wait&lt;K_PIPE_MMAS&gt;(); warpgroup_fence_operand(accum); // 释放已消费的 SMEM stage pipeline.consumer_release(smem_pipe_release); ++smem_pipe_read; ++smem_pipe_release;} 4.4 流程图12345678910111213 +-- consumer_wait (等待 TMA 完成) --+ | | v |+-- fence_operand --+-- arrive --+-- gemm --+-- commit --+-- wait&lt;1&gt; --+-- fence_operand --+| | || &lt;- 批次 N-1 可能在执行 -&gt; &lt;- 批次 N 提交 -&gt; | || v |+-- consumer_release (释放 SMEM stage N-K_PIPE_MMAS) &lt;-----------------+ | | +-----------------------------------------------------------------------+ | v 下一轮迭代 5. GMMA ScaleOut 模式5.1 ScaleOut 枚举12345// 源码: cute/arch/mma_sm90_gmma.hpp:112-115enum class ScaleOut { Zero = 0, // D = A * B (清零累加) One = 1 // D = A * B + D (累加模式)}; 5.2 使用场景 模式 公式 使用场景 Zero D = A × B K 循环第一次迭代，初始化累加器 One D = A × B + D K 循环后续迭代，累加结果 1234567// Prologue: 第一个 MMAtiled_mma.accumulate_ = GMMA::ScaleOut::Zero;cute::gemm(tiled_mma, ...); // D = A * B// 后续: 累加tiled_mma.accumulate_ = GMMA::ScaleOut::One;cute::gemm(tiled_mma, ...); // D = A * B + D 6. SS vs RS 模式6.1 SS 模式 (Shared-Shared)1234567891011// A 和 B 都从 SMEM 读取，使用 descriptorstruct MMA_64x8x16_F16F16F16_SS { using ARegisters = uint64_t[1]; // SMEM descriptor using BRegisters = uint64_t[1]; // SMEM descriptor using CRegisters = uint32_t[2]; // Register static void fma(uint64_t const&amp; desc_a, uint64_t const&amp; desc_b, uint32_t&amp; d0, uint32_t&amp; d1, GMMA::ScaleOut scale_D);}; 6.2 RS 模式 (Register-Shared)123456789101112// A 从 Register 读取，B 从 SMEM 读取struct MMA_64x8x16_F16F16F16_RS { using ARegisters = uint32_t[4]; // 4 个 32-bit 寄存器 using BRegisters = uint64_t[1]; // SMEM descriptor using CRegisters = uint32_t[2]; // Register static void fma(uint32_t const&amp; a0, uint32_t const&amp; a1, uint32_t const&amp; a2, uint32_t const&amp; a3, uint64_t const&amp; desc_b, uint32_t&amp; d0, uint32_t&amp; d1, GMMA::ScaleOut scale_D);}; 6.3 模式选择 特性 SS 模式 RS 模式 A 来源 SMEM (descriptor) Register B 来源 SMEM (descriptor) SMEM (descriptor) 适用场景 TMA 直接加载到 SMEM 需要 A 变换/预处理 本文件 ✓ (主要使用) - 7. Pipeline 集成7.1 Pipeline 状态机12345// Consumer 视角的 Pipeline 操作pipeline.consumer_try_wait(smem_pipe_read); // 尝试获取 barrier tokenpipeline.consumer_wait(smem_pipe_read, token); // 等待数据就绪// ... 执行 MMA ...pipeline.consumer_release(smem_pipe_release); // 释放 stage 7.2 Pipeline 与 WGMMA 同步1234567891011121314151617181920Producer (TMA Load) Consumer (WGMMA) | | v v producer_acquire consumer_try_wait | | TMA copy to SMEM consumer_wait | | [TMA complete_tx] -----barrier----&gt; [ready] | | ++smem_pipe_write warpgroup_arrive | gemm + commit | warpgroup_wait | consumer_release | &lt;---barrier---- [释放] | ++smem_pipe_read 8. 完整代码示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 简化版 WGMMA 主循环template &lt;class FrgTensorC&gt;CUTLASS_DEVICE void mma_simplified( Pipeline pipeline, PipelineState smem_pipe_read, FrgTensorC&amp; accum, int k_tile_count, TensorStorage&amp; shared){ Tensor sA = make_tensor(..., SmemLayoutA{}); Tensor sB = make_tensor(..., SmemLayoutB{}); TiledMma tiled_mma; auto thread_mma = tiled_mma.get_slice(warp_group_idx); Tensor tCrA = thread_mma.make_fragment_A(thread_mma.partition_A(sA)); Tensor tCrB = thread_mma.make_fragment_B(thread_mma.partition_B(sB)); PipelineState smem_pipe_release = smem_pipe_read; // Prologue: 第一个 k-tile { pipeline.consumer_wait(smem_pipe_read, pipeline.consumer_try_wait(smem_pipe_read)); int stage = smem_pipe_read.index(); warpgroup_fence_operand(accum); warpgroup_arrive(); tiled_mma.accumulate_ = GMMA::ScaleOut::Zero; cute::gemm(tiled_mma, tCrA(_,_,_,stage), tCrB(_,_,_,stage), accum); warpgroup_commit_batch(); ++smem_pipe_read; } tiled_mma.accumulate_ = GMMA::ScaleOut::One; // Mainloop: 剩余 k-tiles for (int k = k_tile_count - 1; k &gt; 0; --k) { pipeline.consumer_wait(smem_pipe_read, pipeline.consumer_try_wait(smem_pipe_read)); int stage = smem_pipe_read.index(); warpgroup_fence_operand(accum); warpgroup_arrive(); cute::gemm(tiled_mma, tCrA(_,_,_,stage), tCrB(_,_,_,stage), accum); warpgroup_commit_batch(); warpgroup_wait&lt;1&gt;(); // 允许 1 批在飞 warpgroup_fence_operand(accum); pipeline.consumer_release(smem_pipe_release); ++smem_pipe_read; ++smem_pipe_release; } // Epilogue: 等待所有完成 warpgroup_wait&lt;0&gt;(); // 释放剩余 stages pipeline.consumer_release(smem_pipe_release);} 9. 总结 API PTX 指令 作用 warpgroup_arrive() wgmma.fence.sync.aligned 标记 MMA 批次开始 warpgroup_commit_batch() wgmma.commit_group.sync.aligned 提交当前批次 warpgroup_wait&lt;N&gt;() wgmma.wait_group.sync.aligned N 等待至最多 N 批在飞 warpgroup_fence_operand() 编译器屏障 防止累加器重排 调用顺序: 1fence_operand -&gt; arrive -&gt; gemm -&gt; commit -&gt; wait -&gt; fence_operand -&gt; release 参考资料 源码: sm90_mma_tma_gmma_ss_warpspecialized.hpp 源码: cute/arch/mma_sm90_gmma.hpp NVIDIA PTX ISA - wgmma","link":"/2024/12/24/0x10-warpgroup-mma-api/"},{"title":"0x0F CuTe thrfrg_C 详解：C Tensor 的线程分片机制","text":"本文逐步分析 thrfrg_C，即 CUTLASS TiledMMA 中 C（累加器）Tensor 的线程分片函数。理解这一机制对于掌握 WGMMA 如何在线程间分配输出矩阵至关重要。 示例代码: 0x0F_thrfrg_C.cu 0x0F_thrfrg_C_latex.cu (LaTeX 可视化生成) Key Points thrfrg_C transforms tensor → thread-indexed fragment view CLayout_64x16: 128 threads × 8 values = 1024 elements (64×16 Atom) zipped_divide + compose: Core layout transformation operations AtomLayoutMNK: Enables cooperative tiling across warpgroups Final shape: ((ThrV, (ThrM, ThrN)), (FrgV, (RestM, RestN))) 1. Configuration123456789// Single Atom size: 64 rows × 16 columnsusing AtomShapeM = Int&lt;64&gt;;using AtomShapeN = Int&lt;16&gt;;// Cooperative tiling: 2 Atoms in M directionusing AtomLayoutMNK = Layout&lt;Shape&lt;_2, _1, _1&gt;&gt;;// Thread-Value layout for single Atom (128 threads × 8 values)using AtomLayoutC_TV = CLayout_64x16; CLayout_64x16 Definition123456789template &lt;int N&gt;using CLayout_64xN = Layout&lt; Shape &lt;Shape &lt; _4, _8, _4&gt;, Shape &lt; _2, _2, Int&lt;N/8&gt;&gt;&gt;, Stride&lt;Stride&lt;_128, _1, _16&gt;, Stride&lt;_64, _8, _512&gt;&gt;&gt;;using CLayout_64x16 = CLayout_64xN&lt;16&gt;;// Shape: ((_4,_8,_4),(_2,_2,_2))// Stride: ((_128,_1,_16),(_64,_8,_512))// = 128 threads × 8 values = 1024 elements Visualization: step3_atom_layout_tv.pdf 2. Step 0: Original C Tensor123auto ctensor = make_layout(make_shape(Int&lt;128&gt;{}, Int&lt;128&gt;{}));// ctensor: (_128,_128):(_1,_128)// Total: 16384 elements A 128×128 output matrix in row-major layout. 3. Step 1: Permutation (Identity)12auto t_tensor = ctensor;// No change: (_128,_128):(_1,_128) In this example, no permutation is applied. In general, permutation rearranges tensor dimensions. 4. Step 2: zipped_divide by AtomShape1234auto c_tile = make_tile(make_layout(AtomShapeM{}), // M: 64 make_layout(AtomShapeN{})); // N: 16auto c_tensor = zipped_divide(t_tensor, c_tile);// Result: ((_64,_16),(_2,_8)):((_1,_128),(_64,_2048)) Visualization: step6_zipped_divide_16x16.pdf Shape Decomposition Dimension Components Meaning First (_64,_16) Atom interior 64 rows × 16 cols per Atom Second (_2,_8) Atom indices 2×8 = 16 Atoms total Interpretation 128×128 matrix divided into 64×16 Atoms M direction: 128/64 = 2 Atoms N direction: 128/16 = 8 Atoms Each Atom: 1024 elements 5. Step 3: compose with AtomLayoutC_TV12auto tv_tensor = c_tensor.compose(AtomLayoutC_TV{}, _);// Result: Complex TV layout with thread and value decomposition This is the critical step! compose replaces the Atom-interior layout with the Thread-Value layout. AtomLayoutC_TV Analysis1234AtomLayoutC_TV: ((_4,_8,_4),(_2,_2,_2)):((_128,_1,_16),(_64,_8,_512)) Thr shape: (_4,_8,_4) = 128 threads Val shape: (_2,_2,_2) = 8 values/thread Total: 128 × 8 = 1024 elements Thread 0’s Values: step5_thread0_positions.pdf Thread-Value Layout Visualization12345678910(128 threads × 8 values): 0 1 2 3 4 5 6 7 +------+------+------+------+------+------+------+------+ 0 | 0 | 64 | 8 | 72 | 512 | 576 | 520 | 584 | +------+------+------+------+------+------+------+------+ 1 | 128 | 192 | 136 | 200 | 640 | 704 | 648 | 712 | +------+------+------+------+------+------+------+------+ 2 | 256 | 320 | 264 | 328 | 768 | 832 | 776 | 840 | +------+------+------+------+------+------+------+------+... Full layout: step4_clayout_64x16.pdf Thread 0’s 8 Values123456789Val_idx -&gt; offset -&gt; (m, n)V0 -&gt; offset 0 -&gt; (m= 0, n= 0)V1 -&gt; offset 64 -&gt; (m= 0, n= 1)V2 -&gt; offset 8 -&gt; (m= 8, n= 0)V3 -&gt; offset 72 -&gt; (m= 8, n= 1)V4 -&gt; offset 512 -&gt; (m= 0, n= 8)V5 -&gt; offset 576 -&gt; (m= 0, n= 9)V6 -&gt; offset 520 -&gt; (m= 8, n= 8)V7 -&gt; offset 584 -&gt; (m= 8, n= 9) Pattern: Thread 0 holds 2×2 blocks at positions (0,0), (0,8), (8,0), (8,8) in the 64×16 Atom. 6. Step 4: zipped_divide for Thread Tiling1234567891011auto AtomThrID = Layout&lt;Int&lt;128&gt;&gt;{};auto thr_layout_vmnk = tiled_product(AtomThrID, AtomLayoutMNK{});// thr_layout_vmnk: (_128,_2,_1,_1):(_1,_128,_0,_0)// Total threads: 256auto thr_tile = make_tile(_, make_tile(make_layout(size&lt;1&gt;(thr_layout_vmnk)), make_layout(size&lt;2&gt;(thr_layout_vmnk))));// thr_tile: (_, (_2:_1, _1:_0))auto thr_tensor = zipped_divide(tv_tensor, thr_tile); tiled_product Analysistiled_product(AtomThrID, AtomLayoutMNK): AtomThrID: 128 threads per Atom AtomLayoutMNK: 2×1×1 Atoms (cooperative in M direction) Result: 128×2×1×1 = 256 total threads Final Result12thr_tensor: (((_4,_8,_4),(_2,_1)),((_2,_2,_2),(_1,_8)))Shape: ((ThrV, (ThrM, ThrN)), (FrgV, (RestM', RestN'))) Component Value Meaning ThrV 128 Threads per Atom ThrM 2 Atoms in M (cooperative) ThrN 1 Atoms in N FrgV 8 Values per thread per Atom RestM’ 1 M tiles remaining per thread RestN’ 8 N tiles remaining per thread 7. Summary Verification12345678910111213141516Thread coordinates: ThrV = 128 (threads per Atom) ThrM = 2 (Atoms in M direction) ThrN = 1 (Atoms in N direction) Total = 128 × 2 × 1 = 256 threadsPer-thread values: FrgV = 8 (values per thread per Atom) RestM' = 1 RestN' = 8 Values per thread = 8 × 1 × 8 = 64Verification: Total = threads × values_per_thread = 256 × 64 = 16384 Expected = 128 × 128 = 16384 ✓ 8. Thread Data ExamplesThread 0 (ThrV=0, ThrM=0, ThrN=0) Belongs to: Atom 0 (Warpgroup 0) Covers M rows: 0-63 Values: 64 elements across 8 N-tiles Thread 128 (ThrV=0, ThrM=1, ThrN=0) Belongs to: Atom 1 (Warpgroup 1) Covers M rows: 64-127 Same value pattern but offset by 64 rows Comparing First 4 Threads123456Thread | V0 offset (m,n) | V1 offset (m,n) | V2 offset (m,n) | V3 offset (m,n)-------|-----------------|-----------------|-----------------|---------------- 0 | 0 ( 0, 0) | 64 ( 0, 1) | 8 ( 8, 0) | 72 ( 8, 1) 1 | 128 ( 0, 2) | 192 ( 0, 3) | 136 ( 8, 2) | 200 ( 8, 3) 2 | 256 ( 0, 4) | 320 ( 0, 5) | 264 ( 8, 4) | 328 ( 8, 5) 3 | 384 ( 0, 6) | 448 ( 0, 7) | 392 ( 8, 6) | 456 ( 8, 7) Pattern: Adjacent threads handle adjacent N columns within the same M rows. 9. Data Flow Diagram12345678910111213141516171819Step 0: ctensor (128,128) | vStep 2: zipped_divide by AtomShape (64,16) | | Shape: ((_64,_16), (_2,_8)) | Atom内部 Atom索引 vStep 3: compose with CLayout_64x16 | | Shape: ((128threads, 8values), (2,8)) | Thread-Value Atom索引 vStep 4: zipped_divide by Thread Tiling | | Shape: ((ThrV,(ThrM,ThrN)), (FrgV,(RestM,RestN))) | Thread坐标 Per-thread数据 vFinal: thr_tensor for per-thread access 10. Practical UsageIn actual CUTLASS code, thrfrg_C is used to: Create fragment view: Each thread knows which elements to accumulate Enable WGMMA: Hardware MMA operations expect specific data layouts Cooperative tiling: Multiple warpgroups collaborate on larger tiles 123456// In TiledMMA::thrfrg_Cauto thr_tensor = thrfrg_C(ctensor);// Thread-specific sliceauto thr_frg = thr_tensor(_, threadIdx.x, _);// This gives the 64 elements this thread is responsible for 11. PDF VisualizationsAll visualizations generated by print_latex: step0_original_8x8.pdf - Original layout step1_logical_divide.pdf - logical_divide result step2_zipped_divide.pdf - zipped_divide result step3_atom_layout_tv.pdf - AtomLayoutC_TV step4_clayout_64x16.pdf - CLayout_64x16 (32 threads) step5_thread0_positions.pdf - Thread 0’s values step6_zipped_divide_16x16.pdf - 16x16 zipped_divide References Source: cute-examples/thrfrg_C.cu Source: cute-examples/zipped_divide.cu CUTLASS: include/cute/atom/mma_atom.hpp","link":"/2024/12/24/0x0F-thrfrg-c-analysis/"},{"title":"0x12 TMA Descriptor Fence 详解：Device-Side Descriptor 修改与同步","text":"本文详细解析 CUTLASS 中 TMA Descriptor 的 device-side 修改机制，包括三个关键的 fence 函数：tma_descriptor_cp_fence_release、tma_descriptor_fence_release 和 tma_descriptor_fence_acquire。 背景：为什么需要 Device-Side Descriptor 修改？在标准 TMA 使用场景中，TMA Descriptor 在 host 端创建（通过 cuTensorMapEncode），然后传递给 kernel 使用。但在某些场景下，需要在 device 端动态修改 descriptor： Array GEMM (Batched GEMM)123// 多个 batch 共享相同的 shape，但地址不同ptr_A[] = [ptr_A0, ptr_A1, ptr_A2, ...]ptr_B[] = [ptr_B0, ptr_B1, ptr_B2, ...] 每个 batch 处理完后，需要切换到下一个 batch 的地址。 Grouped GEMM1234// 每个 group 的矩阵形状可能不同Group 0: M=1024, N=512, K=256Group 1: M=2048, N=1024, K=512Group 2: M=512, N=256, K=128 每个 group 需要修改 descriptor 的地址、dims 和 strides。 三个 Fence 函数位于 include/cute/arch/copy_sm90_desc.hpp： 1. tma_descriptor_cp_fence_release1234567void tma_descriptor_cp_fence_release(TmaDescriptor const* gmem_desc_ptr, TmaDescriptor&amp; smem_desc){ asm volatile ( &quot;tensormap.cp_fenceproxy.global.shared::cta.tensormap::generic.release.gpu.sync.aligned [%0], [%1], 128;&quot; :: &quot;l&quot;(gmem_int_desc), &quot;r&quot;(smem_int_desc));} 关键理解：这是一个 SMEM → GMEM 的拷贝操作！ [%0] = GMEM 地址 (dst) [%1] = SMEM 地址 (src) 指令格式：tensormap.cp_fenceproxy.global.shared::cta = 从 shared::cta 拷贝到 global 作用： 将 SMEM 中修改后的 descriptor 拷贝回 GMEM 执行 release fence，确保修改对 TMA Unit 可见 2. tma_descriptor_fence_release1234void tma_descriptor_fence_release(){ asm volatile (&quot;fence.proxy.tensormap::generic.release.gpu;&quot;);} 作用：直接在 GMEM 中修改 descriptor 后使用的 release fence。 3. tma_descriptor_fence_acquire1234567void tma_descriptor_fence_acquire(TmaDescriptor const* desc_ptr){ asm volatile ( &quot;fence.proxy.tensormap::generic.acquire.gpu [%0], 128;&quot; :: &quot;l&quot;(gmem_int_desc) : &quot;memory&quot;);} 作用：Invalidate TMA Unit 的 descriptor cache，确保 TMA Unit 重新读取 GMEM 中的最新 descriptor。 TMA Unit 的 Descriptor CacheTMA Unit 是独立于 SM 的硬件单元，有自己专用的 descriptor cache： 123456789101112131415161718192021222324252627282930┌─────────────────────────────────────────────────────────────┐│ GPU ││ ┌─────────┐ ┌─────────┐ ┌─────────┐ ││ │ SM0 │ │ SM1 │ │ SM2 │ ... ││ │ ┌─────┐ │ │ ┌─────┐ │ │ ┌─────┐ │ ││ │ │ L1 │ │ │ │ L1 │ │ │ │ L1 │ │ ││ │ └─────┘ │ │ └─────┘ │ │ └─────┘ │ ││ └─────────┘ └─────────┘ └─────────┘ ││ │ │ │ ││ └────────────┼────────────┘ ││ ▼ ││ ┌───────────┐ ││ │ L2 │ ││ └───────────┘ ││ │ ││ ┌────────────┼────────────┐ ││ ▼ ▼ ▼ ││ ┌──────────┐ ┌──────────┐ ┌──────────┐ ││ │ TMA Unit │ │ TMA Unit │ │ TMA Unit │ (per GPC/TPC) ││ │┌────────┐│ │┌────────┐│ │┌────────┐│ ││ ││ Desc ││ ││ Desc ││ ││ Desc ││ ← TMA专用缓存 ││ ││ Cache ││ ││ Cache ││ ││ Cache ││ ││ │└────────┘│ │└────────┘│ │└────────┘│ ││ └──────────┘ └──────────┘ └──────────┘ ││ │ ││ ▼ ││ ┌───────────┐ ││ │ GMEM │ (HBM) ││ └───────────┘ │└─────────────────────────────────────────────────────────────┘ 关键点： TMA Unit 的 cache 与 SM 的 L1/L2 是独立的 普通的 __threadfence() 无法影响 TMA Unit 的 cache 必须使用 fence.proxy.tensormap 来 invalidate TMA 的 descriptor cache 完整的 Descriptor 修改流程Array GEMM 场景（只修改地址）1234567891011121314// 1. 在 SMEM 中修改 descriptor 的地址tensormap.replace.tile.global_address [smem_desc], new_ptr;// 2. syncwarp 确保 warp 内所有线程完成修改__syncwarp();// 3. cp_fence_release: SMEM→GMEM + release fencetensormap.cp_fenceproxy.global.shared::cta [gmem_desc], [smem_desc], 128;// 4. fence_acquire: invalidate TMA Unit 的 descriptor cachefence.proxy.tensormap::generic.acquire.gpu [gmem_desc], 128;// 5. 现在可以用更新后的 descriptor 执行 TMA 操作cp.async.bulk.tensor ... Grouped GEMM 场景（修改地址 + dims + strides）1234567891011121314// 1. 修改地址tensormap.replace.tile.global_address [smem_desc], new_ptr;// 2. 修改维度tensormap.replace.tile.global_dim [smem_desc], dim0, new_dim0;tensormap.replace.tile.global_dim [smem_desc], dim1, new_dim1;...// 3. 修改步长tensormap.replace.tile.global_stride [smem_desc], ord0, new_stride0;tensormap.replace.tile.global_stride [smem_desc], ord1, new_stride1;...// 4-6. 同上的同步流程 CUTLASS 中的实现tensormaps_replace_global_address只修改地址，用于 Array GEMM： 123456789// sm90_mma_array_tma_gmma_ss_warpspecialized.hpp:660-665void tensormaps_replace_global_address(..., int32_t next_batch) { cute::tma_descriptor_replace_addr_in_shared_mem( shared_tensormaps.smem_tensormap_A, mainloop_params.ptr_A[next_batch]); cute::tma_descriptor_replace_addr_in_shared_mem( shared_tensormaps.smem_tensormap_B, mainloop_params.ptr_B[next_batch]);} tensormaps_replace_global_tensor_properties修改 dims 和 strides，用于 Grouped GEMM： 123456789101112131415161718192021222324// sm90_mma_array_tma_gmma_ss_warpspecialized.hpp:671-710void tensormaps_replace_global_tensor_properties(...) { const uint32_t M = get&lt;0&gt;(problem_shape_mnkl); const uint32_t N = get&lt;1&gt;(problem_shape_mnkl); const uint32_t K = get&lt;2&gt;(problem_shape_mnkl); // 构建新的 shape 和 stride cute::array&lt;uint32_t, 5&gt; prob_shape_A = {1,1,1,1,1}; cute::array&lt;uint64_t, 5&gt; prob_stride_A = {0,0,0,0,0}; // 从参数获取该 group 的 stride Tensor tensor_a = make_tensor(ptr_A, make_shape(M,K,Int&lt;1&gt;{}), mainloop_params.dA[next_group]); // 填充 shape 和 stride cute::detail::fill_tma_gmem_shape_stride(*observed_tma_load_a_, tensor_a, prob_shape_A, prob_stride_A); // 用 tensormap.replace 指令修改 descriptor cute::tma_descriptor_replace_dims_strides_in_shared_mem( shared_tensormaps.smem_tensormap_A, prob_shape_A, prob_stride_A);} 完整的 tensormaps_perform_update123456789101112void tensormaps_perform_update(..., int32_t next_batch) { if (cute::elect_one_sync()) { // 1. 修改地址 tensormaps_replace_global_address(shared_tensormaps, mainloop_params, next_batch); // 2. 如果是 Grouped GEMM，还需要修改 dims 和 strides if constexpr (IsGroupedGemmKernel) { tensormaps_replace_global_tensor_properties(shared_tensormaps, mainloop_params, next_batch, problem_shape_mnkl); } }} tensormaps_cp_fence_release123456789101112void tensormaps_cp_fence_release(TensorMapStorage&amp; shared_tensormaps, cute::tuple&lt;TensorMapA, TensorMapB&gt; const&amp; input_tensormaps) { if (cute::elect_one_sync()) { cute::tma_desc_commit_group(); cute::tma_desc_wait_group(); } // 整个 warp 必须执行这个操作（对齐要求） tma_descriptor_cp_fence_release(get&lt;0&gt;(input_tensormaps), shared_tensormaps.smem_tensormap_A); tma_descriptor_cp_fence_release(get&lt;1&gt;(input_tensormaps), shared_tensormaps.smem_tensormap_B);} tensormaps_fence_acquire1234void tensormaps_fence_acquire(cute::tuple&lt;TensorMapA, TensorMapB&gt; const&amp; input_tensormaps) { cute::tma_descriptor_fence_acquire(get&lt;0&gt;(input_tensormaps)); cute::tma_descriptor_fence_acquire(get&lt;1&gt;(input_tensormaps));} 数据流总结12345678910111213Host GMEM descriptor (初始) │ ▼ (初始拷贝到SMEM，kernel启动时)SMEM descriptor │ ▼ tensormap.replace (在SMEM中修改)SMEM descriptor (修改后) │ ▼ tensormap.cp_fenceproxy (SMEM→GMEM + release fence)GMEM descriptor (更新后) │ ▼ fence.proxy.tensormap.acquire (invalidate TMA cache)TMA Unit 使用更新后的 descriptor 两种场景对比 场景 每个 batch/group 的矩阵 需要修改的字段 函数调用 Array GEMM 形状相同，地址不同 global_address tensormaps_replace_global_address Grouped GEMM 形状可能不同，地址不同 global_address + dims + strides tensormaps_replace_global_address + tensormaps_replace_global_tensor_properties 为什么要这样设计？节省 host 端创建 descriptor 的开销： TMA descriptor 创建是 host 操作，需要调用 cuTensorMapEncode 如果有 1000 个 batch/group，需要创建 1000 个 descriptor，开销很大 通过在 device 端动态修改地址/dims/strides，只需要创建 1 个 descriptor 模板 PTX 指令参考tensormap.replace12345678// 修改地址tensormap.replace.tile.global_address.shared::cta.b1024.b64 [smem_desc], new_addr;// 修改维度tensormap.replace.tile.global_dim.shared::cta.b1024.b32 [smem_desc], dim_idx, new_dim;// 修改步长tensormap.replace.tile.global_stride.shared::cta.b1024.b64 [smem_desc], ord_idx, new_stride; tensormap.cp_fenceproxy1tensormap.cp_fenceproxy.global.shared::cta.tensormap::generic.release.gpu.sync.aligned [gmem], [smem], 128; fence.proxy.tensormap12345// Release fencefence.proxy.tensormap::generic.release.gpu;// Acquire fencefence.proxy.tensormap::generic.acquire.gpu [gmem_desc], 128; References CUTLASS Source: copy_sm90_desc.hpp CUTLASS Source: sm90_mma_array_tma_gmma_ss_warpspecialized.hpp PTX ISA: tensormap.cp_fenceproxy libcudacxx: tensormap.cp_fenceproxy","link":"/2024/12/28/0x12-tma-descriptor-fence/"}],"tags":[{"name":"layout","slug":"layout","link":"/tags/layout/"},{"name":"thread","slug":"thread","link":"/tags/thread/"},{"name":"value","slug":"value","link":"/tags/value/"},{"name":"mma","slug":"mma","link":"/tags/mma/"},{"name":"wgmma","slug":"wgmma","link":"/tags/wgmma/"},{"name":"ss-mode","slug":"ss-mode","link":"/tags/ss-mode/"},{"name":"rs-mode","slug":"rs-mode","link":"/tags/rs-mode/"},{"name":"tv-layout","slug":"tv-layout","link":"/tags/tv-layout/"},{"name":"CUTLASS","slug":"CUTLASS","link":"/tags/CUTLASS/"},{"name":"CuTe","slug":"CuTe","link":"/tags/CuTe/"},{"name":"GMMA","slug":"GMMA","link":"/tags/GMMA/"},{"name":"SM90","slug":"SM90","link":"/tags/SM90/"},{"name":"TiledMMA","slug":"TiledMMA","link":"/tags/TiledMMA/"},{"name":"elect","slug":"elect","link":"/tags/elect/"},{"name":"warp","slug":"warp","link":"/tags/warp/"},{"name":"synchronization","slug":"synchronization","link":"/tags/synchronization/"},{"name":"leader","slug":"leader","link":"/tags/leader/"},{"name":"Tensor","slug":"Tensor","link":"/tags/Tensor/"},{"name":"Engine","slug":"Engine","link":"/tags/Engine/"},{"name":"Pipeline","slug":"Pipeline","link":"/tags/Pipeline/"},{"name":"mbarrier","slug":"mbarrier","link":"/tags/mbarrier/"},{"name":"Warp Specialization","slug":"Warp-Specialization","link":"/tags/Warp-Specialization/"},{"name":"CUDA","slug":"CUDA","link":"/tags/CUDA/"},{"name":"GEMM","slug":"GEMM","link":"/tags/GEMM/"},{"name":"PTX","slug":"PTX","link":"/tags/PTX/"},{"name":"TMA","slug":"TMA","link":"/tags/TMA/"},{"name":"Multicast","slug":"Multicast","link":"/tags/Multicast/"},{"name":"Cluster","slug":"Cluster","link":"/tags/Cluster/"},{"name":"Layout","slug":"Layout","link":"/tags/Layout/"},{"name":"logical_divide","slug":"logical-divide","link":"/tags/logical-divide/"},{"name":"zipped_divide","slug":"zipped-divide","link":"/tags/zipped-divide/"},{"name":"thrfrg_C","slug":"thrfrg-C","link":"/tags/thrfrg-C/"},{"name":"get_slice","slug":"get-slice","link":"/tags/get-slice/"},{"name":"WGMMA","slug":"WGMMA","link":"/tags/WGMMA/"},{"name":"Hopper","slug":"Hopper","link":"/tags/Hopper/"},{"name":"Thread-Value Layout","slug":"Thread-Value-Layout","link":"/tags/Thread-Value-Layout/"},{"name":"Descriptor","slug":"Descriptor","link":"/tags/Descriptor/"},{"name":"Fence","slug":"Fence","link":"/tags/Fence/"}],"categories":[{"name":"CUDA","slug":"CUDA","link":"/categories/CUDA/"},{"name":"GPU Computing","slug":"GPU-Computing","link":"/categories/GPU-Computing/"},{"name":"CuTe","slug":"CUDA/CuTe","link":"/categories/CUDA/CuTe/"},{"name":"PTX","slug":"CUDA/PTX","link":"/categories/CUDA/PTX/"},{"name":"CUTLASS","slug":"CUTLASS","link":"/categories/CUTLASS/"},{"name":"CUTLASS","slug":"CUDA/CuTe/CUTLASS","link":"/categories/CUDA/CuTe/CUTLASS/"},{"name":"Hopper","slug":"CUDA/PTX/Hopper","link":"/categories/CUDA/PTX/Hopper/"}],"pages":[{"title":"关于","text":"关于本站这是一个专注于 NVIDIA CUTLASS 和 CuTE 内部实现的学习笔记站点。 内容涵盖 Pipeline 同步机制: mbarrier、双 Barrier 架构、PTX 指令映射 Epilogue 融合: EVT (Expression Visitor Tree)、FusionCallbacks CuTE Tensor: Layout、Stride、TMA 联系方式 GitHub: DrXuQian","link":"/about/index.html"},{"title":"","text":"/* Custom styles for wider page width */ /* ===== Dark Mode Theme Toggle ===== */ /* Theme toggle button in navbar */ .theme-toggle { cursor: pointer; padding: 0.5rem; display: flex; align-items: center; justify-content: center; font-size: 1.2rem; color: inherit; background: transparent; border: none; transition: transform 0.3s ease; } .theme-toggle:hover { transform: scale(1.1); } .theme-toggle .icon-sun, .theme-toggle .icon-moon { display: none; } /* Light mode: show moon icon */ html:not([data-theme=\"dark\"]) .theme-toggle .icon-moon { display: inline; } /* Dark mode: show sun icon */ html[data-theme=\"dark\"] .theme-toggle .icon-sun { display: inline; } /* ===== Dark Mode Styles ===== */ html[data-theme=\"dark\"] { --bg-color: #1a1a2e; --bg-color-secondary: #16213e; --text-color: #e4e4e4; --text-color-secondary: #a0a0a0; --border-color: #2d2d4a; --card-bg: #1f1f38; --link-color: #6c9fff; --link-hover: #99b8ff; --code-bg: #0d1117; } html[data-theme=\"dark\"] body { background-color: var(--bg-color) !important; color: var(--text-color) !important; } html[data-theme=\"dark\"] .navbar { background-color: var(--bg-color-secondary) !important; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.3); } html[data-theme=\"dark\"] .navbar-item, html[data-theme=\"dark\"] .navbar-link { color: var(--text-color) !important; } html[data-theme=\"dark\"] .navbar-item:hover, html[data-theme=\"dark\"] .navbar-link:hover { background-color: rgba(255, 255, 255, 0.1) !important; color: var(--link-color) !important; } html[data-theme=\"dark\"] .card { background-color: var(--card-bg) !important; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3) !important; } html[data-theme=\"dark\"] .card-content { background-color: var(--card-bg) !important; } html[data-theme=\"dark\"] .title, html[data-theme=\"dark\"] .subtitle, html[data-theme=\"dark\"] h1, html[data-theme=\"dark\"] h2, html[data-theme=\"dark\"] h3, html[data-theme=\"dark\"] h4, html[data-theme=\"dark\"] h5, html[data-theme=\"dark\"] h6 { color: var(--text-color) !important; } html[data-theme=\"dark\"] .content { color: var(--text-color) !important; } html[data-theme=\"dark\"] a { color: var(--link-color) !important; } html[data-theme=\"dark\"] a:hover { color: var(--link-hover) !important; } html[data-theme=\"dark\"] .menu-label { color: var(--text-color-secondary) !important; } html[data-theme=\"dark\"] .menu-list a { color: var(--text-color) !important; } html[data-theme=\"dark\"] .menu-list a:hover { background-color: rgba(255, 255, 255, 0.05) !important; color: var(--link-color) !important; } html[data-theme=\"dark\"] .tag { background-color: var(--border-color) !important; color: var(--text-color) !important; } html[data-theme=\"dark\"] .footer { background-color: var(--bg-color-secondary) !important; color: var(--text-color-secondary) !important; } html[data-theme=\"dark\"] .widget { background-color: var(--card-bg) !important; } html[data-theme=\"dark\"] pre, html[data-theme=\"dark\"] code { background-color: var(--code-bg) !important; } html[data-theme=\"dark\"] .hljs { background-color: var(--code-bg) !important; } html[data-theme=\"dark\"] .article-meta, html[data-theme=\"dark\"] .level-item { color: var(--text-color-secondary) !important; } html[data-theme=\"dark\"] .button.is-primary { background-color: #3273dc !important; } html[data-theme=\"dark\"] .input, html[data-theme=\"dark\"] .textarea, html[data-theme=\"dark\"] .select select { background-color: var(--bg-color-secondary) !important; border-color: var(--border-color) !important; color: var(--text-color) !important; } html[data-theme=\"dark\"] .pagination-link, html[data-theme=\"dark\"] .pagination-previous, html[data-theme=\"dark\"] .pagination-next { background-color: var(--card-bg) !important; border-color: var(--border-color) !important; color: var(--text-color) !important; } html[data-theme=\"dark\"] .toc-list a { color: var(--text-color) !important; } html[data-theme=\"dark\"] .toc-list a:hover, html[data-theme=\"dark\"] .toc-list a.is-active { color: var(--link-color) !important; } html[data-theme=\"dark\"] hr { background-color: var(--border-color) !important; } html[data-theme=\"dark\"] blockquote { background-color: var(--bg-color-secondary) !important; border-left-color: var(--link-color) !important; } html[data-theme=\"dark\"] table th, html[data-theme=\"dark\"] table td { border-color: var(--border-color) !important; } html[data-theme=\"dark\"] table thead th { background-color: var(--bg-color-secondary) !important; color: var(--text-color) !important; } html[data-theme=\"dark\"] table tbody tr:nth-child(even) { background-color: rgba(255, 255, 255, 0.02) !important; } /* ===== End Dark Mode Styles ===== */ /* ===== Hide right sidebar on post pages ===== */ body.is-post-page .column.column-right { display: none !important; } /* When right sidebar is hidden, expand main content */ body.is-post-page .column.column-main.is-8-tablet.is-8-desktop.is-6-widescreen { width: 70% !important; flex: none !important; } body.is-post-page .column.column-left.is-4-tablet.is-4-desktop.is-3-widescreen { width: 30% !important; flex: none !important; } /* For two-column layout on post pages */ body.is-post-page .columns { justify-content: center; } /* ===== End Hide Right Sidebar ===== */ /* Increase container max-width */ html body .container { max-width: 1600px !important; } /* Main content column - wider */ html body .column.is-8-tablet, html body .column.is-8-desktop, html body .column.is-8-widescreen, html body .is-8-desktop, html body .is-8-widescreen { flex: none !important; width: 70% !important; } /* Sidebar column - narrower */ html body .column.is-4-tablet, html body .column.is-4-desktop, html body .column.is-4-widescreen, html body .is-4-desktop, html body .is-4-widescreen { flex: none !important; width: 30% !important; } /* Two sidebar layout - left sidebar smaller */ html body .column.is-8-tablet.is-8-desktop.is-6-widescreen { width: 55% !important; flex: none !important; } html body .column.is-4-tablet.is-4-desktop.is-3-widescreen { width: 22.5% !important; flex: none !important; } /* Card content padding */ html body .card-content { padding: 1.25rem 1.5rem; } /* Code block */ html body .content pre { max-width: 100%; overflow-x: auto; } /* Article content */ html body .article-content { max-width: 100%; } /* Large screens */ @media screen and (min-width: 1408px) { html body .container { max-width: 1700px !important; } } @media screen and (min-width: 1600px) { html body .container { max-width: 1900px !important; } } @media screen and (min-width: 1800px) { html body .container { max-width: 2100px !important; } html body .column.is-8-desktop, html body .column.is-8-widescreen { width: 72% !important; } html body .column.is-4-desktop, html body .column.is-4-widescreen { width: 28% !important; } }","link":"/css/custom.css"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"","text":"// Theme toggle functionality (function() { 'use strict'; // Detect if current page is a post page and add class to body function detectPostPage() { // Check if URL matches post pattern (e.g., /2024/12/23/post-name/) const path = window.location.pathname; const isPost = /^\\/\\d{4}\\/\\d{2}\\/\\d{2}\\//.test(path); if (isPost) { document.body.classList.add('is-post-page'); } else { document.body.classList.remove('is-post-page'); } } // Get saved theme or default to light function getTheme() { return localStorage.getItem('theme') || 'light'; } // Apply theme to document function applyTheme(theme) { if (theme === 'dark') { document.documentElement.setAttribute('data-theme', 'dark'); } else { document.documentElement.removeAttribute('data-theme'); } } // Save theme preference function saveTheme(theme) { localStorage.setItem('theme', theme); } // Toggle between light and dark function toggleTheme() { const currentTheme = getTheme(); const newTheme = currentTheme === 'dark' ? 'light' : 'dark'; applyTheme(newTheme); saveTheme(newTheme); } // Create and inject toggle button into navbar function createToggleButton() { const navbarEnd = document.querySelector('.navbar-end'); if (!navbarEnd) return; // Check if button already exists if (document.querySelector('.theme-toggle')) return; const toggleBtn = document.createElement('a'); toggleBtn.className = 'navbar-item theme-toggle'; toggleBtn.title = 'Toggle Dark Mode'; toggleBtn.innerHTML = ''; toggleBtn.addEventListener('click', function(e) { e.preventDefault(); toggleTheme(); }); // Insert before the search button const searchBtn = navbarEnd.querySelector('.search'); if (searchBtn) { navbarEnd.insertBefore(toggleBtn, searchBtn); } else { navbarEnd.appendChild(toggleBtn); } } // Initialize on page load function init() { // Apply saved theme immediately applyTheme(getTheme()); // Detect post page and hide right sidebar detectPostPage(); // Create toggle button when DOM is ready if (document.readyState === 'loading') { document.addEventListener('DOMContentLoaded', createToggleButton); } else { createToggleButton(); } } // Apply theme immediately to prevent flash applyTheme(getTheme()); // Run init when DOM is ready if (document.readyState === 'loading') { document.addEventListener('DOMContentLoaded', init); } else { init(); } // Re-create button and detect page type after pjax navigation (Icarus uses pjax) document.addEventListener('pjax:complete', function() { createToggleButton(); detectPostPage(); }); })();","link":"/js/theme-toggle.js"},{"title":"标签","text":"","link":"/tags/index.html"}]}