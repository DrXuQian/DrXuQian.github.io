{"posts":[{"title":"CUTLASS SM90 TMA Descriptor 深度解析","text":"本文深入解析 NVIDIA Hopper (SM90) 架构的 TMA（Tensor Memory Accelerator）机制，包括 Descriptor 构建、Prefetch、Copy 指令以及与 CuTe Tensor 的集成。 1. TMA 概述1.1 什么是 TMATMA（Tensor Memory Accelerator）是 NVIDIA Hopper (SM90) 架构引入的硬件加速单元，专门用于高效的张量数据传输。它的主要特点： 异步执行：TMA 操作与 SM 计算完全异步，由专用硬件单元执行 硬件地址生成：自动计算多维张量的内存地址，无需软件计算 支持复杂布局：原生支持 swizzle、padding、stride 等复杂内存模式 Multicast 支持：单次操作可向 cluster 内多个 CTA 广播数据 1.2 TMA vs 传统 Copy 特性 传统 Copy TMA 地址计算 软件计算，占用寄存器 硬件计算，基于 Descriptor 执行单元 SM (CUDA Cores) 专用 TMA 硬件单元 同步方式 __syncthreads() mbarrier Swizzle 软件实现 硬件原生支持 多维支持 需要手动展开 原生 1D-5D 1.3 TMA 工作流程概览graph LR subgraph Host[\"Host (Kernel Launch)\"] GT[GMEM Tensor] SL[SMEM Layout] MK[make_tma_copy] end subgraph Descriptor[\"TMA Descriptor (128B)\"] DESC[cuTensorMapEncodeTiled] end subgraph Device[\"Device (SM90)\"] PF[Prefetch Descriptor] LOAD[TMA Load/Store] MBAR[mbarrier sync] end GT --> MK SL --> MK MK --> DESC DESC --> PF PF --> LOAD LOAD --> MBAR style DESC fill:#e1f5fe style MBAR fill:#fff3e0 2. TMA Descriptor 结构2.1 类型定义TMA Descriptor 是一个 128 字节的数据结构，存储了完整的张量传输信息： 12345678// 源码: include/cute/arch/copy_sm90_desc.hpp:291-297#if (__CUDACC_VER_MAJOR__ &gt;= 12) &amp;&amp; !defined(__CUDACC_RTC__) using TmaDescriptor = CUtensorMap; // CUDA 12.0+ 使用原生类型 using Im2ColTmaDescriptor = CUtensorMap;#else using TmaDescriptor = struct alignas(64) { char bytes[128]; }; // 128字节，64字节对齐 using Im2ColTmaDescriptor = struct alignas(64) { char bytes[128]; };#endif 关键约束： 大小：128 字节 对齐：64 字节对齐（硬件要求） 存储位置：通常在 constant memory 或 global memory 2.2 Descriptor 编码参数Descriptor 通过 cuTensorMapEncodeTiled() CUDA Driver API 创建： 123456789101112131415// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:1024-1053CUresult result = cuTensorMapEncodeTiled( tma_desc, // 输出：TMA descriptor 指针 tma_format, // 数据类型 (FP32, FP16, BF16, INT8, etc.) tma_dim, // 维度数 (1-5) gmem_address, // 全局内存基地址 (16字节对齐) gmem_prob_shape, // 各维度大小 (uint32_t[5]) gmem_prob_stride + 1, // 各维度步长 (uint64_t[5], 字节为单位) smem_box_shape, // SMEM tile 各维度大小 (uint32_t[5], 最大256) smem_box_stride, // SMEM tile 步长 (uint32_t[5]) tma_interleave, // 交错模式 tma_swizzle, // Swizzle 模式 (32B, 64B, 128B) tma_l2Promotion, // L2 缓存策略 tma_oobFill // 越界填充值 (ZERO 或 CONSTANT)); 2.3 参数详解 参数 类型 约束 描述 gmem_address void* 16 字节对齐 全局内存基地址 gmem_prob_shape[i] uint32_t 1 ~ 2^32 第 i 维的元素数 gmem_prob_stride[i] uint64_t 16 字节对齐，最大 2^40 第 i 维的字节步长 smem_box_shape[i] uint32_t 1 ~ 256 SMEM tile 第 i 维大小 smem_box_stride[i] uint32_t 1 ~ 8 SMEM tile 第 i 维步长 2.4 Swizzle 模式Swizzle 用于优化 shared memory bank conflict： 12345// Swizzle 模式选项CU_TENSOR_MAP_SWIZZLE_NONE // 无 swizzleCU_TENSOR_MAP_SWIZZLE_32B // 32 字节 swizzleCU_TENSOR_MAP_SWIZZLE_64B // 64 字节 swizzleCU_TENSOR_MAP_SWIZZLE_128B // 128 字节 swizzle 2.5 数据类型映射1234567891011// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:906-918// TMA 数据格式映射CU_TENSOR_MAP_DATA_TYPE_UINT8 // uint8_t, int8_tCU_TENSOR_MAP_DATA_TYPE_UINT16 // uint16_t, int16_t, half_t, bfloat16_tCU_TENSOR_MAP_DATA_TYPE_UINT32 // uint32_t, int32_t, floatCU_TENSOR_MAP_DATA_TYPE_UINT64 // uint64_t, int64_t, doubleCU_TENSOR_MAP_DATA_TYPE_FLOAT16 // half_t (FP16 专用)CU_TENSOR_MAP_DATA_TYPE_FLOAT32 // floatCU_TENSOR_MAP_DATA_TYPE_FLOAT64 // doubleCU_TENSOR_MAP_DATA_TYPE_BFLOAT16 // bfloat16_tCU_TENSOR_MAP_DATA_TYPE_FLOAT32_FTZ // TF32 (Tensor Float 32) 3. TMA Descriptor 创建流程3.1 make_tma_copy APICUTLASS/CuTe 提供了高层 API 来创建 TMA copy atom： 12345678910111213141516171819202122232425262728// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:1221-1336template &lt;class TmaInternalType = void, class CopyOp, class GEngine, class GLayout, class SLayout, class CTA_Tiler, class Cluster_Size&gt;CUTE_HOST_RTCautomake_tma_copy(CopyOp const&amp; copy_op, // SM90_TMA_LOAD 等 Tensor&lt;GEngine,GLayout&gt; const&amp; gtensor, // 全局内存张量 SLayout const&amp; slayout, // 共享内存布局 CTA_Tiler const&amp; cta_tiler, // CTA tile 大小 Cluster_Size const&amp; cluster_size) // Cluster 大小{ // 1. 创建 CTA tile 和 cluster tile 的布局 auto cta_v_tile = make_identity_layout(shape(gtensor)).compose(cta_tiler); auto cta_t_tile = make_layout(cluster_size); // 2. 推导 TMA 内部数据类型 using TmaType = conditional_t&lt;is_same&lt;void, TmaInternalType&gt;::value, typename GEngine::value_type, TmaInternalType&gt;; // 3. 调用内部实现创建 TiledCopy return detail::make_tma_copy_tiled&lt;TmaType&gt;(copy_op, gtensor, slayout, cta_t_tile, cta_v_tile);} 3.2 使用示例123456789101112131415161718192021// 创建全局内存张量 (M x K 矩阵)auto gmem_tensor = make_tensor( make_gmem_ptr(ptr_A), make_shape(M, K), make_stride(K, Int&lt;1&gt;{}));// 定义共享内存布局 (128 x 64 tile，带 swizzle)auto smem_layout = make_layout( make_shape(Int&lt;128&gt;{}, Int&lt;64&gt;{}), GenColMajor{});// 创建 TMA copy atomauto tma_load_a = make_tma_copy( SM90_TMA_LOAD{}, // TMA Load 操作 gmem_tensor, // 源：全局内存张量 smem_layout, // 目标：共享内存布局 make_shape(Int&lt;128&gt;{}, Int&lt;64&gt;{}), // CTA tile 大小 Int&lt;1&gt;{} // Cluster 大小); 3.3 内部创建流程graph TB subgraph Input[\"输入\"] GT[GMEM Tensorshape, stride, ptr] SL[SMEM Layoutshape, stride, swizzle] CT[CTA Tiler] CS[Cluster Size] end subgraph Process[\"处理流程\"] V1[提取 GMEM 参数] V2[计算 SMEM box 参数] V3[确定 Swizzle 模式] V4[创建 Basis Mapping] end subgraph Output[\"输出\"] DESC[TmaDescriptor128 bytes] AUX[AuxTmaParamsg_stride_, TmaGmemBasis] TC[TiledCopy Atom] end GT --> V1 SL --> V2 SL --> V3 V1 --> V4 V2 --> V4 V4 --> DESC V4 --> AUX DESC --> TC AUX --> TC style DESC fill:#e1f5fe style TC fill:#e8f5e9 3.4 参数验证TMA 对参数有严格要求，CUTLASS 在创建时会验证： 123456789101112// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:949-979// 地址对齐检查assert((reinterpret_cast&lt;uint64_t&gt;(gmem_address) &amp; 0b1111) == 0); // 16B 对齐// Shape 范围检查assert(gmem_prob_shape[i] &gt;= 1 &amp;&amp; gmem_prob_shape[i] &lt;= (1u &lt;&lt; 32));// Stride 对齐检查assert((gmem_prob_stride[i] &amp; 0b1111) == 0); // 16B 对齐// SMEM box 大小检查assert(smem_box_shape[i] &gt;= 1 &amp;&amp; smem_box_shape[i] &lt;= 256); 4. TMA Prefetch 机制4.1 Descriptor Prefetch将 TMA Descriptor 预取到 L2 缓存，加速后续 TMA 操作： 12345678910111213// 源码: include/cute/arch/copy_sm90_desc.hpp:302-317CUTE_HOST_DEVICE voidprefetch_tma_descriptor(TmaDescriptor const* desc_ptr){#if defined(CUTE_ARCH_TMA_SM90_ENABLED) uint64_t gmem_int_desc = reinterpret_cast&lt;uint64_t&gt;(desc_ptr); asm volatile ( &quot;prefetch.tensormap [%0];&quot; : : &quot;l&quot;(gmem_int_desc) : &quot;memory&quot;);#endif} PTX 指令： 1prefetch.tensormap [desc_addr]; 4.2 Data Prefetch预取数据到 L2 缓存（不写入 SMEM）： 12345678910111213141516// 源码: include/cute/arch/copy_sm90_tma.hpp:81-100struct SM90_TMA_LOAD_1D::PREFETCH{ CUTE_HOST_DEVICE static void copy(void const* desc_ptr, int32_t const&amp; crd0) {#if defined(CUTE_ARCH_TMA_SM90_ENABLED) uint64_t gmem_int_desc = reinterpret_cast&lt;uint64_t&gt;(desc_ptr); asm volatile ( &quot;cp.async.bulk.prefetch.tensor.1d.L2.global [%0, {%1}];&quot; : : &quot;l&quot;(gmem_int_desc), &quot;r&quot;(crd0) : &quot;memory&quot;);#endif }}; PTX 指令（1D-5D）： 1234567// 1Dcp.async.bulk.prefetch.tensor.1d.L2.global [desc], {crd0};// 2Dcp.async.bulk.prefetch.tensor.2d.L2.global [desc], {crd0, crd1};// 3D-5D 类似... 4.3 Prefetch 使用时机sequenceDiagram participant K as Kernel Start participant PF as Prefetch participant L2 as L2 Cache participant TMA as TMA Load K->>PF: prefetch_tma_descriptor() PF->>L2: 预取 Descriptor Note over L2: Descriptor 缓存 K->>PF: TMA_LOAD::PREFETCH PF->>L2: 预取第一批数据 K->>TMA: TMA Load (实际传输) L2-->>TMA: Descriptor 命中 Note over TMA: 快速启动传输 5. TMA Copy 指令5.1 TMA Load从全局内存加载数据到共享内存： 12345678910111213141516171819202122232425262728// 源码: include/cute/arch/copy_sm90_tma.hpp:49-79struct SM90_TMA_LOAD_1D{ CUTE_HOST_DEVICE static void copy(void const* desc_ptr, // TMA Descriptor 指针 uint64_t* mbar_ptr, // mbarrier 指针 uint64_t cache_hint, // L2 缓存提示 void* smem_ptr, // 目标 SMEM 地址 int32_t const&amp; crd0) // 坐标 {#if defined(CUTE_ARCH_TMA_SM90_ENABLED) uint64_t gmem_int_desc = reinterpret_cast&lt;uint64_t&gt;(desc_ptr); uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr); uint32_t smem_int_ptr = cast_smem_ptr_to_uint(smem_ptr); asm volatile ( &quot;cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint&quot; &quot; [%0], [%1, {%3}], [%2], %4;&quot; : : &quot;r&quot;(smem_int_ptr), // 目标 SMEM &quot;l&quot;(gmem_int_desc), // TMA Descriptor &quot;r&quot;(smem_int_mbar), // mbarrier &quot;r&quot;(crd0), // 坐标 &quot;l&quot;(cache_hint) // 缓存提示 : &quot;memory&quot;);#endif }}; PTX 指令格式： 123456// 基本格式cp.async.bulk.tensor.{dim}d.{dst}.{src}.mbarrier::complete_tx::bytes [smem], [desc, {coords}], [mbar];// 完整示例 (2D)cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint [smem_ptr], [tma_desc, {crd0, crd1}], [mbar_ptr], cache_hint; 5.2 TMA Store从共享内存存储数据到全局内存： 12345678910111213141516171819202122// 源码: include/cute/arch/copy_sm90_tma.hpp:980-1001struct SM90_TMA_STORE_2D{ CUTE_HOST_DEVICE static void copy(void const* desc_ptr, void const* smem_ptr, int32_t const&amp; crd0, int32_t const&amp; crd1) {#if defined(CUTE_ARCH_TMA_SM90_ENABLED) uint64_t gmem_int_desc = reinterpret_cast&lt;uint64_t&gt;(desc_ptr); uint32_t smem_int_ptr = cast_smem_ptr_to_uint(smem_ptr); asm volatile ( &quot;cp.async.bulk.tensor.2d.global.shared::cta.bulk_group [%0, {%2, %3}], [%1];&quot; : : &quot;l&quot;(gmem_int_desc), // TMA Descriptor &quot;r&quot;(smem_int_ptr), // 源 SMEM &quot;r&quot;(crd0), &quot;r&quot;(crd1) // 坐标 : &quot;memory&quot;);#endif }}; PTX 指令格式： 1cp.async.bulk.tensor.{dim}d.global.shared::cta.bulk_group [desc, {coords}], [smem]; 5.3 TMA Multicast向 cluster 内多个 CTA 同时广播数据： 12345678910111213141516171819// 源码: include/cute/arch/copy_sm90_tma.hpp:275-306struct SM90_TMA_LOAD_MULTICAST_1D{ CUTE_HOST_DEVICE static void copy(void const* desc_ptr, uint64_t* mbar_ptr, uint64_t cache_hint, uint16_t multicast_mask, // 目标 CTA 掩码 void* smem_ptr, int32_t const&amp; crd0) { asm volatile ( &quot;cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.L2::cache_hint&quot; &quot; [%0], [%1, {%4}], [%2], %3, %5;&quot; : : &quot;r&quot;(smem_int_ptr), &quot;l&quot;(gmem_int_desc), &quot;r&quot;(smem_int_mbar), &quot;h&quot;(multicast_mask), &quot;r&quot;(crd0), &quot;l&quot;(cache_hint) : &quot;memory&quot;); }}; 5.4 TMA Fence/Commit/Wait用于 TMA Store 的同步控制： 1234567891011121314151617181920// 源码: include/cute/arch/copy_sm90_tma.hpp:1213-1274// Fence: 确保之前的 SMEM 写入完成CUTE_HOST_DEVICE static voidtma_store_fence() { asm volatile (&quot;fence.proxy.async.shared::cta;&quot;);}// Commit: 标记一组 TMA store 完成CUTE_HOST_DEVICE static voidtma_store_arrive() { asm volatile(&quot;cp.async.bulk.commit_group;&quot;);}// Wait: 等待最多 Count 个未完成的 TMA storetemplate &lt;int Count&gt;CUTE_HOST_DEVICE static voidtma_store_wait() { asm volatile(&quot;cp.async.bulk.wait_group.read %0;&quot; : : &quot;n&quot;(Count) : &quot;memory&quot;);} 5.5 TMA Load vs Store 对比 特性 TMA Load TMA Store 方向 GMEM → SMEM SMEM → GMEM 同步机制 mbarrier bulk_group + wait Multicast 支持 不支持 Scope shared::cluster shared::cta 6. CuTe Tensor 与 TMA 集成6.1 Copy_Traits 结构TMA 操作通过 Copy_Traits 封装： 1234567891011121314151617181920212223242526272829// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:98-166template &lt;class NumBitsPerTMA, class AuxParams_&gt;struct Copy_Traits&lt;SM90_TMA_LOAD, NumBitsPerTMA, AuxParams_&gt;{ using ThrID = Layout&lt;_1&gt;; // 单线程执行 using SrcLayout = Layout&lt;Shape&lt;_1,NumBitsPerTMA&gt;&gt;; using DstLayout = Layout&lt;Shape&lt;_1,NumBitsPerTMA&gt;&gt;; using RefLayout = SrcLayout; // TMA Descriptor 存储 TmaDescriptor tma_desc_; // 辅助参数（stride 映射等） using AuxParams = AuxParams_; AuxParams aux_params_; // 获取 Descriptor 指针 CUTE_HOST_DEVICE constexpr TmaDescriptor const* get_tma_descriptor() const { return &amp;tma_desc_; } // 生成坐标张量 template &lt;class GShape&gt; CUTE_HOST_DEVICE constexpr auto get_tma_tensor(GShape const&amp; g_shape) const { return make_coord_tensor(make_layout(g_shape, aux_params_.g_stride_)); }}; 6.2 AuxTmaParams 辅助参数存储 GMEM 到 TMA 坐标的映射关系： 123456789101112// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:50-58template &lt;class GmemTmaBasisStrides_, class TmaGmemBasis_, class TmaSwizzle_&gt;struct AuxTmaParams { using GmemStrides = GmemTmaBasisStrides_; GmemStrides g_stride_; // GMEM mode → TMA coord 的映射 using TmaGmemBasis = TmaGmemBasis_; // 静态 basis 信息 static_assert(is_static&lt;TmaGmemBasis&gt;::value); using TmaSwizzle = TmaSwizzle_; // Swizzle 模式 static_assert(is_static&lt;TmaSwizzle&gt;::value);}; 6.3 可执行 TMA Copy运行时携带 barrier 和 cache hint： 12345678910111213141516// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:169-198template &lt;class NumBitsPerTMA&gt;struct Copy_Traits&lt;SM90_TMA_LOAD_OP, NumBitsPerTMA&gt; : TMA_LOAD_Unpack&lt;SM90_TMA_LOAD_OP, NumBitsPerTMA&gt;{ // 运行时参数 tuple&lt; TmaDescriptor const*, // Descriptor 指针 uint64_t*, // mbarrier 指针 uint64_t // L2 cache hint &gt; const opargs_; CUTE_HOST_DEVICE Copy_Traits(TmaDescriptor const* desc, uint64_t* mbar, uint64_t cache) : opargs_(desc, mbar, cache) {}}; 6.4 Copy Unpack 执行将 CuTe tensor copy 转换为 TMA 指令： 12345678910111213141516171819202122232425262728// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:60-87template &lt;class CopyOp, class... Args&gt;struct TMA_LOAD_Unpack{ template &lt;class TS, class SLayout, class TD, class DLayout&gt; CUTE_HOST_DEVICE friend constexpr void copy_unpack(Copy_Traits&lt;CopyOp, Args...&gt; const&amp; traits, Tensor&lt;TS,SLayout&gt; const&amp; src, // GMEM 坐标张量 Tensor&lt;TD,DLayout&gt; &amp; dst) // SMEM 数据张量 { // 验证目标是共享内存 static_assert(is_smem&lt;TD&gt;::value, &quot;SM90_TMA_LOAD requires the destination be shared memory.&quot;); // 提取源坐标 auto src_coord = src.data().coord_; // 获取目标指针 void* dst_ptr = cute::raw_pointer_cast(dst.data()); // 展开并调用 TMA 指令 return detail::explode_tuple(detail::CallCOPY&lt;CopyOp&gt;{}, traits.opargs_, ..., make_tuple(dst_ptr), ..., src_coord, ...); }}; 6.5 完整数据流graph TB subgraph CuTe[\"CuTe Layer\"] GS[gmem_tensorshape + stride + ptr] CT[TiledCopyCopy_Traits] CR[coord_tensorget_tma_tensor] ST[smem_tensor] end subgraph TMA[\"TMA Layer\"] DESC[TmaDescriptor] MBAR[mbarrier] PTX[PTX Instructions] end subgraph HW[\"Hardware\"] TU[TMA Unit] GMEM[(Global Memory)] SMEM[(Shared Memory)] end GS --> CT CT --> DESC CT --> CR CR -->|src coords| PTX ST -->|dst ptr| PTX DESC -->|desc ptr| PTX MBAR -->|mbar ptr| PTX PTX --> TU TU --> GMEM TU --> SMEM style DESC fill:#e1f5fe style MBAR fill:#fff3e0 style TU fill:#e8f5e9 7. 完整使用示例7.1 创建 TMA Copy1234567891011121314151617181920212223242526272829// 步骤 1: 定义矩阵参数constexpr int M = 4096;constexpr int K = 4096;constexpr int TILE_M = 128;constexpr int TILE_K = 64;// 步骤 2: 创建全局内存张量half_t* ptr_A = ...; // 设备内存指针auto gmem_A = make_tensor( make_gmem_ptr(ptr_A), make_shape(M, K), make_stride(K, Int&lt;1&gt;{}) // Row-major);// 步骤 3: 定义共享内存布局（带 swizzle）auto smem_layout_A = composition( Swizzle&lt;3, 3, 3&gt;{}, make_layout(make_shape(Int&lt;TILE_M&gt;{}, Int&lt;TILE_K&gt;{}), make_stride(Int&lt;TILE_K&gt;{}, Int&lt;1&gt;{})));// 步骤 4: 创建 TMA copy atomauto tma_load_A = make_tma_copy( SM90_TMA_LOAD{}, gmem_A, smem_layout_A, make_shape(Int&lt;TILE_M&gt;{}, Int&lt;TILE_K&gt;{}), Int&lt;1&gt;{}); 7.2 Kernel 中使用 TMA123456789101112131415161718192021222324252627282930313233343536373839404142__global__ void kernel(TmaLoadA tma_load_A, ...) { // 共享内存 extern __shared__ char smem[]; half_t* smem_A = reinterpret_cast&lt;half_t*&gt;(smem); // mbarrier __shared__ uint64_t mbar; // 初始化 barrier (仅 thread 0) if (threadIdx.x == 0) { mbarrier_init(&amp;mbar, 1); // 期望 1 次 arrive // Prefetch descriptor prefetch_tma_descriptor(tma_load_A.get_tma_descriptor()); } __syncthreads(); // 计算 tile 坐标 int tile_m = blockIdx.x; int tile_k = 0; // 获取坐标张量 auto coord_tensor = tma_load_A.get_tma_tensor(make_shape(M, K)); auto coord = coord_tensor(tile_m, tile_k); // 创建 SMEM 张量 auto smem_tensor = make_tensor(make_smem_ptr(smem_A), smem_layout_A); // 发起 TMA load (仅 thread 0) if (threadIdx.x == 0) { // 设置期望传输字节数 mbarrier_arrive_expect_tx(&amp;mbar, size(smem_tensor) * sizeof(half_t)); // 发起 TMA copy(tma_load_A.with(&amp;mbar, 0), coord, smem_tensor); } // 等待 TMA 完成 mbarrier_wait(&amp;mbar, 0); // phase = 0 // 使用 smem_A 进行计算...} 7.3 与 Pipeline 配合1234567891011121314151617181920212223242526272829// 多 stage pipelinefor (int k = 0; k &lt; num_k_tiles; ++k) { // 获取当前 stage auto stage = k % Stages; auto&amp; smem_A = smem_A_stages[stage]; auto&amp; mbar = mbar_full[stage]; // Producer: TMA load if (is_producer_thread) { pipeline.producer_acquire(pipe_state); auto coord = coord_tensor(tile_m, k); copy(tma_load_A.with(pipeline.producer_get_barrier(pipe_state), 0), coord, smem_A); ++pipe_state; } // Consumer: MMA compute if (is_consumer_thread) { pipeline.consumer_wait(pipe_state_c); // 使用 smem_A 进行 MMA gemm(smem_A, smem_B, accum); pipeline.consumer_release(pipe_state_c); ++pipe_state_c; }} 8. TMA API 到 PTX 映射总览 CuTe/CUTLASS API PTX 指令 prefetch_tma_descriptor() prefetch.tensormap [addr] SM90_TMA_LOAD::PREFETCH cp.async.bulk.prefetch.tensor.Xd.L2.global SM90_TMA_LOAD_Xd::copy() cp.async.bulk.tensor.Xd.shared::cluster.global.mbarrier::complete_tx::bytes SM90_TMA_LOAD_MULTICAST 同上 + .multicast::cluster SM90_TMA_STORE_Xd::copy() cp.async.bulk.tensor.Xd.global.shared::cta.bulk_group tma_store_fence() fence.proxy.async.shared::cta tma_store_arrive() cp.async.bulk.commit_group tma_store_wait&lt;N&gt;() cp.async.bulk.wait_group.read N 9. 关键要点总结 TMA Descriptor 是核心：128 字节结构，编码完整的张量传输信息 Host 端创建，Device 端使用：Descriptor 在 kernel launch 前创建，kernel 中只需传递指针 与 mbarrier 配合：TMA Load 完成时自动 signal barrier，实现异步同步 Prefetch 提升性能：预取 descriptor 和数据到 L2 缓存 Multicast 节省带宽：单次 TMA 可向多个 CTA 广播数据 CuTe 抽象层：make_tma_copy() 隐藏底层复杂性，提供高层 API 参考资料 CUTLASS GitHub 仓库 copy_sm90_tma.hpp copy_sm90_desc.hpp copy_traits_sm90_tma.hpp NVIDIA PTX ISA - TMA CUDA Programming Guide - TMA","link":"/2024/12/24/tma-descriptor-deep-dive/"},{"title":"CUTLASS SM90 TMA Multicast 深度解析","text":"本文深入解析 NVIDIA Hopper (SM90) 架构的 TMA Multicast 机制，包括 multicast mask 的计算、Cluster 与 Multicast 的关系，以及在 GEMM 中的实际应用。 1. TMA Multicast 概述1.1 什么是 TMA MulticastTMA Multicast 是 Hopper 架构 TMA 的高级特性，允许单次 TMA 操作将数据广播到 Cluster 内的多个 CTA（Thread Block）。这对于 GEMM 等需要数据复用的场景非常有价值。 1.2 为什么需要 Multicast在 GEMM 中，矩阵 A 和 B 的数据被多个输出 tile 共享： graph TB subgraph \"Cluster (2x2)\" subgraph \"Row 0\" CTA00[\"CTA(0,0)C[0,0]\"] CTA01[\"CTA(0,1)C[0,1]\"] end subgraph \"Row 1\" CTA10[\"CTA(1,0)C[1,0]\"] CTA11[\"CTA(1,1)C[1,1]\"] end end A0[\"A row 0\"] --> CTA00 A0 --> CTA01 A1[\"A row 1\"] --> CTA10 A1 --> CTA11 B0[\"B col 0\"] --> CTA00 B0 --> CTA10 B1[\"B col 1\"] --> CTA01 B1 --> CTA11 style A0 fill:#e3f2fd style A1 fill:#e3f2fd style B0 fill:#fff3e0 style B1 fill:#fff3e0 矩阵 A：同一行的 CTA 共享相同的 A tile（沿 N 方向广播） 矩阵 B：同一列的 CTA 共享相同的 B tile（沿 M 方向广播） 1.3 Multicast vs 独立加载 方式 带宽消耗 延迟 每个 CTA 独立加载 N × 数据量 竞争 L2/HBM Multicast 1 × 数据量 一次加载，硬件广播 对于 2×2 Cluster，Multicast 可节省约 50% 的内存带宽。 2. Multicast Mask 原理2.1 Mask 结构Multicast mask 是一个 16-bit 整数，每一位对应 Cluster 内的一个 CTA： 1234567891011Cluster 内 CTA 编号 (block_rank_in_cluster): Cluster 2x2 示例: +-------+-------+ | CTA 0 | CTA 1 | (row 0) +-------+-------+ | CTA 2 | CTA 3 | (row 1) +-------+-------+ multicast_mask = 0b0011 表示广播到 CTA 0 和 CTA 1 multicast_mask = 0b0101 表示广播到 CTA 0 和 CTA 2 2.2 CTA 编号计算CTA 在 Cluster 内的编号由 Layout&lt;ClusterShape&gt; 决定： 123456// 源码: include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp:331constexpr uint32_t cluster_shape_x = get&lt;0&gt;(typename DispatchPolicy::ClusterShape());uint2 cluster_local_block_id = { block_rank_in_cluster % cluster_shape_x, // x = M 方向 block_rank_in_cluster / cluster_shape_x // y = N 方向}; 对于 ClusterShape = Shape&lt;_2, _2&gt;： block_rank = 0 → (x=0, y=0) block_rank = 1 → (x=1, y=0) block_rank = 2 → (x=0, y=1) block_rank = 3 → (x=1, y=1) 2.3 Mask 计算逻辑graph LR subgraph Input CS[ClusterShape] BR[block_rank_in_cluster] end subgraph Process LO[Layout ClusterShape] LC[local_block_idx, y] MA[Mask for A沿 N 方向] MB[Mask for B沿 M 方向] end CS --> LO BR --> LC LO --> MA LO --> MB LC --> MA LC --> MB style MA fill:#e3f2fd style MB fill:#fff3e0 3. Multicast Mask 实现详解3.1 矩阵 A 的 Mask（沿 N 方向广播）矩阵 A 的同一行数据被 N 方向的所有 CTA 共享： 12345678// 源码: include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp:357-362uint16_t mcast_mask_a = 0;auto block_layout = Layout&lt;typename DispatchPolicy::ClusterShape&gt;{}; // (m,n) -&gt; block_id// 固定 M 坐标（当前 CTA 的 x），遍历所有 N 坐标for (int n = 0; n &lt; size&lt;1&gt;(block_layout); ++n) { mcast_mask_a |= (uint16_t(1) &lt;&lt; block_layout(cluster_local_block_id.x, n, Int&lt;0&gt;{}));} 示例：ClusterShape = (2, 2)，当前 CTA 的 x = 0 n block_layout(0, n) 累加 mask 0 0 0b0001 1 2 0b0101 最终 mcast_mask_a = 0b0101（广播到 CTA 0 和 CTA 2） 3.2 矩阵 B 的 Mask（沿 M 方向广播）矩阵 B 的同一列数据被 M 方向的所有 CTA 共享： 12345678// 源码: include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp:364-368uint16_t mcast_mask_b = 0;auto block_layout = Layout&lt;typename DispatchPolicy::ClusterShape&gt;{};// 固定 N 坐标（当前 CTA 的 y），遍历所有 M 坐标for (int m = 0; m &lt; size&lt;0&gt;(block_layout); ++m) { mcast_mask_b |= (uint16_t(1) &lt;&lt; block_layout(m, cluster_local_block_id.y, Int&lt;0&gt;{}));} 示例：ClusterShape = (2, 2)，当前 CTA 的 y = 0 m block_layout(m, 0) 累加 mask 0 0 0b0001 1 1 0b0011 最终 mcast_mask_b = 0b0011（广播到 CTA 0 和 CTA 1） 3.3 完整 Mask 计算图示graph TB subgraph \"Cluster 2x2\" CTA0[\"CTA 0(0,0)\"] CTA1[\"CTA 1(1,0)\"] CTA2[\"CTA 2(0,1)\"] CTA3[\"CTA 3(1,1)\"] end subgraph \"Matrix A Masks\" MA0[\"CTA 0: mask_a=0b0101广播到 CTA 0,2\"] MA1[\"CTA 1: mask_a=0b1010广播到 CTA 1,3\"] end subgraph \"Matrix B Masks\" MB0[\"CTA 0: mask_b=0b0011广播到 CTA 0,1\"] MB2[\"CTA 2: mask_b=0b1100广播到 CTA 2,3\"] end CTA0 -.-> MA0 CTA1 -.-> MA1 CTA2 -.-> MA0 CTA3 -.-> MA1 CTA0 -.-> MB0 CTA1 -.-> MB0 CTA2 -.-> MB2 CTA3 -.-> MB2 style MA0 fill:#e3f2fd style MA1 fill:#e3f2fd style MB0 fill:#fff3e0 style MB2 fill:#fff3e0 4. create_tma_multicast_mask 函数4.1 通用版本CuTe 提供了通用的 multicast mask 创建函数： 123456789101112131415161718192021222324252627282930313233// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:1439-1469template &lt;class CTA_Layout, class CTA_Coord&gt;CUTE_HOST_DEVICE constexpruint16_tcreate_tma_multicast_mask(CTA_Layout const&amp; cta_layout, // Cluster 布局 CTA_Coord const&amp; cta_coord) // 当前 CTA 坐标{ // 将坐标分解为参与 multicast 的部分和不参与的部分 auto [rest_coord, mcast_coord] = slice_and_offset(cta_coord, cta_layout); // 获取 multicast 相关的子布局 auto mcast_layout = cta_layout.slice(rest_coord); uint16_t mcast_mask = 0; // 优化路径：rank-1 且深度 &lt;= 1 if constexpr (rank(mcast_layout) == 1 &amp;&amp; depth(mcast_layout) &lt;= 1) { // 位 smearing 技术 mcast_mask = uint16_t(1) &lt;&lt; mcast_coord; mcast_mask |= mcast_mask &lt;&lt; (1 * stride&lt;0&gt;(mcast_layout)); mcast_mask |= mcast_mask &lt;&lt; (2 * stride&lt;0&gt;(mcast_layout)); mcast_mask |= mcast_mask &lt;&lt; (4 * stride&lt;0&gt;(mcast_layout)); mcast_mask |= mcast_mask &lt;&lt; (8 * stride&lt;0&gt;(mcast_layout)); } else { // 通用路径：遍历所有位置 for (int i = 0; i &lt; size(mcast_layout); ++i) { mcast_mask |= (uint16_t(1) &lt;&lt; mcast_layout(i)); } } return mcast_mask;} 4.2 带 Mode 参数的版本用于指定沿哪个维度进行 multicast： 1234567891011121314// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:1475-1479template &lt;int... Mode, class CTA_Layout, class CTA_Coord&gt;CUTE_HOST_DEVICE constexpruint16_tcreate_tma_multicast_mask(CTA_Layout const&amp; cta_layout, CTA_Coord const&amp; cta_coord){ // 只保留指定 Mode 的坐标，其他设为 0 auto proj_coord = cute::make_tuple( cute::conditional_return&lt;cute::is_one_of&lt;Int&lt;Mode&gt;...&gt;::template eval&lt;Int&lt;I&gt;&gt;()&gt;( get&lt;I&gt;(cta_coord), Int&lt;0&gt;{})... ); return create_tma_multicast_mask(cta_layout, proj_coord);} 使用示例： 123456789// 源码: include/cutlass/gemm/collective/sm90_sparse_mma_tma_gmma_ss_warpspecialized.hpp:425-431Layout cta_layout_mnk = make_layout(ClusterShape{});auto cta_coord_mnk = cta_layout_mnk.get_flat_coord(block_rank_in_cluster);// Mode&lt;1&gt; = N 方向，用于矩阵 Auint16_t mcast_mask_a = create_tma_multicast_mask&lt;1&gt;(cta_layout_mnk, cta_coord_mnk);// Mode&lt;0&gt; = M 方向，用于矩阵 Buint16_t mcast_mask_b = create_tma_multicast_mask&lt;0&gt;(cta_layout_mnk, cta_coord_mnk); 5. TMA Multicast 指令5.1 PTX 指令格式12cp.async.bulk.tensor.{dim}d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.L2::cache_hint [smem_ptr], [tma_desc, {coords}], [mbar_ptr], multicast_mask, cache_hint; 关键修饰符： .multicast::cluster：启用 cluster 内 multicast multicast_mask：16-bit 掩码，指示目标 CTA 5.2 实现代码123456789101112131415161718192021222324252627282930// 源码: include/cute/arch/copy_sm90_tma.hpp:626-652struct SM90_TMA_LOAD_MULTICAST_1D{ CUTE_HOST_DEVICE static void copy(void const* desc_ptr, uint64_t* mbar_ptr, uint16_t multicast_mask, // &lt;-- multicast 掩码 uint64_t cache_hint, void* smem_ptr, int32_t const&amp; crd0) {#if defined(CUTE_ARCH_TMA_SM90_ENABLED) uint64_t gmem_int_desc = reinterpret_cast&lt;uint64_t&gt;(desc_ptr); uint32_t smem_int_mbar = cast_smem_ptr_to_uint(mbar_ptr); uint32_t smem_int_ptr = cast_smem_ptr_to_uint(smem_ptr); asm volatile ( &quot;cp.async.bulk.tensor.1d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.L2::cache_hint&quot; &quot; [%0], [%1, {%4}], [%2], %3, %5;&quot; : : &quot;r&quot;(smem_int_ptr), // 目标 SMEM 地址 &quot;l&quot;(gmem_int_desc), // TMA Descriptor &quot;r&quot;(smem_int_mbar), // mbarrier 地址 &quot;h&quot;(multicast_mask), // 16-bit multicast 掩码 &quot;r&quot;(crd0), // 坐标 &quot;l&quot;(cache_hint) // L2 缓存提示 : &quot;memory&quot;);#endif }}; 5.3 Copy_Traits 集成123456789101112131415// 源码: include/cute/atom/copy_traits_sm90_tma.hpp:278-296// .with() 方法绑定运行时参数CUTE_HOST_DEVICE constexprCopy_Traits&lt;SM90_TMA_LOAD_MULTICAST_OP, NumBitsPerTMA&gt;with(uint64_t&amp; tma_load_mbar, uint16_t const&amp; multicast_mask, TMA::CacheHintSm90 const&amp; cache_hint = TMA::CacheHintSm90::EVICT_NORMAL) const{ return { &amp;tma_desc_, &amp;tma_load_mbar, multicast_mask, static_cast&lt;uint64_t&gt;(cache_hint) };} 6. 在 GEMM Collective 中的应用6.1 Warp-Specialized Mainloop1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// 源码: include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp:310-390template &lt;class... Args&gt;CUTLASS_DEVICE voidload(Params const&amp; mainloop_params, MainloopPipeline pipeline, PipelineState smem_pipe_write, cute::tuple&lt;Args...&gt; const&amp; load_inputs, BlockCoord const&amp; blk_coord, KTileIterator k_tile_iter, int k_tile_count, int lane_idx, uint32_t block_rank_in_cluster, SharedStorage&amp; shared_storage){ // 步骤 1: 计算本地 CTA 坐标 constexpr uint32_t cluster_shape_x = get&lt;0&gt;(ClusterShape{}); uint2 cluster_local_block_id = { block_rank_in_cluster % cluster_shape_x, block_rank_in_cluster / cluster_shape_x }; // 步骤 2: 根据 CTA 坐标分区 TMA descriptor auto [tAgA, tAsA] = tma_partition_A(cluster_local_block_id.y); auto [tBgB, tBsB] = tma_partition_B(cluster_local_block_id.x); // 步骤 3: 计算 multicast mask uint16_t mcast_mask_a = 0; uint16_t mcast_mask_b = 0; auto block_layout = Layout&lt;ClusterShape&gt;{}; // A: 沿 N 方向广播（固定 M，遍历 N） CUTLASS_PRAGMA_UNROLL for (int n = 0; n &lt; size&lt;1&gt;(block_layout); ++n) { mcast_mask_a |= (uint16_t(1) &lt;&lt; block_layout(cluster_local_block_id.x, n, Int&lt;0&gt;{})); } // B: 沿 M 方向广播（固定 N，遍历 M） CUTLASS_PRAGMA_UNROLL for (int m = 0; m &lt; size&lt;0&gt;(block_layout); ++m) { mcast_mask_b |= (uint16_t(1) &lt;&lt; block_layout(m, cluster_local_block_id.y, Int&lt;0&gt;{})); } // 步骤 4: Mainloop - TMA Load with Multicast CUTLASS_PRAGMA_NO_UNROLL for ( ; k_tile_count &gt; 0; --k_tile_count) { // 获取 pipeline stage pipeline.producer_acquire(smem_pipe_write); BarrierType* tma_barrier = pipeline.producer_get_barrier(smem_pipe_write); int write_stage = smem_pipe_write.index(); // 发起 TMA Multicast Load copy(mainloop_params.tma_load_a.with(*tma_barrier, mcast_mask_a), tAgA(_,_,_,*k_tile_iter), tAsA(_,_,_,write_stage)); copy(mainloop_params.tma_load_b.with(*tma_barrier, mcast_mask_b), tBgB(_,_,_,*k_tile_iter), tBsB(_,_,_,write_stage)); ++k_tile_iter; ++smem_pipe_write; }} 6.2 执行流程时序图sequenceDiagram participant CTA0 as CTA 0 (Producer) participant CTA1 as CTA 1 participant TMA as TMA Unit participant GMEM as Global Memory participant SMEM0 as SMEM (CTA 0) participant SMEM1 as SMEM (CTA 1) Note over CTA0,CTA1: Cluster 2x1, 加载矩阵 B (mask=0b11) CTA0->>CTA0: 计算 mcast_mask_b = 0b11 CTA0->>TMA: copy(tma_load_b.with(mbar, 0b11), ...) TMA->>GMEM: 读取 B tile TMA->>SMEM0: 写入数据 (multicast) TMA->>SMEM1: 写入数据 (multicast) TMA->>CTA0: signal mbar (CTA 0) TMA->>CTA1: signal mbar (CTA 1) Note over CTA0,CTA1: 两个 CTA 同时收到数据 7. Multicast 性能优化7.1 最佳实践 选择合适的 Cluster Shape 太小：multicast 收益有限 太大：受限于 SM 资源，可能降低 occupancy 对齐数据 TMA 要求 16 字节对齐 Multicast 不会改变对齐要求 平衡 A 和 B 的 multicast ClusterShape = (M, N) A multicast 因子 = N B multicast 因子 = M 7.2 Cluster Shape 选择指南 Cluster Shape A Multicast B Multicast 总带宽节省 (1, 1) 1x 1x 0% (2, 1) 1x 2x ~25% (1, 2) 2x 1x ~25% (2, 2) 2x 2x ~50% (4, 1) 1x 4x ~37.5% (2, 4) 4x 2x ~62.5% 7.3 限制条件 Cluster 最大支持 16 个 CTA（由 16-bit mask 限制） Multicast 只在同一 Cluster 内有效 所有目标 CTA 的 SMEM 布局必须相同 8. API 总结 API 描述 create_tma_multicast_mask(layout, coord) 创建 multicast mask create_tma_multicast_mask&lt;Mode&gt;(layout, coord) 沿指定 Mode 创建 mask tma_load.with(mbar, mask) 绑定 mbarrier 和 mask SM90_TMA_LOAD_MULTICAST_Xd::copy(...) 底层 PTX 包装 9. 关键要点 Multicast Mask 是 16-bit：每位对应 cluster 内一个 CTA Layout 决定编号：Layout&lt;ClusterShape&gt; 将 (m,n) 映射到线性 block_id A/B 不同方向： A 沿 N 方向广播（固定 M） B 沿 M 方向广播（固定 N） 由 Producer 发起：只需一个 CTA 发起 TMA，硬件负责广播 与 mbarrier 配合：所有目标 CTA 的 barrier 都会被 signal 参考资料 CUTLASS GitHub 仓库 sm90_mma_tma_gmma_ss_warpspecialized.hpp copy_traits_sm90_tma.hpp copy_sm90_tma.hpp NVIDIA PTX ISA - TMA Multicast","link":"/2024/12/24/tma-multicast-deep-dive/"},{"title":"CUTLASS SM90 Pipeline 与 mbarrier 深度解析","text":"本文深入解析 CUTLASS SM90 Pipeline 机制及其底层 mbarrier PTX 指令的映射关系。所有代码引用均来自 NVIDIA CUTLASS 官方仓库。 1. mbarrier 原理1.1 什么是 mbarriermbarrier（Memory Barrier）是 NVIDIA Hopper (SM90) 架构引入的硬件同步原语，存储在共享内存（SMEM）中。它是一个 64-bit 的硬件对象，支持： 到达计数（Arrival Counting）：追踪有多少线程已经到达 barrier 事务计数（Transaction Counting）：追踪 TMA 传输了多少字节（仅 ClusterTransactionBarrier） Phase 位：用于区分不同轮次的同步 1.2 mbarrier 64-bit 结构 字段 位数 描述 Phase Bit 1 完成时翻转，用于区分不同轮次 Pending TX Count ~20 期望传输的字节数（TMA 完成时递减） Arrival Count ~20 剩余需要 arrive 的线程数 完成条件： 1Pending TX Count == 0 AND Arrival Count == 0 → Phase Bit 翻转 1.3 Phase-Parity 机制Phase 位是 mbarrier 实现循环复用的关键： Barrier 初始化时 phase = 0 当所有条件满足（到达计数和事务计数都归零），phase 翻转（0→1 或 1→0） try_wait.parity 指令检查当前 phase 是否匹配期望值 这样同一个 barrier 可以在不同迭代中重复使用 1.4 两种 Barrier 类型CUTLASS 定义了两种 barrier 类型： 类型 用途 完成条件 ClusterBarrier 纯到达计数 Arrival Count == 0 ClusterTransactionBarrier 到达 + 事务计数 Arrival Count == 0 AND TX Count == 0 源码位置：barrier.h 1.5 双 Barrier 架构Pipeline 使用双 Barrier 实现生产者-消费者同步： graph LR subgraph Producer[\"Producer (TMA Warp)\"] P1[producer_acquire] P2[TMA Load] P1 --> P2 end subgraph Stage[\"Pipeline Stage [i]\"] EB[EmptyBarrierClusterBarrier] BUF[(SMEM Buffer)] FB[FullBarrierClusterTransactionBarrier] end subgraph Consumer[\"Consumer (MMA Warps)\"] C1[consumer_wait] C2[MMA Compute] C3[consumer_release] C1 --> C2 --> C3 end P1 -.->|1. wait| EB P1 -->|2. expect_tx| FB P2 -->|3. write| BUF P2 -->|4. complete_tx| FB C1 -.->|5. wait| FB BUF -->|6. read| C2 C3 -->|7. arrive| EB style FB fill:#e1f5fe style EB fill:#fff3e0 style BUF fill:#e8f5e9 图例说明： 虚线 -.-&gt; 表示等待/阻塞操作 实线 --&gt; 表示信号/数据传输操作 Barrier 类型 谁 Signal 谁 Wait 含义 FullBarrier ClusterTransactionBarrier Producer (TMA) Consumer “Data is ready” EmptyBarrier ClusterBarrier Consumer Producer “Buffer is free” 1.6 Cluster 级别的 mbarrier 同步与 DSMEMmbarrier 存储在共享内存（SMEM）中，但 Hopper 架构引入的 Thread Block Cluster 和 Distributed Shared Memory (DSMEM) 机制使得跨 SM 的 barrier 同步成为可能。 1.6.1 DSMEM 硬件架构Hopper 引入了专用的 SM-to-SM 网络，使 Cluster 内的多个 SM 可以直接访问彼此的共享内存： graph TB subgraph GPC[\"Graphics Processing Cluster (GPC)\"] subgraph SM0[\"SM 0 (CTA 0)\"] SMEM0[SMEMmbarrier_0] end subgraph SM1[\"SM 1 (CTA 1)\"] SMEM1[SMEMmbarrier_1] end subgraph SM2[\"SM 2 (CTA 2)\"] SMEM2[SMEMmbarrier_2] end subgraph SM3[\"SM 3 (CTA 3)\"] SMEM3[SMEMmbarrier_3] end NET[SM-to-SM NetworkLow Latency NOC] SM0 NET SM1 NET SM2 NET SM3 NET end style NET fill:#e3f2fd style SMEM0 fill:#fff8e1 style SMEM1 fill:#fff8e1 style SMEM2 fill:#fff8e1 style SMEM3 fill:#fff8e1 关键特性： 专用互连：SM-to-SM 网络是独立于 L2 缓存的专用硬件通路 低延迟：跨 SM 访问延迟约 180 cycles，比经过 L2（~956 cycles）快约 5.3 倍 高带宽：2-block cluster 可达 ~3.27 TB/s，4-block cluster 约 2.65 TB/s 并发使用：DSMEM 可与 L2 缓存同时使用，带宽可叠加 1.6.2 mapa 指令：跨 SM 地址映射要访问其他 SM 的共享内存，需要使用 mapa 指令将本地 SMEM 地址转换为远程可访问地址： 12// PTX: 将本地 smem_addr 映射到目标 CTA 的对应地址mapa.shared::cluster.u32 remote_addr, smem_addr, dst_cta_rank; 工作原理： 每个 CTA 在 Cluster 内有唯一的 rank（0 到 cluster_size-1） mapa 根据目标 CTA rank 计算出可跨 SM 访问的地址 返回的地址可用于 load/store/atomic 操作 123456789101112// CUTLASS 封装：cute/arch/cluster_sm90.hpp:166-177CUTE_DEVICE uint32_t set_block_rank(uint32_t smemAddr, uint32_t rank) {#if defined(CUTE_ARCH_CLUSTER_SM90_ENABLED) uint32_t result; asm volatile(&quot;mapa.shared::cluster.u32 %0, %1, %2;\\n&quot; : &quot;=r&quot;(result) : &quot;r&quot;(smemAddr), &quot;r&quot;(rank)); return result;#else return smemAddr;#endif} 1.6.3 跨 SM 的 mbarrier Arrive当 Cluster 内的一个 CTA 需要 signal 另一个 CTA 的 mbarrier 时，使用 mbarrier.arrive.shared::cluster： 123456789101112131415// 源码: include/cutlass/arch/barrier.h:480-499static void arrive(ValueType const* smem_ptr, uint32_t cta_id, uint32_t pred) {#if CUDA_BARRIER_ENABLED uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); if (pred) { asm volatile( &quot;{\\n\\t&quot; &quot;.reg .b32 remAddr32;\\n\\t&quot; &quot;mapa.shared::cluster.u32 remAddr32, %0, %1;\\n\\t&quot; // Step 1: 地址映射 &quot;mbarrier.arrive.shared::cluster.b64 _, [remAddr32];\\n\\t&quot; // Step 2: 远程 arrive &quot;}&quot; : : &quot;r&quot;(smem_addr), &quot;r&quot;(cta_id)); }#endif} PTX 指令对比： 操作范围 PTX 指令 说明 CTA 内 mbarrier.arrive.shared::cta.b64 仅本 CTA 线程可见 Cluster 内 mbarrier.arrive.shared::cluster.b64 Cluster 内所有 CTA 可见 1.6.4 Cluster Barrier 同步流程sequenceDiagram participant CTA0 as CTA 0 (SM 0) participant NET as SM-to-SM Network participant CTA1 as CTA 1 (SM 1) participant CTA2 as CTA 2 (SM 2) participant CTA3 as CTA 3 (SM 3) Note over CTA0,CTA3: ClusterShape = 2x2, 共 4 个 CTA rect rgb(232, 245, 233) Note over CTA0: CTA 0 完成计算 CTA0->>CTA0: mbarrier.arrive.shared::cta (本地) CTA0->>NET: mapa + arrive (到 CTA 1) NET->>CTA1: arrive signal CTA0->>NET: mapa + arrive (到 CTA 2) NET->>CTA2: arrive signal CTA0->>NET: mapa + arrive (到 CTA 3) NET->>CTA3: arrive signal end rect rgb(227, 242, 253) Note over CTA1,CTA3: 其他 CTA 类似操作... end Note over CTA0,CTA3: 所有 arrive 完成后，barrier phase 翻转 1.6.5 Cluster 全局同步除了 mbarrier 的点对点同步，Hopper 还提供了 Cluster 级别的全局 barrier： 12345678910111213// 源码: cute/arch/cluster_sm90.hpp:48-83CUTE_DEVICE void cluster_arrive() { asm volatile(&quot;barrier.cluster.arrive.aligned;\\n&quot; : : );}CUTE_DEVICE void cluster_wait() { asm volatile(&quot;barrier.cluster.wait.aligned;\\n&quot; : : );}CUTE_DEVICE void cluster_sync() { cluster_arrive(); cluster_wait();} 使用场景： barrier.cluster 用于 Cluster 内所有 CTA 的全局同步 常用于 barrier 初始化后确保所有 CTA 可见性 与 fence.mbarrier_init.release.cluster 配合使用 1.6.6 硬件加速的 Asynchronous Transaction BarrierHopper 的 mbarrier 支持硬件加速等待： graph LR subgraph Traditional[\"传统 Barrier (Ampere)\"] T1[Thread Arrive] --> T2[Spin Wait] T2 --> T3[Barrier Complete] style T2 fill:#ffcdd2 end subgraph Hopper[\"Hopper Async Barrier\"] H1[Thread Arrive] --> H2[Hardware Sleep] H2 --> H3[Hardware Wakeup] H3 --> H4[Barrier Complete] style H2 fill:#c8e6c9 end 关键优势： 线程 arrive 后可以 sleep 而非 spin，节省能耗 硬件自动唤醒等待线程 与 TMA 操作深度集成，TMA 完成时自动 complete_tx 1.6.7 性能数据参考 指标 数值 备注 本地 SMEM 访问 ~29 cycles 基准延迟 DSMEM 本地访问 ~33 cycles 轻微额外开销 跨 SM DSMEM 访问 ~181 cycles 2-block cluster 经过 L2 访问 ~956 cycles 传统路径 DSMEM 带宽 (2-block) ~3.27 TB/s 峰值吞吐 DSMEM 带宽 (4-block) ~2.65 TB/s 带宽竞争 注意：随着 Cluster size 增大，SM 间带宽竞争加剧，吞吐量会下降。 1.6.8 参考资料 NVIDIA Hopper Architecture In-Depth Dissecting the NVIDIA Hopper Architecture through Microbenchmarking Hopper Tuning Guide CUTLASS Tutorial: Mastering the TMA 2. PipelineState 详解PipelineState 是 Pipeline 的状态追踪器，管理循环缓冲区中的当前位置。 2.1 数据结构定义12345678910111213141516171819// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:170-250template&lt;uint32_t Stages_&gt;struct PipelineState { static constexpr uint32_t Stages = Stages_; int index_ = 0; // 当前 stage 索引 (0 到 Stages-1) uint32_t phase_ = 0; // 当前 phase (0 或 1)，每绕回一次翻转 uint32_t count_ = 0; // 总迭代次数 CUTLASS_DEVICE PipelineState(): index_{}, phase_{}, count_{} {} CUTLASS_DEVICE PipelineState(int index, uint32_t phase, uint32_t count) : index_(index) , phase_(phase) , count_(count) {}}; 2.2 成员变量说明 成员 类型 描述 index_ int 当前 stage 在循环缓冲区中的索引，范围 [0, Stages-1] phase_ uint32_t 当前 phase 值，0 或 1，用于 barrier 同步 count_ uint32_t 总迭代计数，用于追踪已处理多少次 2.3 自增操作符123456789101112// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:203-213CUTLASS_DEVICEvoid operator++() { if constexpr (Stages &gt; 0) { ++index_; ++count_; if (index_ == Stages) { index_ = 0; // 回绕到开头 phase_ ^= 1; // 翻转 phase } }} 关键点：当 index_ 从 Stages-1 回绕到 0 时，phase_ 翻转。这确保了 barrier 能区分不同轮次的操作。 stateDiagram-v2 direction LR state \"Stage 0\" as S0 state \"Stage 1\" as S1 state \"Stage 2\" as S2 state \"Stage 3\" as S3 [*] --> S0: index=0, phase=0 S0 --> S1: ++index S1 --> S2: ++index S2 --> S3: ++index S3 --> S0: index回绕, phase翻转 note right of S3 当 index == Stages 时: index = 0 phase ^= 1 end note 2.4 advance 方法12345678910111213141516// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:228-244CUTLASS_DEVICEPipelineState&amp; advance(uint32_t num_iterations) { if constexpr (Stages &gt; 0) { // 判断是否需要翻转 phase if ((num_iterations &lt; Stages) &amp;&amp; (index_ + num_iterations) &gt;= Stages ) { phase_ ^= 1; } if ((num_iterations &gt;= Stages) &amp;&amp; (((index_ + num_iterations) / Stages) % 2) == 1) { phase_ ^= 1; } index_ = (index_ + num_iterations) % Stages; count_ += num_iterations; } return *this;} 2.5 访问器方法123CUTLASS_DEVICE int index() const { return index_; }CUTLASS_DEVICE uint32_t phase() const { return phase_; }CUTLASS_DEVICE uint32_t count() const { return count_; } 2.6 Producer 起始状态12345678910// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:252-260template&lt;class Pipeline&gt;CUTLASS_DEVICEPipelineState&lt;Pipeline::Stages&gt; make_producer_start_state() { // Producer 以相反的 phase 开始，因为缓冲区初始为空 constexpr int InitialProducerStage = 0; constexpr uint32_t InitialProducerPhase = 1; // 注意：phase 为 1 constexpr uint32_t InitialProducerCount = 0; return {InitialProducerStage, InitialProducerPhase, InitialProducerCount};} 重要：Producer 初始 phase 为 1，而 Consumer 初始 phase 为 0。这是因为缓冲区一开始是空的，Producer 需要先填充数据。 3. PipelineTmaAsync 详解PipelineTmaAsync 是 SM90 上用于 TMA 异步加载的 Pipeline 类，实现了生产者-消费者同步模式。 源码位置：sm90_pipeline.hpp 3.1 类型定义123456789101112// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:270-278template &lt;int Stages_&gt;class PipelineTmaAsync {public: using FullBarrier = cutlass::arch::ClusterTransactionBarrier; using EmptyBarrier = cutlass::arch::ClusterBarrier; using ProducerBarrierType = FullBarrier::ValueType; // uint64_t using ConsumerBarrierType = EmptyBarrier::ValueType; // uint64_t static constexpr uint32_t Stages = Stages_; using PipelineState = cutlass::PipelineState&lt;Stages&gt;; // ...}; 类型别名 实际类型 用途 FullBarrier ClusterTransactionBarrier 数据就绪信号，支持事务计数 EmptyBarrier ClusterBarrier 空间释放信号，纯到达计数 ProducerBarrierType uint64_t Full barrier 的原始值类型 ConsumerBarrierType uint64_t Empty barrier 的原始值类型 3.2 SharedStorage 结构12345// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:280-283struct SharedStorage { FullBarrier full_barrier_[Stages]; // 每个 stage 一个 full barrier EmptyBarrier empty_barrier_[Stages]; // 每个 stage 一个 empty barrier}; 每个 Pipeline stage 都有一对 barrier： full_barrier_：Producer 填充数据后 signal，Consumer 等待 empty_barrier_：Consumer 使用完毕后 signal，Producer 等待 graph TB subgraph \"SharedStorage (SMEM)\" subgraph \"Stage 0\" FB0[full_barrier_0] EB0[empty_barrier_0] end subgraph \"Stage 1\" FB1[full_barrier_1] EB1[empty_barrier_1] end subgraph \"Stage 2\" FB2[full_barrier_2] EB2[empty_barrier_2] end subgraph \"Stage N-1\" FBN[full_barrier_N-1] EBN[empty_barrier_N-1] end end FULL[full_barrier_ptr_] --> FB0 EMPTY[empty_barrier_ptr_] --> EB0 style FB0 fill:#e3f2fd style FB1 fill:#e3f2fd style FB2 fill:#e3f2fd style FBN fill:#e3f2fd style EB0 fill:#fff8e1 style EB1 fill:#fff8e1 style EB2 fill:#fff8e1 style EBN fill:#fff8e1 3.3 ThreadCategory 枚举1234567// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:285-290enum class ThreadCategory { NonParticipant, // 不参与 Pipeline 操作 Producer, // 仅作为生产者 Consumer, // 仅作为消费者 ProducerConsumer // 同时是生产者和消费者}; 3.4 Params 参数结构123456789// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:292-299struct Params { uint32_t transaction_bytes = 0; // 每次 TMA 传输的字节数 ThreadCategory role = ThreadCategory::NonParticipant; // 线程角色 uint32_t is_leader = 0; // 是否为 leader 线程（负责 barrier 操作） uint32_t num_consumers = 0; // Consumer 线程总数 uint32_t num_producers = 1; // Producer 线程总数 int initializing_warp = 0; // 负责初始化 barrier 的 warp}; 3.5 私有成员变量1234567// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:494-499private: uint32_t dst_blockid_ = 0; // 目标 CTA ID（用于 cluster 内通信） uint32_t is_signaling_thread_ = 0; // 是否负责发送 arrive 信号 FullBarrier *full_barrier_ptr_ = nullptr; // 指向 full barrier 数组 EmptyBarrier *empty_barrier_ptr_ = nullptr; // 指向 empty barrier 数组 Params params_; // 配置参数 成员 描述 dst_blockid_ 在 cluster 模式下，标识要发送 arrive 信号的目标 CTA is_signaling_thread_ 标记此线程是否负责发送 arrive 信号（避免重复发送） full_barrier_ptr_ 指向共享内存中 FullBarrier 数组的指针 empty_barrier_ptr_ 指向共享内存中 EmptyBarrier 数组的指针 params_ 保存构造时传入的配置参数 3.6 构造函数1234567891011121314151617181920212223242526272829303132// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:326-377template&lt;class ClusterShape, class InitBarriers, class InitMasks&gt;CUTLASS_DEVICEPipelineTmaAsync(SharedStorage&amp; storage, Params params, ClusterShape cluster_shape, InitBarriers = {}, InitMasks = {}) : params_(params) , full_barrier_ptr_(&amp;storage.full_barrier_[0]) , empty_barrier_ptr_(&amp;storage.empty_barrier_[0]) { int warp_idx = canonical_warp_idx_sync(); int thread_idx = threadIdx.x; // 初始化 barrier（如果需要） if constexpr (cute::is_same_v&lt;InitBarriers, cute::true_type&gt;) { init_barriers(storage, params_, cluster_shape); } // 初始化信号掩码（用于 cluster 内通信） if constexpr (cute::is_same_v&lt;InitMasks, cute::true_type&gt;) { dim3 block_id = cute::block_id_in_cluster(); auto cluster_size = cute::size(cluster_shape); if (cluster_size == 1) { is_signaling_thread_ = true; dst_blockid_ = 0; } else { // 在 warp group 内分配 arrive 职责 // ... } }} 3.7 Barrier 初始化12345678910111213141516171819202122232425// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:301-324template &lt;class ClusterShape&gt;static CUTLASS_DEVICE voidinit_barriers(SharedStorage&amp; storage, Params params, ClusterShape cluster_shape) { int warp_idx = canonical_warp_idx_sync(); bool is_initializing_warp = (warp_idx == params.initializing_warp); if (is_initializing_warp) { uint32_t const producer_arv_cnt = params.num_producers; uint32_t multicast_consumer_arrival_count = params.num_consumers; // Cluster 模式下调整 arrival count if (cute::size(cluster_shape) &gt; 1) { uint32_t const num_consumer_warpgroups_per_cluster = cute::ceil_div(params.num_consumers, NumThreadsPerWarpGroup); multicast_consumer_arrival_count = (cute::size&lt;0&gt;(cluster_shape) + cute::size&lt;1&gt;(cluster_shape) - 1) * num_consumer_warpgroups_per_cluster; } // 初始化 barrier 对 initialize_barrier_array_pair_aligned(...); } cutlass::arch::fence_barrier_init();} 4. Producer API 详解4.1 producer_try_acquire非阻塞尝试获取缓冲区空间： 12345678910111213141516// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:419-422 (public)CUTLASS_DEVICEProducerToken producer_try_acquire(PipelineState state, uint32_t skip_wait = false) { return producer_try_acquire(state.index(), state.phase(), skip_wait);}// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:501-509 (private)CUTLASS_DEVICEProducerToken producer_try_acquire(uint32_t stage, uint32_t phase, uint32_t skip_wait) { detail::pipeline_check_is_producer(params_.role); if (skip_wait) { return {BarrierStatus::WaitDone}; } bool barrier_status = empty_barrier_ptr_[stage].try_wait(phase); return {static_cast&lt;BarrierStatus&gt;(barrier_status)};} PTX 指令： 12mbarrier.try_wait.parity.shared::cta.b64 P1, [smem_addr], phase;selp.b32 result, 1, 0, P1; 4.2 producer_acquire阻塞等待缓冲区空间，并设置期望传输字节数： 1234567891011121314151617// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:424-427 (public)CUTLASS_DEVICEvoid producer_acquire(PipelineState state) { producer_acquire(state.index(), state.phase());}// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:511-528 (private)CUTLASS_DEVICEvoid producer_acquire(uint32_t stage, uint32_t phase) { // Step 1: 等待 Consumer 释放空间 empty_barrier_ptr_[stage].wait(phase); // Step 2: Leader 线程设置期望传输字节数 if (params_.is_leader) { full_barrier_ptr_[stage].arrive_and_expect_tx(params_.transaction_bytes); }} PTX 指令： 等待部分（spin loop）： 12345LAB_WAIT: mbarrier.try_wait.parity.shared::cta.b64 P1, [smem_addr], phase, 0x989680; @P1 bra DONE; bra LAB_WAIT;DONE: 设置期望字节数： 1mbarrier.arrive.expect_tx.shared::cta.b64 _, [smem_addr], transaction_bytes; 4.3 producer_acquire (带 token)12345678910111213// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:530-550CUTLASS_DEVICEvoid producer_acquire(uint32_t stage, uint32_t phase, ProducerToken barrier_token) { detail::pipeline_check_is_producer(params_.role); // 如果 try_acquire 已经成功，跳过等待 if (barrier_token != BarrierStatus::WaitDone) { empty_barrier_ptr_[stage].wait(phase); } if (params_.is_leader) { full_barrier_ptr_[stage].arrive_and_expect_tx(params_.transaction_bytes); }} 4.4 producer_expect_transaction额外增加期望传输字节数（用于多次 TMA 操作）： 12345678// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:552-558CUTLASS_DEVICEvoid producer_expect_transaction(uint32_t stage, uint32_t transaction_bytes) { detail::pipeline_check_is_producer(params_.role); if (params_.is_leader) { full_barrier_ptr_[stage].expect_transaction(transaction_bytes); }} PTX 指令： 1mbarrier.expect_tx.shared::cta.b64 [smem_addr], transaction_bytes; 4.5 producer_commitTMA 完成后由硬件自动触发： 1234567891011// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:560-587CUTLASS_DEVICEvoid producer_commit(uint32_t stage, uint32_t bytes) { // 仅用于单元测试（无 TMA 时手动提交） #if CUTLASS_UNIT_TEST_PIPELINE if (params_.is_leader) { full_barrier_ptr_[stage].complete_transaction(bytes); // Cluster 模式下通知其他 CTA... } #endif} PTX 指令（TMA 硬件自动执行）： 1mbarrier.complete_tx.shared::cluster.relaxed.cluster.b64 [smem_addr], transaction_bytes; 4.6 producer_get_barrier返回 barrier 指针供 TMA 使用： 12345// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:638-641CUTLASS_DEVICEProducerBarrierType* producer_get_barrier(uint32_t stage) { return reinterpret_cast&lt;ProducerBarrierType*&gt;(&amp;full_barrier_ptr_[stage]);} 4.7 producer_tail防止 Producer block 过早退出： 123456789// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:447-454CUTLASS_DEVICEvoid producer_tail(PipelineState state) { detail::pipeline_check_is_producer(params_.role); for (int count = 0; count &lt; Stages; ++count) { empty_barrier_ptr_[state.index()].wait(state.phase()); ++state; }} 5. Consumer API 详解5.1 consumer_try_wait非阻塞尝试等待数据就绪： 12345678910// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:590-597CUTLASS_DEVICEConsumerToken consumer_try_wait(uint32_t stage, uint32_t phase, uint32_t skip_wait) { detail::pipeline_check_is_consumer(params_.role); if (skip_wait) { return {BarrierStatus::WaitDone}; } bool barrier_status = full_barrier_ptr_[stage].try_wait(phase); return {static_cast&lt;BarrierStatus&gt;(barrier_status)};} 返回值： BarrierStatus::WaitDone (1)：数据已就绪 BarrierStatus::WaitAgain (0)：数据未就绪 5.2 consumer_test_wait与 try_wait 类似，但使用 test_wait PTX 指令： 12345678910// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:599-607CUTLASS_DEVICEConsumerToken consumer_test_wait(uint32_t stage, uint32_t phase, uint32_t skip_wait) { detail::pipeline_check_is_consumer(params_.role); if (skip_wait) { return {BarrierStatus::WaitDone}; } bool barrier_status = full_barrier_ptr_[stage].test_wait(phase); return {static_cast&lt;BarrierStatus&gt;(barrier_status)};} 5.3 consumer_wait阻塞等待数据就绪： 12345678910111213141516// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:609-623CUTLASS_DEVICEvoid consumer_wait(uint32_t stage, uint32_t phase) { detail::pipeline_check_is_consumer(params_.role); full_barrier_ptr_[stage].wait(phase);}// 带 token 版本CUTLASS_DEVICEvoid consumer_wait(uint32_t stage, uint32_t phase, ConsumerToken barrier_token) { detail::pipeline_check_is_consumer(params_.role); if (barrier_token == BarrierStatus::WaitAgain) { full_barrier_ptr_[stage].wait(phase); } // 如果已经 WaitDone，直接返回} 5.4 consumer_release通知 Producer 空间已释放： 123456// 源码: include/cutlass/pipeline/sm90_pipeline.hpp:625-636CUTLASS_DEVICEvoid consumer_release(uint32_t stage, uint32_t skip = false) { detail::pipeline_check_is_consumer(params_.role); empty_barrier_ptr_[stage].arrive(dst_blockid_, is_signaling_thread_ &amp; (!skip));} PTX 指令： 本地 CTA arrive： 1mbarrier.arrive.shared::cta.b64 _, [smem_addr]; 远程 Cluster arrive： 12mapa.shared::cluster.u32 remAddr32, smem_addr, cta_id;mbarrier.arrive.shared::cluster.b64 _, [remAddr32]; 6. Pipeline API 到 PTX 映射总览 Pipeline API Barrier 类型 PTX 指令 producer_try_acquire EmptyBarrier mbarrier.try_wait.parity (单次) producer_acquire (wait) EmptyBarrier mbarrier.try_wait.parity (spin) producer_acquire (leader) FullBarrier mbarrier.arrive.expect_tx producer_expect_transaction FullBarrier mbarrier.expect_tx producer_commit FullBarrier mbarrier.complete_tx (TMA 自动) consumer_try_wait FullBarrier mbarrier.try_wait.parity (单次) consumer_test_wait FullBarrier mbarrier.test_wait.parity consumer_wait FullBarrier mbarrier.try_wait.parity (spin) consumer_release EmptyBarrier mbarrier.arrive 7. ClusterBarrier 实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 源码: include/cutlass/arch/barrier.h:341-532struct ClusterBarrier { using ValueType = uint64_t;protected: ValueType barrier_; // SMEM 中的 64-bit mbarrier 对象public: // 初始化 barrier static void init(ValueType const* smem_ptr, uint32_t arrive_count) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); asm volatile( &quot;mbarrier.init.shared::cta.b64 [%1], %0;&quot; : : &quot;r&quot;(arrive_count), &quot;r&quot;(smem_addr)); } // 阻塞等待（spin loop） static void wait(ValueType const* smem_ptr, uint32_t phase) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); uint32_t ticks = 0x989680; // ~10M cycles 超时后重试 asm volatile( &quot;.reg .pred P1;\\n&quot; &quot;LAB_WAIT:\\n&quot; &quot;mbarrier.try_wait.parity.shared::cta.b64 P1, [%0], %1, %2;\\n&quot; &quot;@P1 bra DONE;\\n&quot; &quot;bra LAB_WAIT;\\n&quot; &quot;DONE:&quot; : : &quot;r&quot;(smem_addr), &quot;r&quot;(phase), &quot;r&quot;(ticks)); } // 非阻塞尝试等待 static bool try_wait(ValueType const* smem_ptr, uint32_t phase) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); uint32_t waitComplete; asm volatile( &quot;.reg .pred P1;\\n&quot; &quot;mbarrier.try_wait.parity.shared::cta.b64 P1, [%1], %2;\\n&quot; &quot;selp.b32 %0, 1, 0, P1;&quot; : &quot;=r&quot;(waitComplete) : &quot;r&quot;(smem_addr), &quot;r&quot;(phase)); return static_cast&lt;bool&gt;(waitComplete); } // 本地 arrive static void arrive(ValueType const* smem_ptr) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); asm volatile( &quot;mbarrier.arrive.shared::cta.b64 _, [%0];&quot; : : &quot;r&quot;(smem_addr)); } // 远程 cluster arrive static void arrive(ValueType const* smem_ptr, uint32_t cta_id, uint32_t pred) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); if (pred) { asm volatile( &quot;.reg .b32 remAddr32;\\n&quot; &quot;mapa.shared::cluster.u32 remAddr32, %0, %1;\\n&quot; &quot;mbarrier.arrive.shared::cluster.b64 _, [remAddr32];&quot; : : &quot;r&quot;(smem_addr), &quot;r&quot;(cta_id)); } }}; 8. ClusterTransactionBarrier 实现1234567891011121314151617181920212223242526272829303132// 源码: include/cutlass/arch/barrier.h:538-693struct ClusterTransactionBarrier : public ClusterBarrier { // Arrive + 设置期望传输字节数 static void arrive_and_expect_tx(ValueType const* smem_ptr, uint32_t transaction_bytes) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); asm volatile( &quot;mbarrier.arrive.expect_tx.shared::cta.b64 _, [%1], %0;&quot; : : &quot;r&quot;(transaction_bytes), &quot;r&quot;(smem_addr)); } // 仅设置期望字节数（不 arrive） static void expect_transaction(ValueType const* smem_ptr, uint32_t transaction_bytes) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); asm volatile( &quot;mbarrier.expect_tx.shared::cta.b64 [%1], %0;&quot; : : &quot;r&quot;(transaction_bytes), &quot;r&quot;(smem_addr)); } // 完成传输（减少 pending 字节数）- TMA 硬件自动调用 static void complete_transaction( ValueType const* smem_ptr, uint32_t dst_cta_id, uint32_t transaction_bytes, uint32_t pred = 1) { uint32_t smem_addr = cute::cast_smem_ptr_to_uint(smem_ptr); smem_addr = cute::set_block_rank(smem_addr, dst_cta_id); asm volatile( &quot;.reg .pred p;\\n&quot; &quot;setp.eq.u32 p, %2, 1;\\n&quot; &quot;@p mbarrier.complete_tx.shared::cluster.relaxed.cluster.b64 [%1], %0;&quot; : : &quot;r&quot;(transaction_bytes), &quot;r&quot;(smem_addr), &quot;r&quot;(pred)); }}; 9. 完整工作流程示例下图展示了 Producer 和 Consumer 之间的交互时序： sequenceDiagram participant P as Producer participant FB as FullBarrier participant EB as EmptyBarrier participant BUF as SMEM Buffer participant C as Consumer Note over P,C: Stage i 的一次完整迭代 rect rgb(232, 245, 233) Note over P: Producer Phase P->>EB: wait(phase) - 等待空间 EB-->>P: 空间已释放 P->>FB: arrive_and_expect_tx(bytes) P->>BUF: TMA Load (异步) Note over FB: TMA完成后自动complete_tx end rect rgb(227, 242, 253) Note over C: Consumer Phase C->>FB: wait(phase) - 等待数据 FB-->>C: 数据已就绪 BUF->>C: 读取数据 Note over C: MMA 计算 C->>EB: arrive() - 释放空间 end Note over P,C: Phase 翻转后进入下一轮 12345678910111213141516171819202122232425262728293031323334353637383940// Producer 线程（TMA 加载器）PipelineState producer_state = make_producer_start_state&lt;Pipeline&gt;();for (int k = 0; k &lt; num_tiles; ++k) { // 1. 获取缓冲区（等待空间 + 设置期望字节数） pipeline.producer_acquire(producer_state); // 2. 获取 barrier 指针供 TMA 使用 auto* barrier = pipeline.producer_get_barrier(producer_state); // 3. 发起 TMA 加载（硬件自动完成 barrier） copy(tma_load, gmem_tensor, smem_tensor, barrier); ++producer_state;}// 4. 退出前等待所有 Consumer 完成pipeline.producer_tail(producer_state);// Consumer 线程（MMA 计算）PipelineState consumer_state{0, 0, 0}; // phase = 0for (int k = 0; k &lt; num_tiles; ++k) { // 1. 尝试等待（非阻塞） auto token = pipeline.consumer_try_wait(consumer_state); // 2. 可以做一些其他工作... // 3. 如果需要，阻塞等待 pipeline.consumer_wait(consumer_state, token); // 4. 使用 SMEM 数据进行 MMA 计算 gemm(smem_tensor, accumulators); // 5. 释放缓冲区给 Producer pipeline.consumer_release(consumer_state); ++consumer_state;} 10. 关键要点总结 SM90 专属：Pipeline 机制专为 NVIDIA Hopper 架构（SM90+）设计 双 Barrier 架构： FullBarrier（ClusterTransactionBarrier）：Producer → Consumer，数据就绪信号 EmptyBarrier（ClusterBarrier）：Consumer → Producer，空间释放信号 硬件加速：TMA 完成时自动 signal barrier，无需软件干预 Phase 机制：phase 位使 barrier 可跨迭代复用 Cluster 支持：通过 mapa 指令实现跨 CTA 的 barrier 操作 Leader 模式：只有 leader 线程执行 barrier 的 arrive/expect_tx 操作，避免重复 参考资料 CUTLASS GitHub 仓库 sm90_pipeline.hpp barrier.h NVIDIA PTX ISA - mbarrier CUDA Programming Guide - Asynchronous Barrier","link":"/2024/12/23/pipeline-barrier-ptx-mapping/"}],"tags":[{"name":"CUTLASS","slug":"CUTLASS","link":"/tags/CUTLASS/"},{"name":"TMA","slug":"TMA","link":"/tags/TMA/"},{"name":"CuTe","slug":"CuTe","link":"/tags/CuTe/"},{"name":"SM90","slug":"SM90","link":"/tags/SM90/"},{"name":"PTX","slug":"PTX","link":"/tags/PTX/"},{"name":"Multicast","slug":"Multicast","link":"/tags/Multicast/"},{"name":"Cluster","slug":"Cluster","link":"/tags/Cluster/"},{"name":"Pipeline","slug":"Pipeline","link":"/tags/Pipeline/"},{"name":"mbarrier","slug":"mbarrier","link":"/tags/mbarrier/"}],"categories":[{"name":"CUTLASS","slug":"CUTLASS","link":"/categories/CUTLASS/"}],"pages":[{"title":"关于","text":"关于本站这是一个专注于 NVIDIA CUTLASS 和 CuTE 内部实现的学习笔记站点。 内容涵盖 Pipeline 同步机制: mbarrier、双 Barrier 架构、PTX 指令映射 Epilogue 融合: EVT (Expression Visitor Tree)、FusionCallbacks CuTE Tensor: Layout、Stride、TMA 联系方式 GitHub: DrXuQian","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"","text":"/* Custom styles for wider page width */ /* ===== Dark Mode Theme Toggle ===== */ /* Theme toggle button in navbar */ .theme-toggle { cursor: pointer; padding: 0.5rem; display: flex; align-items: center; justify-content: center; font-size: 1.2rem; color: inherit; background: transparent; border: none; transition: transform 0.3s ease; } .theme-toggle:hover { transform: scale(1.1); } .theme-toggle .icon-sun, .theme-toggle .icon-moon { display: none; } /* Light mode: show moon icon */ html:not([data-theme=\"dark\"]) .theme-toggle .icon-moon { display: inline; } /* Dark mode: show sun icon */ html[data-theme=\"dark\"] .theme-toggle .icon-sun { display: inline; } /* ===== Dark Mode Styles ===== */ html[data-theme=\"dark\"] { --bg-color: #1a1a2e; --bg-color-secondary: #16213e; --text-color: #e4e4e4; --text-color-secondary: #a0a0a0; --border-color: #2d2d4a; --card-bg: #1f1f38; --link-color: #6c9fff; --link-hover: #99b8ff; --code-bg: #0d1117; } html[data-theme=\"dark\"] body { background-color: var(--bg-color) !important; color: var(--text-color) !important; } html[data-theme=\"dark\"] .navbar { background-color: var(--bg-color-secondary) !important; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.3); } html[data-theme=\"dark\"] .navbar-item, html[data-theme=\"dark\"] .navbar-link { color: var(--text-color) !important; } html[data-theme=\"dark\"] .navbar-item:hover, html[data-theme=\"dark\"] .navbar-link:hover { background-color: rgba(255, 255, 255, 0.1) !important; color: var(--link-color) !important; } html[data-theme=\"dark\"] .card { background-color: var(--card-bg) !important; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3) !important; } html[data-theme=\"dark\"] .card-content { background-color: var(--card-bg) !important; } html[data-theme=\"dark\"] .title, html[data-theme=\"dark\"] .subtitle, html[data-theme=\"dark\"] h1, html[data-theme=\"dark\"] h2, html[data-theme=\"dark\"] h3, html[data-theme=\"dark\"] h4, html[data-theme=\"dark\"] h5, html[data-theme=\"dark\"] h6 { color: var(--text-color) !important; } html[data-theme=\"dark\"] .content { color: var(--text-color) !important; } html[data-theme=\"dark\"] a { color: var(--link-color) !important; } html[data-theme=\"dark\"] a:hover { color: var(--link-hover) !important; } html[data-theme=\"dark\"] .menu-label { color: var(--text-color-secondary) !important; } html[data-theme=\"dark\"] .menu-list a { color: var(--text-color) !important; } html[data-theme=\"dark\"] .menu-list a:hover { background-color: rgba(255, 255, 255, 0.05) !important; color: var(--link-color) !important; } html[data-theme=\"dark\"] .tag { background-color: var(--border-color) !important; color: var(--text-color) !important; } html[data-theme=\"dark\"] .footer { background-color: var(--bg-color-secondary) !important; color: var(--text-color-secondary) !important; } html[data-theme=\"dark\"] .widget { background-color: var(--card-bg) !important; } html[data-theme=\"dark\"] pre, html[data-theme=\"dark\"] code { background-color: var(--code-bg) !important; } html[data-theme=\"dark\"] .hljs { background-color: var(--code-bg) !important; } html[data-theme=\"dark\"] .article-meta, html[data-theme=\"dark\"] .level-item { color: var(--text-color-secondary) !important; } html[data-theme=\"dark\"] .button.is-primary { background-color: #3273dc !important; } html[data-theme=\"dark\"] .input, html[data-theme=\"dark\"] .textarea, html[data-theme=\"dark\"] .select select { background-color: var(--bg-color-secondary) !important; border-color: var(--border-color) !important; color: var(--text-color) !important; } html[data-theme=\"dark\"] .pagination-link, html[data-theme=\"dark\"] .pagination-previous, html[data-theme=\"dark\"] .pagination-next { background-color: var(--card-bg) !important; border-color: var(--border-color) !important; color: var(--text-color) !important; } html[data-theme=\"dark\"] .toc-list a { color: var(--text-color) !important; } html[data-theme=\"dark\"] .toc-list a:hover, html[data-theme=\"dark\"] .toc-list a.is-active { color: var(--link-color) !important; } html[data-theme=\"dark\"] hr { background-color: var(--border-color) !important; } html[data-theme=\"dark\"] blockquote { background-color: var(--bg-color-secondary) !important; border-left-color: var(--link-color) !important; } html[data-theme=\"dark\"] table th, html[data-theme=\"dark\"] table td { border-color: var(--border-color) !important; } html[data-theme=\"dark\"] table thead th { background-color: var(--bg-color-secondary) !important; color: var(--text-color) !important; } html[data-theme=\"dark\"] table tbody tr:nth-child(even) { background-color: rgba(255, 255, 255, 0.02) !important; } /* ===== End Dark Mode Styles ===== */ /* ===== Hide right sidebar on post pages ===== */ body.is-post-page .column.column-right { display: none !important; } /* When right sidebar is hidden, expand main content */ body.is-post-page .column.column-main.is-8-tablet.is-8-desktop.is-6-widescreen { width: 70% !important; flex: none !important; } body.is-post-page .column.column-left.is-4-tablet.is-4-desktop.is-3-widescreen { width: 30% !important; flex: none !important; } /* For two-column layout on post pages */ body.is-post-page .columns { justify-content: center; } /* ===== End Hide Right Sidebar ===== */ /* Increase container max-width */ html body .container { max-width: 1600px !important; } /* Main content column - wider */ html body .column.is-8-tablet, html body .column.is-8-desktop, html body .column.is-8-widescreen, html body .is-8-desktop, html body .is-8-widescreen { flex: none !important; width: 70% !important; } /* Sidebar column - narrower */ html body .column.is-4-tablet, html body .column.is-4-desktop, html body .column.is-4-widescreen, html body .is-4-desktop, html body .is-4-widescreen { flex: none !important; width: 30% !important; } /* Two sidebar layout - left sidebar smaller */ html body .column.is-8-tablet.is-8-desktop.is-6-widescreen { width: 55% !important; flex: none !important; } html body .column.is-4-tablet.is-4-desktop.is-3-widescreen { width: 22.5% !important; flex: none !important; } /* Card content padding */ html body .card-content { padding: 1.25rem 1.5rem; } /* Code block */ html body .content pre { max-width: 100%; overflow-x: auto; } /* Article content */ html body .article-content { max-width: 100%; } /* Large screens */ @media screen and (min-width: 1408px) { html body .container { max-width: 1700px !important; } } @media screen and (min-width: 1600px) { html body .container { max-width: 1900px !important; } } @media screen and (min-width: 1800px) { html body .container { max-width: 2100px !important; } html body .column.is-8-desktop, html body .column.is-8-widescreen { width: 72% !important; } html body .column.is-4-desktop, html body .column.is-4-widescreen { width: 28% !important; } }","link":"/css/custom.css"},{"title":"","text":"// Theme toggle functionality (function() { 'use strict'; // Detect if current page is a post page and add class to body function detectPostPage() { // Check if URL matches post pattern (e.g., /2024/12/23/post-name/) const path = window.location.pathname; const isPost = /^\\/\\d{4}\\/\\d{2}\\/\\d{2}\\//.test(path); if (isPost) { document.body.classList.add('is-post-page'); } else { document.body.classList.remove('is-post-page'); } } // Get saved theme or default to light function getTheme() { return localStorage.getItem('theme') || 'light'; } // Apply theme to document function applyTheme(theme) { if (theme === 'dark') { document.documentElement.setAttribute('data-theme', 'dark'); } else { document.documentElement.removeAttribute('data-theme'); } } // Save theme preference function saveTheme(theme) { localStorage.setItem('theme', theme); } // Toggle between light and dark function toggleTheme() { const currentTheme = getTheme(); const newTheme = currentTheme === 'dark' ? 'light' : 'dark'; applyTheme(newTheme); saveTheme(newTheme); } // Create and inject toggle button into navbar function createToggleButton() { const navbarEnd = document.querySelector('.navbar-end'); if (!navbarEnd) return; // Check if button already exists if (document.querySelector('.theme-toggle')) return; const toggleBtn = document.createElement('a'); toggleBtn.className = 'navbar-item theme-toggle'; toggleBtn.title = 'Toggle Dark Mode'; toggleBtn.innerHTML = ''; toggleBtn.addEventListener('click', function(e) { e.preventDefault(); toggleTheme(); }); // Insert before the search button const searchBtn = navbarEnd.querySelector('.search'); if (searchBtn) { navbarEnd.insertBefore(toggleBtn, searchBtn); } else { navbarEnd.appendChild(toggleBtn); } } // Initialize on page load function init() { // Apply saved theme immediately applyTheme(getTheme()); // Detect post page and hide right sidebar detectPostPage(); // Create toggle button when DOM is ready if (document.readyState === 'loading') { document.addEventListener('DOMContentLoaded', createToggleButton); } else { createToggleButton(); } } // Apply theme immediately to prevent flash applyTheme(getTheme()); // Run init when DOM is ready if (document.readyState === 'loading') { document.addEventListener('DOMContentLoaded', init); } else { init(); } // Re-create button and detect page type after pjax navigation (Icarus uses pjax) document.addEventListener('pjax:complete', function() { createToggleButton(); detectPostPage(); }); })();","link":"/js/theme-toggle.js"},{"title":"标签","text":"","link":"/tags/index.html"}]}